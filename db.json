{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/random/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/random/source/css/blank.gif","path":"css/blank.gif","modified":0,"renderable":1},{"_id":"themes/random/source/css/fancybox_loading.gif","path":"css/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/random/source/css/fancybox_overlay.png","path":"css/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/random/source/css/fancybox_sprite.png","path":"css/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/random/source/css/highlight-railscasts.css","path":"css/highlight-railscasts.css","modified":0,"renderable":1},{"_id":"themes/random/source/css/jquery.fancybox-thumbs.css","path":"css/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/random/source/css/next.png","path":"css/next.png","modified":0,"renderable":1},{"_id":"themes/random/source/css/plyr.css","path":"css/plyr.css","modified":0,"renderable":1},{"_id":"themes/random/source/css/prev.png","path":"css/prev.png","modified":0,"renderable":1},{"_id":"themes/random/source/css/fancybox_loading@2x.gif","path":"css/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/random/source/css/random.styl","path":"css/random.styl","modified":0,"renderable":1},{"_id":"themes/random/source/css/fancybox_sprite@2x.png","path":"css/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/random/source/js/highlight.pack.js","path":"js/highlight.pack.js","modified":0,"renderable":1},{"_id":"themes/random/source/css/vegas.min.css","path":"css/vegas.min.css","modified":0,"renderable":1},{"_id":"themes/random/source/css/jquery.fancybox.css","path":"css/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/random/source/css/sprite.svg","path":"css/sprite.svg","modified":0,"renderable":1},{"_id":"themes/random/source/js/jquery.mousewheel.pack.js","path":"js/jquery.mousewheel.pack.js","modified":0,"renderable":1},{"_id":"themes/random/source/js/random.js","path":"js/random.js","modified":0,"renderable":1},{"_id":"themes/random/source/js/jquery.fancybox-thumbs.js","path":"js/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/random/source/js/plyr.js","path":"js/plyr.js","modified":0,"renderable":1},{"_id":"themes/random/source/js/jquery-2.2.3.min.js","path":"js/jquery-2.2.3.min.js","modified":0,"renderable":1},{"_id":"themes/random/source/js/jquery.fancybox.pack.js","path":"js/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.css","path":"css/iconfont/iconfont.css","modified":0,"renderable":1},{"_id":"themes/random/source/js/vegas.min.js","path":"js/vegas.min.js","modified":0,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.eot","path":"css/iconfont/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.svg","path":"css/iconfont/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.woff","path":"css/iconfont/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.ttf","path":"css/iconfont/iconfont.ttf","modified":0,"renderable":1}],"Cache":[{"_id":"themes/random/.gitignore","hash":"6993cdad70ce92d2734ccd7a0a944e539a79e738","modified":1466575630716},{"_id":"themes/random/LICENSE","hash":"03bec2e3a3b7bad96e2688d57197c99b494dcdb5","modified":1466575630717},{"_id":"themes/random/_config.yml","hash":"e60f6ef27aef59b84e3d8943ccdba85296172ba4","modified":1481552883879},{"_id":"themes/random/README.CN.md","hash":"197e736dc03c31a666b54b2c26f56c7e9c8169c6","modified":1466575630718},{"_id":"themes/random/README.md","hash":"26bc4bf3b66583fb8a9d400722de223b346d4586","modified":1466575630718},{"_id":"source/_posts/graphite.md","hash":"b789b50e46364c61dcd708a6122961d47acff79a","modified":1475913216699},{"_id":"source/_posts/hello-world.md","hash":"153e516fc3e55ecc19c34344a4a19a8396a6d891","modified":1468565798880},{"_id":"source/_posts/centos-scrapy-ghost.py.md","hash":"3632357933369f23c6dc216906e7409d46430881","modified":1471528148002},{"_id":"source/_posts/img2txt.md","hash":"4ff151ea11a5ebaf45d4cc58a2b550722bebf24e","modified":1469024430941},{"_id":"source/about/index.md","hash":"449909708312f92ce5159f61c0815d08df6dcbbc","modified":1466125557734},{"_id":"source/categories/index.md","hash":"174e63bdf5768ee16d002dad34de812b200f4ae2","modified":1466125557734},{"_id":"source/tags/index.md","hash":"63773e15ffc42a9c4488f47fbdaf1a5f978a5c79","modified":1466125557735},{"_id":"themes/random/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1466575630522},{"_id":"themes/random/.git/config","hash":"a50b6632189a332eb4f3128bbbf30a6278addf86","modified":1481552895974},{"_id":"themes/random/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1466575625530},{"_id":"themes/random/.git/index","hash":"f68802f2b1911c3dd32549ce9196dc35efba9e0a","modified":1466585131672},{"_id":"themes/random/.git/packed-refs","hash":"fa836080405cb6079c1c640bd65cae6be761a7c7","modified":1466575630519},{"_id":"themes/random/layout/archive.swig","hash":"23f96a38b04d116aedbed0820d0eb0c9a4b8622c","modified":1466575630722},{"_id":"themes/random/layout/category.swig","hash":"58492ccd8a863b06742f89dcac97782dfd86e43a","modified":1466575630722},{"_id":"source/_posts/hmm-memm-crf.md","hash":"88105118fd132feef3932ef498d9b0a8e4b6b179","modified":1469065355744},{"_id":"themes/random/layout/index.swig","hash":"411520dcdcf83f16e19cbf039f6aa544a6c332d3","modified":1466575630730},{"_id":"themes/random/layout/page.swig","hash":"146e6587c08e6bea08eec98b45b8e0438793cfd5","modified":1466575630730},{"_id":"themes/random/layout/tag.swig","hash":"340ca3d9dceff6827865ffceabbfbd6eb75a927f","modified":1466575630731},{"_id":"themes/random/languages/en.yml","hash":"1435cf69d9607a7912f647d861ec4a30a8c08dba","modified":1466575630720},{"_id":"themes/random/languages/zh-CN.yml","hash":"1f18252ba42703ccdecd9ce01ac57a829b99d9fe","modified":1466575630720},{"_id":"source/_posts/zhihu-link.md","hash":"5407fa07f57a425c7c414dc3bccff8fe60e45ceb","modified":1469022587806},{"_id":"themes/random/layout/post.swig","hash":"10570f6c48f4accf65cff253639b03ee5cd2e993","modified":1466575630731},{"_id":"themes/random/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1466575625555},{"_id":"themes/random/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1466575625603},{"_id":"themes/random/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1466575625603},{"_id":"themes/random/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1466575625613},{"_id":"themes/random/source/favicon.ico","hash":"96b9a549337c2bec483c2879eeafa4d1f8748fed","modified":1466575630767},{"_id":"themes/random/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1466575625614},{"_id":"themes/random/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1466575625588},{"_id":"themes/random/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1466575625626},{"_id":"themes/random/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1466575625627},{"_id":"themes/random/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1466575625628},{"_id":"themes/random/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1466575625629},{"_id":"themes/random/layout/includes/baidu-tongji.swig","hash":"3b88cbae8980212859e898d7b3c16e839f99cd80","modified":1466575630722},{"_id":"themes/random/layout/includes/duoshuo.swig","hash":"3e97d0c6b0243bb16e3d76ba3f481338486322c9","modified":1466575630723},{"_id":"themes/random/layout/includes/gallery.swig","hash":"7d749acb90d424f2d04587fca396e578bf1fabb5","modified":1466575630724},{"_id":"themes/random/layout/includes/google-analytics.swig","hash":"3e4cad2175c599edd6f3e66f0a738a57e6eeffd7","modified":1466575630725},{"_id":"source/_posts/hmm4nlp.md","hash":"629cf25ba022b2d9a875038081628f86331e2e43","modified":1472805857690},{"_id":"themes/random/layout/includes/disqus.swig","hash":"d81bc248e3926019e21deaac52123678ae6af55b","modified":1466575630723},{"_id":"themes/random/layout/includes/footer.swig","hash":"1512b1b974bc7bd0cf7a04234c9ccf32500ed2c3","modified":1466575630724},{"_id":"themes/random/layout/includes/head.swig","hash":"6dfd7afb26d79729a90bf2dd3c453d43e751d58b","modified":1466575630725},{"_id":"themes/random/layout/includes/layout.swig","hash":"d0a884710325964ad236c8dde66be96d34c14dda","modified":1466575630726},{"_id":"themes/random/layout/includes/side-pagination.swig","hash":"f3d4b832aece4a22eb9a5503063ccd384dc4538d","modified":1466575630728},{"_id":"themes/random/layout/includes/pagination.swig","hash":"1c4c841816c7fbfd4d34c32f05797264942d6a22","modified":1466575630727},{"_id":"themes/random/layout/includes/toc.swig","hash":"704b68b52663734e00e2bd4e21c51fab92868e12","modified":1466575630729},{"_id":"themes/random/layout/includes/user-card.swig","hash":"7c17b70595754638043b3eddee626420c27dffd7","modified":1466575630729},{"_id":"themes/random/layout/includes/uyan.swig","hash":"3803b211124a7967aebf5fb96870b0f8b4be3e6f","modified":1466575630729},{"_id":"themes/random/source/css/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1466575630732},{"_id":"themes/random/layout/includes/post-title-item.swig","hash":"10238491556e3104b5ee5ca52df3eb50029cc47d","modified":1466575630727},{"_id":"themes/random/layout/includes/recent-posts.swig","hash":"281446641c5d33762bd00d4fa28447aeb2fa5d18","modified":1466575630727},{"_id":"themes/random/layout/includes/social-icon.swig","hash":"665510baf91266f9693cbcb7b4b47b6f6e6a4947","modified":1466575630728},{"_id":"themes/random/source/css/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1466575630733},{"_id":"themes/random/source/css/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1466575630734},{"_id":"themes/random/source/css/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1466575630734},{"_id":"themes/random/layout/includes/jiathis.swig","hash":"53fb1038a56b2428c6ff3ffae01203dddc112f6a","modified":1466575630726},{"_id":"themes/random/source/css/highlight-railscasts.css","hash":"1bb2dd8ccba3e33aa3fd419bad757b0710ca7bf3","modified":1466575630735},{"_id":"themes/random/source/css/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1466575630762},{"_id":"themes/random/source/css/next.png","hash":"1bf3e61fbe6858bd9d154e23a477a06307f80436","modified":1466575630763},{"_id":"themes/random/source/css/plyr.css","hash":"29202451dd2547739b2b763952ff58c8a4d11df9","modified":1466575630764},{"_id":"themes/random/source/css/prev.png","hash":"c35bf41b4597a371e80ffdaf338b5693a082a5f4","modified":1466575630764},{"_id":"themes/random/source/css/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1466575630733},{"_id":"themes/random/source/css/random.styl","hash":"9a72972f08947b9e4fde8f4d085a10bca97e53e7","modified":1466582214977},{"_id":"themes/random/source/css/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1466575630735},{"_id":"themes/random/source/js/highlight.pack.js","hash":"8407be86478389b07f54296e976a1d6f0a6cf69a","modified":1466575630786},{"_id":"themes/random/source/css/vegas.min.css","hash":"5810e20875386f98565b69de5ca8ee1d0a6d1feb","modified":1466575630766},{"_id":"themes/random/source/css/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1466575630763},{"_id":"themes/random/source/css/sprite.svg","hash":"4ed6a0d335ce214ec00fc9e56867687798a53ee3","modified":1466575630765},{"_id":"themes/random/source/js/jquery.mousewheel.pack.js","hash":"5d6f224e3080fd4066f8ef5c63d3f467e9d29e66","modified":1466575630796},{"_id":"themes/random/source/js/random.js","hash":"2ff5837668e5a8fe5f53dcfe9d0b8c005e58ced3","modified":1466575630798},{"_id":"themes/random/source/js/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1466575630788},{"_id":"themes/random/source/js/plyr.js","hash":"58ddde47173120b23f752872a535d7b3c9166ae8","modified":1466575630797},{"_id":"themes/random/.git/logs/HEAD","hash":"f10f4700b496f0c2825b977927dee73df01dc7f1","modified":1466575630524},{"_id":"themes/random/source/js/jquery-2.2.3.min.js","hash":"223a49c329f0f0a651d142be9dadc95008678d26","modified":1466575630788},{"_id":"themes/random/.git/objects/pack/pack-7f0245a058983d2de7f7843f803886e99be35bae.idx","hash":"b72944ea7a99711417311977337f808833882506","modified":1466575630190},{"_id":"themes/random/.git/refs/heads/master","hash":"24ea10a0bcde23d65d704386d054ddaa0a1edbf6","modified":1466575630523},{"_id":"themes/random/source/js/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1466575630795},{"_id":"themes/random/source/css/iconfont/iconfont.css","hash":"1955cec6e833e3b782bb6b150682ecbb3b0d9c1f","modified":1466575630736},{"_id":"themes/random/source/js/vegas.min.js","hash":"49a911f3434d0d5a7d0372129e4a80b4b4bb3923","modified":1466575630798},{"_id":"themes/random/source/css/iconfont/iconfont.eot","hash":"b4d3e20bd54983f7d9f95e789e9e28d056592f11","modified":1466575630737},{"_id":"themes/random/source/css/iconfont/iconfont.svg","hash":"ed867bba0ead29c55997b7b9568ee5c0499a4379","modified":1466575630737},{"_id":"themes/random/source/css/iconfont/iconfont.woff","hash":"5c122ce49b9fc0c05c0e0f88da009469e5286a55","modified":1466575630738},{"_id":"themes/random/source/css/iconfont/iconfont.ttf","hash":"3a02ec5b0453b13944311361e95081f1c3085f6d","modified":1466575630738},{"_id":"themes/random/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1466575630522},{"_id":"themes/random/.git/logs/refs/heads/master","hash":"f10f4700b496f0c2825b977927dee73df01dc7f1","modified":1466575630523},{"_id":"themes/random/.git/objects/pack/pack-7f0245a058983d2de7f7843f803886e99be35bae.pack","hash":"11b9bb61b2bb694d8533c34bd174ad23e01158b8","modified":1466575630239},{"_id":"themes/random/.git/logs/refs/remotes/origin/HEAD","hash":"f10f4700b496f0c2825b977927dee73df01dc7f1","modified":1466575630522},{"_id":"public/search.xml","hash":"d47fa9511eb6b84811cf6b619fe1ef5b98207b73","modified":1481552946967},{"_id":"public/favicon.ico","hash":"96b9a549337c2bec483c2879eeafa4d1f8748fed","modified":1481552947068},{"_id":"public/css/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1481552947069},{"_id":"public/css/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1481552947069},{"_id":"public/css/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1481552947070},{"_id":"public/css/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1481552947070},{"_id":"public/css/next.png","hash":"1bf3e61fbe6858bd9d154e23a477a06307f80436","modified":1481552947070},{"_id":"public/css/prev.png","hash":"c35bf41b4597a371e80ffdaf338b5693a082a5f4","modified":1481552947070},{"_id":"public/css/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1481552947070},{"_id":"public/css/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1481552947070},{"_id":"public/css/sprite.svg","hash":"4ed6a0d335ce214ec00fc9e56867687798a53ee3","modified":1481552947070},{"_id":"public/css/iconfont/iconfont.eot","hash":"b4d3e20bd54983f7d9f95e789e9e28d056592f11","modified":1481552947070},{"_id":"public/css/iconfont/iconfont.svg","hash":"ed867bba0ead29c55997b7b9568ee5c0499a4379","modified":1481552947070},{"_id":"public/css/iconfont/iconfont.woff","hash":"5c122ce49b9fc0c05c0e0f88da009469e5286a55","modified":1481552947070},{"_id":"public/css/iconfont/iconfont.ttf","hash":"3a02ec5b0453b13944311361e95081f1c3085f6d","modified":1481552947070},{"_id":"public/index.html","hash":"61275ca883d5ca9669b8a996778b00ab6e8ae5ea","modified":1481552948572},{"_id":"public/about/index.html","hash":"1c036648325f6c66af930475f9e49a9f20bec2e1","modified":1481552948584},{"_id":"public/categories/index.html","hash":"031d6502f998a1c12d5ff8f5fb7343bdf12e0fbb","modified":1481552948584},{"_id":"public/tags/index.html","hash":"c132ded5249139000f0a3b51ac14a53ed6f7e958","modified":1481552948584},{"_id":"public/2016/07/28/hmm4nlp/index.html","hash":"fdac7dd251d316f1952c53bd2186a4d17df09ff8","modified":1481552948584},{"_id":"public/2016/07/20/img2txt/index.html","hash":"281863f043f15e1ab84d06fd1f1cee3fe1212c68","modified":1481552948584},{"_id":"public/2016/06/16/centos-scrapy-ghost.py/index.html","hash":"5f421af425373e04b67383a7453f9ca2fff7fd4a","modified":1481552948584},{"_id":"public/2016/06/12/hello-world/index.html","hash":"e851147e2df3934339d31fe8a299e85edd2d52d2","modified":1481552948584},{"_id":"public/archives/index.html","hash":"5a766b86a38873fb71b1088d1f06c46b1a445a4c","modified":1481552948584},{"_id":"public/archives/2016/index.html","hash":"b1468029768c68b530f707e0c6dae11b284f51ec","modified":1481552948585},{"_id":"public/archives/2016/06/index.html","hash":"ea8b4b97b6e0ff5ae8f2525eca5d243bc012e775","modified":1481552948585},{"_id":"public/archives/2016/07/index.html","hash":"ce84a364d39d63287f3d7897ac43134ee684b494","modified":1481552948585},{"_id":"public/archives/2016/10/index.html","hash":"4fa7a93cb45389bee0acd7f90ed952a356e1bf09","modified":1481552948585},{"_id":"public/categories/Crawler/index.html","hash":"ec4f76002ff3384d5b066b90c706b30aedb3a2ca","modified":1481552948585},{"_id":"public/categories/Reviews/index.html","hash":"bc73a170fee4b7bf29cb44618524404c266539c8","modified":1481552948585},{"_id":"public/categories/Site-Est/index.html","hash":"49f5d4bbaded0dbb3613474fcd3a8839aa0c63a1","modified":1481552948585},{"_id":"public/categories/Fun/index.html","hash":"f5aebad8f0aeecdf820c143ccb129ba5bf23d5fa","modified":1481552948585},{"_id":"public/categories/NLP/index.html","hash":"333ccc30b3cdde83e5747d805c2988be5fb9a654","modified":1481552948585},{"_id":"public/tags/Graphite/index.html","hash":"cd3e9c6cc3fc91cd27c02f34b9b7933eb8423d98","modified":1481552948585},{"_id":"public/tags/Scrapy/index.html","hash":"96e1d13d80d44ab22ced5dc719b82904898664f3","modified":1481552948585},{"_id":"public/tags/CentOS/index.html","hash":"153123dd0fa29234bcb9d4b9018f2b22e59b5064","modified":1481552948586},{"_id":"public/tags/python/index.html","hash":"94ffdc5f595d442e27be04152596e4cd0b7f3778","modified":1481552948586},{"_id":"public/tags/ghost-py/index.html","hash":"dfaf31173cf66b4842c9db4ef56ac732d9a6c161","modified":1481552948586},{"_id":"public/tags/English/index.html","hash":"95d41a77bb48406d7a43142c94771cc98f704fce","modified":1481552948586},{"_id":"public/tags/Hexo/index.html","hash":"8a0fa7c6aa910a05912ccfaaaf992c0a4068050f","modified":1481552948586},{"_id":"public/tags/PIL/index.html","hash":"9bba8a86a4f81dcba5273681b7d28d6b43905590","modified":1481552948586},{"_id":"public/tags/HMM/index.html","hash":"559997bbafeb7ccda6d0ff13f94addd179cfef45","modified":1481552948586},{"_id":"public/tags/MEMM/index.html","hash":"8d329ccc9b9c8918d6bdf97432cb3c8965300847","modified":1481552948586},{"_id":"public/tags/CRF/index.html","hash":"813cff808a40cb56c1aee89daeb64e6896ff8419","modified":1481552948586},{"_id":"public/tags/zhihu/index.html","hash":"9241c3a1f75ed362464841f4d489a0c649c609fe","modified":1481552948586},{"_id":"public/tags/Neo4j/index.html","hash":"0fb4c6eb45ca3d0d1e8cd3245fb7f944bacd6866","modified":1481552948587},{"_id":"public/css/highlight-railscasts.css","hash":"a6d2043478fae5915926914cbd96fe9b706d98a6","modified":1481552948587},{"_id":"public/css/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1481552948587},{"_id":"public/css/plyr.css","hash":"29202451dd2547739b2b763952ff58c8a4d11df9","modified":1481552948587},{"_id":"public/css/random.css","hash":"264edb971f706a5717b3dc728df3654d74023f8f","modified":1481552948587},{"_id":"public/css/vegas.min.css","hash":"5810e20875386f98565b69de5ca8ee1d0a6d1feb","modified":1481552948587},{"_id":"public/css/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1481552948587},{"_id":"public/js/random.js","hash":"e678af85471e86c3e25a7709496d517e50008d7a","modified":1481552948587},{"_id":"public/js/jquery.mousewheel.pack.js","hash":"1e1b44eb7cfade680c52d8748846425ecd809bfd","modified":1481552948587},{"_id":"public/js/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1481552948587},{"_id":"public/js/vegas.min.js","hash":"8d24ba5346a600c4b77ef39d68ee924ab48c8790","modified":1481552948587},{"_id":"public/css/iconfont/iconfont.css","hash":"4d5f8113307fbb6b99df0ee2ce5817af5137ec27","modified":1481552948587},{"_id":"public/2016/10/06/graphite/index.html","hash":"d4feaa8c10470e27af0b5b378394a6afedded428","modified":1481552948587},{"_id":"public/2016/07/14/hmm-memm-crf/index.html","hash":"9eabd234f3f18bd8dad338b2e2db70cbedc7a827","modified":1481552948588},{"_id":"public/2016/06/29/zhihu-link/index.html","hash":"453ec7c43c877a70a23f79761e58e27a5fd2bd5d","modified":1481552948588},{"_id":"public/js/highlight.pack.js","hash":"87868df3be2e575e47d6218ea08c0e5922808318","modified":1481552948588},{"_id":"public/js/plyr.js","hash":"11e09c25a5821fc08880b8ab1f691ad58780bde3","modified":1481552948588},{"_id":"public/js/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1481552948588},{"_id":"public/js/jquery-2.2.3.min.js","hash":"e3dbb65f2b541d842b50d37304b0102a2d5f2387","modified":1481552948588}],"Category":[{"name":"Crawler","_id":"ciwm6dce90005dgc5s52csjon"},{"name":"Reviews","_id":"ciwm6dcel000bdgc5h6gu3fe2"},{"name":"Site Est","_id":"ciwm6dceq000fdgc5ajjj227m"},{"name":"Fun","_id":"ciwm6dcet000idgc5pmddylxv"},{"name":"NLP","_id":"ciwm6dcez000mdgc5oxruzfr0"}],"Data":[],"Page":[{"title":"About","date":"2016-06-12T11:19:01.000Z","type":"about","comments":0,"photos":["https://avatars3.githubusercontent.com/u/16516510?v=3&s=460"],"_content":"\n# About Me\nMy name is Haotian Wu(吴昊天). I'm a master student in [School of Computer Science & Engineering](http://cse.seu.edu.cn/en/index.html) at [Southeast University](http://www.seu.edu.cn/english/main.htm) and will graduate in Summer 2018.\n* Research Interests:\n  * Web Crawler\n  * Data Mining & Visualization\n  * Ad Hoc Networks\n* Sports: \n  * Badminton\n  * Swimming\n\n# Contact\nJust to avoid spams, here is the Base64 code of my e-mail address: \n`aGFvdGlhbnd1c2V1QGdtYWlsLmNvbQ==`","source":"about/index.md","raw":"---\ntitle: About\ndate: 2016-06-12 19:19:01\ntype: \"about\"\ncomments: false\nphotos:\n - https://avatars3.githubusercontent.com/u/16516510?v=3&s=460\n---\n\n# About Me\nMy name is Haotian Wu(吴昊天). I'm a master student in [School of Computer Science & Engineering](http://cse.seu.edu.cn/en/index.html) at [Southeast University](http://www.seu.edu.cn/english/main.htm) and will graduate in Summer 2018.\n* Research Interests:\n  * Web Crawler\n  * Data Mining & Visualization\n  * Ad Hoc Networks\n* Sports: \n  * Badminton\n  * Swimming\n\n# Contact\nJust to avoid spams, here is the Base64 code of my e-mail address: \n`aGFvdGlhbnd1c2V1QGdtYWlsLmNvbQ==`","updated":"2016-06-17T01:05:57.734Z","path":"about/index.html","layout":"page","_id":"ciwm6dce20001dgc50f2uubok","content":"<h1 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h1><p>My name is Haotian Wu(吴昊天). I’m a master student in <a href=\"http://cse.seu.edu.cn/en/index.html\" target=\"_blank\" rel=\"external\">School of Computer Science &amp; Engineering</a> at <a href=\"http://www.seu.edu.cn/english/main.htm\" target=\"_blank\" rel=\"external\">Southeast University</a> and will graduate in Summer 2018.</p>\n<ul>\n<li>Research Interests:<ul>\n<li>Web Crawler</li>\n<li>Data Mining &amp; Visualization</li>\n<li>Ad Hoc Networks</li>\n</ul>\n</li>\n<li>Sports: <ul>\n<li>Badminton</li>\n<li>Swimming</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>Just to avoid spams, here is the Base64 code of my e-mail address:<br><code>aGFvdGlhbnd1c2V1QGdtYWlsLmNvbQ==</code></p>\n","excerpt":"","more":"<h1 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h1><p>My name is Haotian Wu(吴昊天). I’m a master student in <a href=\"http://cse.seu.edu.cn/en/index.html\">School of Computer Science &amp; Engineering</a> at <a href=\"http://www.seu.edu.cn/english/main.htm\">Southeast University</a> and will graduate in Summer 2018.</p>\n<ul>\n<li>Research Interests:<ul>\n<li>Web Crawler</li>\n<li>Data Mining &amp; Visualization</li>\n<li>Ad Hoc Networks</li>\n</ul>\n</li>\n<li>Sports: <ul>\n<li>Badminton</li>\n<li>Swimming</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>Just to avoid spams, here is the Base64 code of my e-mail address:<br><code>aGFvdGlhbnd1c2V1QGdtYWlsLmNvbQ==</code></p>\n"},{"title":"categories","date":"2016-06-11T13:59:08.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-06-11 21:59:08\ntype: \"categories\"\ncomments: false\n---\n","updated":"2016-06-17T01:05:57.734Z","path":"categories/index.html","layout":"page","_id":"ciwm6dce60003dgc5h3pxxuyn","content":"","excerpt":"","more":""},{"title":"tags","date":"2016-06-11T13:57:52.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2016-06-11 21:57:52\ntype: \"tags\"\ncomments: false\n---\n","updated":"2016-06-17T01:05:57.735Z","path":"tags/index.html","layout":"page","_id":"ciwm6dcec0007dgc5qgsc384h","content":"","excerpt":"","more":""}],"Post":[{"title":"CentOS下使用Graphite监测Scrapy","date":"2016-10-06T13:43:19.000Z","comments":1,"_content":"一直在做分布式爬虫的项目。项目都是要验收的，给领导验收就需要一些可视化的呈现，因为爬虫本身就算你采用了各种了不得的技术，命令行一启动什么都没有的看，控制台的输出和日志不是程序员估计也不会有什么兴趣。\n\n受github上一个前人的[爬虫项目](https://github.com/gnemoug/distribute_crawler)的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。\n\n# Graphite简介\nGraphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率,RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite 自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。\n\n简单来说，Graphite主要做两件事情：\n- 实时监控第三方工具传来的数据\n- 根据数据绘制图形\n\nGraphite包含3个组件，carbon，whisper，graphite webapp其中：\n- carbon\t- 用于监控数据的 Twisted 守护进程\n- whisper\t- 用于存放和操作数据的库\n- graphite webapp\t- 用于绘制图形的Django webapp\n\n关于Graphite的详细官方文档可以参考[Graphite Documentation](http://graphite.readthedocs.io/en/latest/)。\n\n# Graphite安装\nGraphite的安装，我更多地参考了这一篇博客[用graphite diamond做监控](https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4)。\n\n首先，我的安装环境是CentOS 6.6，Python2.7.10。\n\n在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：\n```sh\npip install whisper\npip install carbon\npip install graphite-web\n```\n这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。\n\n启动carbon，carbon会在默认的2003端口接收数据。\n```sh\npython /opt/graphite/bin/carbon-cache.py start\n```\n\n启动django，即整个Graphite的web应用。\n```sh\npython /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222\n```\n其中的12222号端口可以自己任意修改。\n这样浏览器打开`http://127.0.0.1:12222`就可以看到Graphite的界面。 \n\n**然而启动django这一步我在运行的时候遇到了各种错误。**\n- 在我`pip install`的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。\n- 然而我`pip install django==1.9`之后，还是遇到了\n  - no such table: auth_user\n  - no such table: account_profile\n  - Unknown command: 'syncdb'\n\n  这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。\n- 这些错误都是因为django版本不兼容导致的，在我`pip install django==1.8`之后，整个世界就清静了。\n\n如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！\nGraphite界面会提示`import cairo`出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，`pip install cairo`之后你终于能松一口气欣赏一下Graphite了。\n\n页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在`/opt/graphite/storage/whisper`。\n\n另外，Graphite有个**时区设置**的问题，如果不更改，你的时间显示都是GMT时间。只需将`/opt/graphite/webapp/graphite/local_settings.py`文件里的TIME_ZONE配置改成如下：\n```python\n# Set your local timezone (Django's default is America/Chicago)\n# If your graphs appear to be offset by a couple hours then this probably\n# needs to be explicitly set to your local timezone.\nTIME_ZONE = 'Asia/Shanghai'\n```\n\n如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：\n- [Graphite](https://github.com/springside/springside4/wiki/Graphite)\n- [Graphite监控新手入门 ](http://m.linuxeden.com/wap.php?action=article&id=159746)\n\n# Graphite与Scrapy的结合\n结合的方法详见原作者项目中[graphite.py](https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py)文件中的注释，我总结为一下几点：\n- 把`/opt/graphite/webapp/content/js/composer_widgets.js`文件中`toggleAutoRefresh`函数里的interval变量从60改为1。\n- 在配置文件`storage-aggregation.conf`里添加\n```sh\n[scrapy_min]\npattern = ^scrapy\\..*_min$\nxFilesFactor = 0.1\naggregationMethod = min\n[scrapy_max]\npattern = ^scrapy\\..*_max$\nxFilesFactor = 0.1\naggregationMethod = max\n[scrapy_sum]\npattern = ^scrapy\\..*_count$\nxFilesFactor = 0.1\naggregationMethod = sum\n```\n- 在爬虫的配置文件`setting.py`里添加\n```python\nSTATS_CLASS = 'scrapygraphite.GraphiteStatsCollector'\nGRAPHITE_HOST = '127.0.0.1'\nGRAPHITE_PORT = 2003\n```\n- 后两点是我自己的修改。\nScrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让Scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在Scrapy源码的`scrapy/extensions/logstats.py`文件中添加了两个状态变量的发送。\n```python\ndef spider_opened(self, spider):\n    self.pagesprev = 0\n    self.itemsprev = 0\n\n    self.task = task.LoopingCall(self.log, spider)\n    self.task.start(self.interval)\n\n    self.stats.set_value('pages_min', 0, spider=spider)\n    self.stats.set_value('items_min', 0, spider=spider)\n\ndef log(self, spider):\n    items = self.stats.get_value('item_scraped_count', 0)\n    pages = self.stats.get_value('response_received_count', 0)\n    irate = (items - self.itemsprev) * self.multiplier\n    prate = (pages - self.pagesprev) * self.multiplier\n    self.pagesprev, self.itemsprev = pages, items\n\n    msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n           \"scraped %(items)d items (at %(itemrate)d items/min)\")\n    log_args = {'pages': pages, 'pagerate': prate,\n                'items': items, 'itemrate': irate}\n    logger.info(msg, log_args, extra={'spider': spider})\n    self.stats.set_value('pages_min', prate, spider=spider)\n    self.stats.set_value('items_min', irate, spider=spider)\n    states = self.stats.get_stats()\n    for key in states:\n        self.stats._set_value(key, states[key], spider=spider) \n```\n  这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置`LOGSTATS_INTERVAL`的值。因为Scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。\n- 在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。\n```\n[default_1min_for_1day]\npattern = .*\nretentions = 60s:1d\n```\n  但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。\n\n# 最后\n贴一个Graphite的效果图：\n![Graphite](/uploads/img/20161006/graphite.png)\n如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤[graph-index](https://github.com/douban/graph-index)。","source":"_posts/graphite.md","raw":"title: CentOS下使用Graphite监测Scrapy\ndate: 2016-10-06 21:43:19\ncomments: true\ntags: \n - Graphite\n - Scrapy\n - CentOS\n - python\ncategories: Crawler\n---\n一直在做分布式爬虫的项目。项目都是要验收的，给领导验收就需要一些可视化的呈现，因为爬虫本身就算你采用了各种了不得的技术，命令行一启动什么都没有的看，控制台的输出和日志不是程序员估计也不会有什么兴趣。\n\n受github上一个前人的[爬虫项目](https://github.com/gnemoug/distribute_crawler)的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。\n\n# Graphite简介\nGraphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率,RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite 自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。\n\n简单来说，Graphite主要做两件事情：\n- 实时监控第三方工具传来的数据\n- 根据数据绘制图形\n\nGraphite包含3个组件，carbon，whisper，graphite webapp其中：\n- carbon\t- 用于监控数据的 Twisted 守护进程\n- whisper\t- 用于存放和操作数据的库\n- graphite webapp\t- 用于绘制图形的Django webapp\n\n关于Graphite的详细官方文档可以参考[Graphite Documentation](http://graphite.readthedocs.io/en/latest/)。\n\n# Graphite安装\nGraphite的安装，我更多地参考了这一篇博客[用graphite diamond做监控](https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4)。\n\n首先，我的安装环境是CentOS 6.6，Python2.7.10。\n\n在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：\n```sh\npip install whisper\npip install carbon\npip install graphite-web\n```\n这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。\n\n启动carbon，carbon会在默认的2003端口接收数据。\n```sh\npython /opt/graphite/bin/carbon-cache.py start\n```\n\n启动django，即整个Graphite的web应用。\n```sh\npython /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222\n```\n其中的12222号端口可以自己任意修改。\n这样浏览器打开`http://127.0.0.1:12222`就可以看到Graphite的界面。 \n\n**然而启动django这一步我在运行的时候遇到了各种错误。**\n- 在我`pip install`的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。\n- 然而我`pip install django==1.9`之后，还是遇到了\n  - no such table: auth_user\n  - no such table: account_profile\n  - Unknown command: 'syncdb'\n\n  这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。\n- 这些错误都是因为django版本不兼容导致的，在我`pip install django==1.8`之后，整个世界就清静了。\n\n如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！\nGraphite界面会提示`import cairo`出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，`pip install cairo`之后你终于能松一口气欣赏一下Graphite了。\n\n页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在`/opt/graphite/storage/whisper`。\n\n另外，Graphite有个**时区设置**的问题，如果不更改，你的时间显示都是GMT时间。只需将`/opt/graphite/webapp/graphite/local_settings.py`文件里的TIME_ZONE配置改成如下：\n```python\n# Set your local timezone (Django's default is America/Chicago)\n# If your graphs appear to be offset by a couple hours then this probably\n# needs to be explicitly set to your local timezone.\nTIME_ZONE = 'Asia/Shanghai'\n```\n\n如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：\n- [Graphite](https://github.com/springside/springside4/wiki/Graphite)\n- [Graphite监控新手入门 ](http://m.linuxeden.com/wap.php?action=article&id=159746)\n\n# Graphite与Scrapy的结合\n结合的方法详见原作者项目中[graphite.py](https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py)文件中的注释，我总结为一下几点：\n- 把`/opt/graphite/webapp/content/js/composer_widgets.js`文件中`toggleAutoRefresh`函数里的interval变量从60改为1。\n- 在配置文件`storage-aggregation.conf`里添加\n```sh\n[scrapy_min]\npattern = ^scrapy\\..*_min$\nxFilesFactor = 0.1\naggregationMethod = min\n[scrapy_max]\npattern = ^scrapy\\..*_max$\nxFilesFactor = 0.1\naggregationMethod = max\n[scrapy_sum]\npattern = ^scrapy\\..*_count$\nxFilesFactor = 0.1\naggregationMethod = sum\n```\n- 在爬虫的配置文件`setting.py`里添加\n```python\nSTATS_CLASS = 'scrapygraphite.GraphiteStatsCollector'\nGRAPHITE_HOST = '127.0.0.1'\nGRAPHITE_PORT = 2003\n```\n- 后两点是我自己的修改。\nScrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让Scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在Scrapy源码的`scrapy/extensions/logstats.py`文件中添加了两个状态变量的发送。\n```python\ndef spider_opened(self, spider):\n    self.pagesprev = 0\n    self.itemsprev = 0\n\n    self.task = task.LoopingCall(self.log, spider)\n    self.task.start(self.interval)\n\n    self.stats.set_value('pages_min', 0, spider=spider)\n    self.stats.set_value('items_min', 0, spider=spider)\n\ndef log(self, spider):\n    items = self.stats.get_value('item_scraped_count', 0)\n    pages = self.stats.get_value('response_received_count', 0)\n    irate = (items - self.itemsprev) * self.multiplier\n    prate = (pages - self.pagesprev) * self.multiplier\n    self.pagesprev, self.itemsprev = pages, items\n\n    msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n           \"scraped %(items)d items (at %(itemrate)d items/min)\")\n    log_args = {'pages': pages, 'pagerate': prate,\n                'items': items, 'itemrate': irate}\n    logger.info(msg, log_args, extra={'spider': spider})\n    self.stats.set_value('pages_min', prate, spider=spider)\n    self.stats.set_value('items_min', irate, spider=spider)\n    states = self.stats.get_stats()\n    for key in states:\n        self.stats._set_value(key, states[key], spider=spider) \n```\n  这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置`LOGSTATS_INTERVAL`的值。因为Scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。\n- 在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。\n```\n[default_1min_for_1day]\npattern = .*\nretentions = 60s:1d\n```\n  但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。\n\n# 最后\n贴一个Graphite的效果图：\n![Graphite](/uploads/img/20161006/graphite.png)\n如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤[graph-index](https://github.com/douban/graph-index)。","slug":"graphite","published":1,"updated":"2016-10-08T07:53:36.699Z","layout":"post","photos":[],"link":"","_id":"ciwm6dcdv0000dgc5gldjqowe","content":"<p>一直在做分布式爬虫的项目。项目都是要验收的，给领导验收就需要一些可视化的呈现，因为爬虫本身就算你采用了各种了不得的技术，命令行一启动什么都没有的看，控制台的输出和日志不是程序员估计也不会有什么兴趣。</p>\n<p>受github上一个前人的<a href=\"https://github.com/gnemoug/distribute_crawler\" target=\"_blank\" rel=\"external\">爬虫项目</a>的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。</p>\n<h1 id=\"Graphite简介\"><a href=\"#Graphite简介\" class=\"headerlink\" title=\"Graphite简介\"></a>Graphite简介</h1><p>Graphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率,RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite 自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。</p>\n<p>简单来说，Graphite主要做两件事情：</p>\n<ul>\n<li>实时监控第三方工具传来的数据</li>\n<li>根据数据绘制图形</li>\n</ul>\n<p>Graphite包含3个组件，carbon，whisper，graphite webapp其中：</p>\n<ul>\n<li>carbon    - 用于监控数据的 Twisted 守护进程</li>\n<li>whisper    - 用于存放和操作数据的库</li>\n<li>graphite webapp    - 用于绘制图形的Django webapp</li>\n</ul>\n<p>关于Graphite的详细官方文档可以参考<a href=\"http://graphite.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"external\">Graphite Documentation</a>。</p>\n<h1 id=\"Graphite安装\"><a href=\"#Graphite安装\" class=\"headerlink\" title=\"Graphite安装\"></a>Graphite安装</h1><p>Graphite的安装，我更多地参考了这一篇博客<a href=\"https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4\" target=\"_blank\" rel=\"external\">用graphite diamond做监控</a>。</p>\n<p>首先，我的安装环境是CentOS 6.6，Python2.7.10。</p>\n<p>在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install whisper</span><br><span class=\"line\">pip install carbon</span><br><span class=\"line\">pip install graphite-web</span><br></pre></td></tr></table></figure></p>\n<p>这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。</p>\n<p>启动carbon，carbon会在默认的2003端口接收数据。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/bin/carbon-cache.py start</span><br></pre></td></tr></table></figure></p>\n<p>启动django，即整个Graphite的web应用。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222</span><br></pre></td></tr></table></figure></p>\n<p>其中的12222号端口可以自己任意修改。<br>这样浏览器打开<code>http://127.0.0.1:12222</code>就可以看到Graphite的界面。 </p>\n<p><strong>然而启动django这一步我在运行的时候遇到了各种错误。</strong></p>\n<ul>\n<li>在我<code>pip install</code>的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。</li>\n<li><p>然而我<code>pip install django==1.9</code>之后，还是遇到了</p>\n<ul>\n<li>no such table: auth_user</li>\n<li>no such table: account_profile</li>\n<li>Unknown command: ‘syncdb’</li>\n</ul>\n<p>这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。</p>\n</li>\n<li>这些错误都是因为django版本不兼容导致的，在我<code>pip install django==1.8</code>之后，整个世界就清静了。</li>\n</ul>\n<p>如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！<br>Graphite界面会提示<code>import cairo</code>出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，<code>pip install cairo</code>之后你终于能松一口气欣赏一下Graphite了。</p>\n<p>页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在<code>/opt/graphite/storage/whisper</code>。</p>\n<p>另外，Graphite有个<strong>时区设置</strong>的问题，如果不更改，你的时间显示都是GMT时间。只需将<code>/opt/graphite/webapp/graphite/local_settings.py</code>文件里的TIME_ZONE配置改成如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Set your local timezone (Django's default is America/Chicago)</span></span><br><span class=\"line\"><span class=\"comment\"># If your graphs appear to be offset by a couple hours then this probably</span></span><br><span class=\"line\"><span class=\"comment\"># needs to be explicitly set to your local timezone.</span></span><br><span class=\"line\">TIME_ZONE = <span class=\"string\">'Asia/Shanghai'</span></span><br></pre></td></tr></table></figure></p>\n<p>如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：</p>\n<ul>\n<li><a href=\"https://github.com/springside/springside4/wiki/Graphite\" target=\"_blank\" rel=\"external\">Graphite</a></li>\n<li><a href=\"http://m.linuxeden.com/wap.php?action=article&amp;id=159746\" target=\"_blank\" rel=\"external\">Graphite监控新手入门 </a></li>\n</ul>\n<h1 id=\"Graphite与Scrapy的结合\"><a href=\"#Graphite与Scrapy的结合\" class=\"headerlink\" title=\"Graphite与Scrapy的结合\"></a>Graphite与Scrapy的结合</h1><p>结合的方法详见原作者项目中<a href=\"https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py\" target=\"_blank\" rel=\"external\">graphite.py</a>文件中的注释，我总结为一下几点：</p>\n<ul>\n<li>把<code>/opt/graphite/webapp/content/js/composer_widgets.js</code>文件中<code>toggleAutoRefresh</code>函数里的interval变量从60改为1。</li>\n<li><p>在配置文件<code>storage-aggregation.conf</code>里添加</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[scrapy_min]</span><br><span class=\"line\">pattern = ^scrapy\\..*_min$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = min</span><br><span class=\"line\">[scrapy_max]</span><br><span class=\"line\">pattern = ^scrapy\\..*_max$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = max</span><br><span class=\"line\">[scrapy_sum]</span><br><span class=\"line\">pattern = ^scrapy\\..*_count$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = sum</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在爬虫的配置文件<code>setting.py</code>里添加</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">STATS_CLASS = <span class=\"string\">'scrapygraphite.GraphiteStatsCollector'</span></span><br><span class=\"line\">GRAPHITE_HOST = <span class=\"string\">'127.0.0.1'</span></span><br><span class=\"line\">GRAPHITE_PORT = <span class=\"number\">2003</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>后两点是我自己的修改。<br>Scrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让Scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在Scrapy源码的<code>scrapy/extensions/logstats.py</code>文件中添加了两个状态变量的发送。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spider_opened</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    self.pagesprev = <span class=\"number\">0</span></span><br><span class=\"line\">    self.itemsprev = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    self.task = task.LoopingCall(self.log, spider)</span><br><span class=\"line\">    self.task.start(self.interval)</span><br><span class=\"line\"></span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">log</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    items = self.stats.get_value(<span class=\"string\">'item_scraped_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    pages = self.stats.get_value(<span class=\"string\">'response_received_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    irate = (items - self.itemsprev) * self.multiplier</span><br><span class=\"line\">    prate = (pages - self.pagesprev) * self.multiplier</span><br><span class=\"line\">    self.pagesprev, self.itemsprev = pages, items</span><br><span class=\"line\"></span><br><span class=\"line\">    msg = (<span class=\"string\">\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"</span></span><br><span class=\"line\">           <span class=\"string\">\"scraped %(items)d items (at %(itemrate)d items/min)\"</span>)</span><br><span class=\"line\">    log_args = &#123;<span class=\"string\">'pages'</span>: pages, <span class=\"string\">'pagerate'</span>: prate,</span><br><span class=\"line\">                <span class=\"string\">'items'</span>: items, <span class=\"string\">'itemrate'</span>: irate&#125;</span><br><span class=\"line\">    logger.info(msg, log_args, extra=&#123;<span class=\"string\">'spider'</span>: spider&#125;)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, prate, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, irate, spider=spider)</span><br><span class=\"line\">    states = self.stats.get_stats()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> states:</span><br><span class=\"line\">        self.stats._set_value(key, states[key], spider=spider)</span><br></pre></td></tr></table></figure>\n<p>这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置<code>LOGSTATS_INTERVAL</code>的值。因为Scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。</p>\n</li>\n<li><p>在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[default_1min_for_1day]</span><br><span class=\"line\">pattern = .*</span><br><span class=\"line\">retentions = 60s:1d</span><br></pre></td></tr></table></figure>\n<p>但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。</p>\n</li>\n</ul>\n<h1 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h1><p>贴一个Graphite的效果图：<br><img src=\"/uploads/img/20161006/graphite.png\" alt=\"Graphite\"><br>如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤<a href=\"https://github.com/douban/graph-index\" target=\"_blank\" rel=\"external\">graph-index</a>。</p>\n","excerpt":"","more":"<p>一直在做分布式爬虫的项目。项目都是要验收的，给领导验收就需要一些可视化的呈现，因为爬虫本身就算你采用了各种了不得的技术，命令行一启动什么都没有的看，控制台的输出和日志不是程序员估计也不会有什么兴趣。</p>\n<p>受github上一个前人的<a href=\"https://github.com/gnemoug/distribute_crawler\">爬虫项目</a>的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。</p>\n<h1 id=\"Graphite简介\"><a href=\"#Graphite简介\" class=\"headerlink\" title=\"Graphite简介\"></a>Graphite简介</h1><p>Graphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率,RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite 自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。</p>\n<p>简单来说，Graphite主要做两件事情：</p>\n<ul>\n<li>实时监控第三方工具传来的数据</li>\n<li>根据数据绘制图形</li>\n</ul>\n<p>Graphite包含3个组件，carbon，whisper，graphite webapp其中：</p>\n<ul>\n<li>carbon    - 用于监控数据的 Twisted 守护进程</li>\n<li>whisper    - 用于存放和操作数据的库</li>\n<li>graphite webapp    - 用于绘制图形的Django webapp</li>\n</ul>\n<p>关于Graphite的详细官方文档可以参考<a href=\"http://graphite.readthedocs.io/en/latest/\">Graphite Documentation</a>。</p>\n<h1 id=\"Graphite安装\"><a href=\"#Graphite安装\" class=\"headerlink\" title=\"Graphite安装\"></a>Graphite安装</h1><p>Graphite的安装，我更多地参考了这一篇博客<a href=\"https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4\">用graphite diamond做监控</a>。</p>\n<p>首先，我的安装环境是CentOS 6.6，Python2.7.10。</p>\n<p>在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install whisper</span><br><span class=\"line\">pip install carbon</span><br><span class=\"line\">pip install graphite-web</span><br></pre></td></tr></table></figure></p>\n<p>这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。</p>\n<p>启动carbon，carbon会在默认的2003端口接收数据。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/bin/carbon-cache.py start</span><br></pre></td></tr></table></figure></p>\n<p>启动django，即整个Graphite的web应用。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222</span><br></pre></td></tr></table></figure></p>\n<p>其中的12222号端口可以自己任意修改。<br>这样浏览器打开<code>http://127.0.0.1:12222</code>就可以看到Graphite的界面。 </p>\n<p><strong>然而启动django这一步我在运行的时候遇到了各种错误。</strong></p>\n<ul>\n<li>在我<code>pip install</code>的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。</li>\n<li><p>然而我<code>pip install django==1.9</code>之后，还是遇到了</p>\n<ul>\n<li>no such table: auth_user</li>\n<li>no such table: account_profile</li>\n<li>Unknown command: ‘syncdb’</li>\n</ul>\n<p>这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。</p>\n</li>\n<li>这些错误都是因为django版本不兼容导致的，在我<code>pip install django==1.8</code>之后，整个世界就清静了。</li>\n</ul>\n<p>如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！<br>Graphite界面会提示<code>import cairo</code>出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，<code>pip install cairo</code>之后你终于能松一口气欣赏一下Graphite了。</p>\n<p>页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在<code>/opt/graphite/storage/whisper</code>。</p>\n<p>另外，Graphite有个<strong>时区设置</strong>的问题，如果不更改，你的时间显示都是GMT时间。只需将<code>/opt/graphite/webapp/graphite/local_settings.py</code>文件里的TIME_ZONE配置改成如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Set your local timezone (Django's default is America/Chicago)</span></span><br><span class=\"line\"><span class=\"comment\"># If your graphs appear to be offset by a couple hours then this probably</span></span><br><span class=\"line\"><span class=\"comment\"># needs to be explicitly set to your local timezone.</span></span><br><span class=\"line\">TIME_ZONE = <span class=\"string\">'Asia/Shanghai'</span></span><br></pre></td></tr></table></figure></p>\n<p>如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：</p>\n<ul>\n<li><a href=\"https://github.com/springside/springside4/wiki/Graphite\">Graphite</a></li>\n<li><a href=\"http://m.linuxeden.com/wap.php?action=article&amp;id=159746\">Graphite监控新手入门 </a></li>\n</ul>\n<h1 id=\"Graphite与Scrapy的结合\"><a href=\"#Graphite与Scrapy的结合\" class=\"headerlink\" title=\"Graphite与Scrapy的结合\"></a>Graphite与Scrapy的结合</h1><p>结合的方法详见原作者项目中<a href=\"https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py\">graphite.py</a>文件中的注释，我总结为一下几点：</p>\n<ul>\n<li>把<code>/opt/graphite/webapp/content/js/composer_widgets.js</code>文件中<code>toggleAutoRefresh</code>函数里的interval变量从60改为1。</li>\n<li><p>在配置文件<code>storage-aggregation.conf</code>里添加</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[scrapy_min]</span><br><span class=\"line\">pattern = ^scrapy\\..*_min$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = min</span><br><span class=\"line\">[scrapy_max]</span><br><span class=\"line\">pattern = ^scrapy\\..*_max$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = max</span><br><span class=\"line\">[scrapy_sum]</span><br><span class=\"line\">pattern = ^scrapy\\..*_count$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = sum</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在爬虫的配置文件<code>setting.py</code>里添加</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">STATS_CLASS = <span class=\"string\">'scrapygraphite.GraphiteStatsCollector'</span></span><br><span class=\"line\">GRAPHITE_HOST = <span class=\"string\">'127.0.0.1'</span></span><br><span class=\"line\">GRAPHITE_PORT = <span class=\"number\">2003</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>后两点是我自己的修改。<br>Scrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让Scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在Scrapy源码的<code>scrapy/extensions/logstats.py</code>文件中添加了两个状态变量的发送。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spider_opened</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    self.pagesprev = <span class=\"number\">0</span></span><br><span class=\"line\">    self.itemsprev = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    self.task = task.LoopingCall(self.log, spider)</span><br><span class=\"line\">    self.task.start(self.interval)</span><br><span class=\"line\"></span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">log</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    items = self.stats.get_value(<span class=\"string\">'item_scraped_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    pages = self.stats.get_value(<span class=\"string\">'response_received_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    irate = (items - self.itemsprev) * self.multiplier</span><br><span class=\"line\">    prate = (pages - self.pagesprev) * self.multiplier</span><br><span class=\"line\">    self.pagesprev, self.itemsprev = pages, items</span><br><span class=\"line\"></span><br><span class=\"line\">    msg = (<span class=\"string\">\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"</span></span><br><span class=\"line\">           <span class=\"string\">\"scraped %(items)d items (at %(itemrate)d items/min)\"</span>)</span><br><span class=\"line\">    log_args = &#123;<span class=\"string\">'pages'</span>: pages, <span class=\"string\">'pagerate'</span>: prate,</span><br><span class=\"line\">                <span class=\"string\">'items'</span>: items, <span class=\"string\">'itemrate'</span>: irate&#125;</span><br><span class=\"line\">    logger.info(msg, log_args, extra=&#123;<span class=\"string\">'spider'</span>: spider&#125;)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, prate, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, irate, spider=spider)</span><br><span class=\"line\">    states = self.stats.get_stats()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> states:</span><br><span class=\"line\">        self.stats._set_value(key, states[key], spider=spider)</span><br></pre></td></tr></table></figure>\n<p>这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置<code>LOGSTATS_INTERVAL</code>的值。因为Scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。</p>\n</li>\n<li><p>在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[default_1min_for_1day]</span><br><span class=\"line\">pattern = .*</span><br><span class=\"line\">retentions = 60s:1d</span><br></pre></td></tr></table></figure>\n<p>但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。</p>\n</li>\n</ul>\n<h1 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h1><p>贴一个Graphite的效果图：<br><img src=\"/uploads/img/20161006/graphite.png\" alt=\"Graphite\"><br>如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤<a href=\"https://github.com/douban/graph-index\">graph-index</a>。</p>\n"},{"title":"CentOS 6.5 下scrapy与ghost.py的安装干货","date":"2016-06-16T05:34:08.000Z","comments":1,"_content":"之前一直在做Scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。\n在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：\n* [Python 爬虫如何获取 JS 生成的 URL 和网页内容？](https://www.zhihu.com/question/21471960)\n* [Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？](https://www.zhihu.com/question/36450326)\n\n一些前人的技术博客如：\n* 开源中国上[斑ban](http://my.oschina.net/u/1024140?ft=blog)的[《使用python，scrapy写（定制）爬虫的经验，资料，杂》](http://my.oschina.net/u/1024140/blog/188154)。\n这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。\n\n上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。\n\nghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对**PyQt4**或者**PySide**的一个封装，需要安装其中一个才能运行。\n\n# PySide\n当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。\n下面是我在CentOS 6.5和python2.7.11的环境上安装Scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF[源地址](http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf)。\n{% pdf http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf %}\n上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。\n\n# PyQt4\n我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。\n\n安装PyQt4之前是需要安装**SIP**的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。\n\n在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。\n贴一个能够成功安装的博客链接：[CentOS7.1下python2.7.10安装PyQt4](http://blog.csdn.net/dgatiger/article/details/50331361)\n\n文中SIP的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz\ntar xvf sip-4.17.tar.gz\ncd sip-4.17\npython configure.py\nmake & make install & make clean\n```\nPyQt的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz\ntar xvf PyQt-x11-gpl-4.11.4.tar.gz\ncd PyQt-x11-gpl-4.11.4\npython configure.py -q  /usr/lib64/qt4/bin/qmake\nmake & make install & make clean\n```\n这篇博客里用的是**sip-4.17**和**PyQt-4.11.4**是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。\n","source":"_posts/centos-scrapy-ghost.py.md","raw":"title: CentOS 6.5 下scrapy与ghost.py的安装干货\ndate: 2016-06-16 13:34:08\ncomments: true\ntags: \n - CentOS\n - Scrapy\n - ghost.py\n - python\ncategories: Reviews\n---\n之前一直在做Scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。\n在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：\n* [Python 爬虫如何获取 JS 生成的 URL 和网页内容？](https://www.zhihu.com/question/21471960)\n* [Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？](https://www.zhihu.com/question/36450326)\n\n一些前人的技术博客如：\n* 开源中国上[斑ban](http://my.oschina.net/u/1024140?ft=blog)的[《使用python，scrapy写（定制）爬虫的经验，资料，杂》](http://my.oschina.net/u/1024140/blog/188154)。\n这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。\n\n上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。\n\nghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对**PyQt4**或者**PySide**的一个封装，需要安装其中一个才能运行。\n\n# PySide\n当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。\n下面是我在CentOS 6.5和python2.7.11的环境上安装Scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF[源地址](http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf)。\n{% pdf http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf %}\n上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。\n\n# PyQt4\n我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。\n\n安装PyQt4之前是需要安装**SIP**的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。\n\n在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。\n贴一个能够成功安装的博客链接：[CentOS7.1下python2.7.10安装PyQt4](http://blog.csdn.net/dgatiger/article/details/50331361)\n\n文中SIP的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz\ntar xvf sip-4.17.tar.gz\ncd sip-4.17\npython configure.py\nmake & make install & make clean\n```\nPyQt的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz\ntar xvf PyQt-x11-gpl-4.11.4.tar.gz\ncd PyQt-x11-gpl-4.11.4\npython configure.py -q  /usr/lib64/qt4/bin/qmake\nmake & make install & make clean\n```\n这篇博客里用的是**sip-4.17**和**PyQt-4.11.4**是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。\n","slug":"centos-scrapy-ghost.py","published":1,"updated":"2016-08-18T13:49:08.002Z","layout":"post","photos":[],"link":"","_id":"ciwm6dce30002dgc5i6iiqtdn","content":"<p>之前一直在做Scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。<br>在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/21471960\" target=\"_blank\" rel=\"external\">Python 爬虫如何获取 JS 生成的 URL 和网页内容？</a></li>\n<li><a href=\"https://www.zhihu.com/question/36450326\" target=\"_blank\" rel=\"external\">Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？</a></li>\n</ul>\n<p>一些前人的技术博客如：</p>\n<ul>\n<li>开源中国上<a href=\"http://my.oschina.net/u/1024140?ft=blog\" target=\"_blank\" rel=\"external\">斑ban</a>的<a href=\"http://my.oschina.net/u/1024140/blog/188154\" target=\"_blank\" rel=\"external\">《使用python，scrapy写（定制）爬虫的经验，资料，杂》</a>。<br>这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。</li>\n</ul>\n<p>上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。</p>\n<p>ghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对<strong>PyQt4</strong>或者<strong>PySide</strong>的一个封装，需要安装其中一个才能运行。</p>\n<h1 id=\"PySide\"><a href=\"#PySide\" class=\"headerlink\" title=\"PySide\"></a>PySide</h1><p>当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。<br>下面是我在CentOS 6.5和python2.7.11的环境上安装Scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF<a href=\"http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf\">源地址</a>。<br>\n\n\t<div class=\"row\">\n\t  <iframe src=\"http://nagland.github.io/viewer/web/viewer.html?val=http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf\" style=\"width:100%; height:900px\"></iframe>\n\t</div>\n\n\n<br>上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。</p>\n<h1 id=\"PyQt4\"><a href=\"#PyQt4\" class=\"headerlink\" title=\"PyQt4\"></a>PyQt4</h1><p>我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。</p>\n<p>安装PyQt4之前是需要安装<strong>SIP</strong>的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。</p>\n<p>在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。<br>贴一个能够成功安装的博客链接：<a href=\"http://blog.csdn.net/dgatiger/article/details/50331361\" target=\"_blank\" rel=\"external\">CentOS7.1下python2.7.10安装PyQt4</a></p>\n<p>文中SIP的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz</span><br><span class=\"line\">tar xvf sip-4.17.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> sip-4.17</span><br><span class=\"line\">python configure.py</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>PyQt的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\">tar xvf PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> PyQt-x11-gpl-4.11.4</span><br><span class=\"line\">python configure.py -q  /usr/lib64/qt4/bin/qmake</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>这篇博客里用的是<strong>sip-4.17</strong>和<strong>PyQt-4.11.4</strong>是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。</p>\n","excerpt":"","more":"<p>之前一直在做Scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。<br>在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/21471960\">Python 爬虫如何获取 JS 生成的 URL 和网页内容？</a></li>\n<li><a href=\"https://www.zhihu.com/question/36450326\">Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？</a></li>\n</ul>\n<p>一些前人的技术博客如：</p>\n<ul>\n<li>开源中国上<a href=\"http://my.oschina.net/u/1024140?ft=blog\">斑ban</a>的<a href=\"http://my.oschina.net/u/1024140/blog/188154\">《使用python，scrapy写（定制）爬虫的经验，资料，杂》</a>。<br>这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。</li>\n</ul>\n<p>上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。</p>\n<p>ghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对<strong>PyQt4</strong>或者<strong>PySide</strong>的一个封装，需要安装其中一个才能运行。</p>\n<h1 id=\"PySide\"><a href=\"#PySide\" class=\"headerlink\" title=\"PySide\"></a>PySide</h1><p>当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。<br>下面是我在CentOS 6.5和python2.7.11的环境上安装Scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF<a href=\"http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf\">源地址</a>。<br>\n\n\t<div class=\"row\">\n\t  <iframe src=\"http://nagland.github.io/viewer/web/viewer.html?val=http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf\" style=\"width:100%; height:900px\"></iframe>\n\t</div>\n\n\n<br>上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。</p>\n<h1 id=\"PyQt4\"><a href=\"#PyQt4\" class=\"headerlink\" title=\"PyQt4\"></a>PyQt4</h1><p>我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。</p>\n<p>安装PyQt4之前是需要安装<strong>SIP</strong>的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。</p>\n<p>在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。<br>贴一个能够成功安装的博客链接：<a href=\"http://blog.csdn.net/dgatiger/article/details/50331361\">CentOS7.1下python2.7.10安装PyQt4</a></p>\n<p>文中SIP的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz</span><br><span class=\"line\">tar xvf sip-4.17.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> sip-4.17</span><br><span class=\"line\">python configure.py</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>PyQt的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\">tar xvf PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> PyQt-x11-gpl-4.11.4</span><br><span class=\"line\">python configure.py -q  /usr/lib64/qt4/bin/qmake</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>这篇博客里用的是<strong>sip-4.17</strong>和<strong>PyQt-4.11.4</strong>是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。</p>\n"},{"title":"Hello World","date":"2016-06-12T13:28:52.000Z","comments":1,"photos":["/uploads/img/20160612/cover.gif"],"_content":"\nThis is my **first blog** after I established this site using [Hexo](https://hexo.io/) and [random](https://github.com/stiekel/hexo-theme-random). \nHere are some tests for Hexo:\n\n# Code Test:\n```javascript\n$ hexo clean\n$ hexo generate\n$ hexo server\n$ hexo deploy\n```\n# Mathjax Test:\n$$\\sum_{i=1}^n a_i=0$$\n\n$$\n\\begin{eqnarray}\nf(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2\n\\end{eqnarray}\n$$ \n\n$$\n\\begin{eqnarray}\n\\nabla\\cdot\\vec{E} &=& \\frac{\\rho}{\\epsilon_0} \\\\\n\\nabla\\cdot\\vec{B} &=& 0 \\\\\n\\nabla\\times\\vec{E} &=& -\\frac{\\partial B}{\\partial t} \\\\\n\\nabla\\times\\vec{B} &=& \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)\n\\end{eqnarray}\n$$\n\n# PDF Test:\n{% pdf http://tripleday.github.io/uploads/pdf/Clean%20Code.pdf %}\n\n# iFrame Test:\n{% iframe http://www.seu.edu.cn/english/main.htm 100% 500 %}\n\n# Picture Test:\n![Facebook](/uploads/img/20160612/facebook.jpg)\n\n# Youtube Test:\n{% youtube https://youtu.be/QBJxGklvHRg %}\n\n# Youku Test:\n{% youku 480 %}\nXMTU3NjExOTUwMA==\n{% endyouku %}\n\n","source":"_posts/hello-world.md","raw":"title: Hello World\ndate: 2016-06-12 21:28:52\ncomments: true\ntags: \n - English\n - Hexo\ncategories: Site Est\nphotos: \n - /uploads/img/20160612/cover.gif\n---\n\nThis is my **first blog** after I established this site using [Hexo](https://hexo.io/) and [random](https://github.com/stiekel/hexo-theme-random). \nHere are some tests for Hexo:\n\n# Code Test:\n```javascript\n$ hexo clean\n$ hexo generate\n$ hexo server\n$ hexo deploy\n```\n# Mathjax Test:\n$$\\sum_{i=1}^n a_i=0$$\n\n$$\n\\begin{eqnarray}\nf(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2\n\\end{eqnarray}\n$$ \n\n$$\n\\begin{eqnarray}\n\\nabla\\cdot\\vec{E} &=& \\frac{\\rho}{\\epsilon_0} \\\\\n\\nabla\\cdot\\vec{B} &=& 0 \\\\\n\\nabla\\times\\vec{E} &=& -\\frac{\\partial B}{\\partial t} \\\\\n\\nabla\\times\\vec{B} &=& \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)\n\\end{eqnarray}\n$$\n\n# PDF Test:\n{% pdf http://tripleday.github.io/uploads/pdf/Clean%20Code.pdf %}\n\n# iFrame Test:\n{% iframe http://www.seu.edu.cn/english/main.htm 100% 500 %}\n\n# Picture Test:\n![Facebook](/uploads/img/20160612/facebook.jpg)\n\n# Youtube Test:\n{% youtube https://youtu.be/QBJxGklvHRg %}\n\n# Youku Test:\n{% youku 480 %}\nXMTU3NjExOTUwMA==\n{% endyouku %}\n\n","slug":"hello-world","published":1,"updated":"2016-07-15T06:56:38.880Z","layout":"post","link":"","_id":"ciwm6dcea0006dgc55zrq0azk","content":"<p>This is my <strong>first blog</strong> after I established this site using <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a> and <a href=\"https://github.com/stiekel/hexo-theme-random\" target=\"_blank\" rel=\"external\">random</a>.<br>Here are some tests for Hexo:</p>\n<h1 id=\"Code-Test\"><a href=\"#Code-Test\" class=\"headerlink\" title=\"Code Test:\"></a>Code Test:</h1><figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo server</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<h1 id=\"Mathjax-Test\"><a href=\"#Mathjax-Test\" class=\"headerlink\" title=\"Mathjax Test:\"></a>Mathjax Test:</h1><p>$$\\sum_{i=1}^n a_i=0$$</p>\n<p>$$<br>\\begin{eqnarray}<br>f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2<br>\\end{eqnarray}<br>$$ </p>\n<p>$$<br>\\begin{eqnarray}<br>\\nabla\\cdot\\vec{E} &amp;=&amp; \\frac{\\rho}{\\epsilon_0} \\\\<br>\\nabla\\cdot\\vec{B} &amp;=&amp; 0 \\\\<br>\\nabla\\times\\vec{E} &amp;=&amp; -\\frac{\\partial B}{\\partial t} \\\\<br>\\nabla\\times\\vec{B} &amp;=&amp; \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)<br>\\end{eqnarray}<br>$$</p>\n<h1 id=\"PDF-Test\"><a href=\"#PDF-Test\" class=\"headerlink\" title=\"PDF Test:\"></a>PDF Test:</h1>\n\n\t<div class=\"row\">\n\t  <iframe src=\"http://nagland.github.io/viewer/web/viewer.html?val=http://tripleday.github.io/uploads/pdf/Clean%20Code.pdf\" style=\"width:100%; height:900px\"></iframe>\n\t</div>\n\n\n\n<h1 id=\"iFrame-Test\"><a href=\"#iFrame-Test\" class=\"headerlink\" title=\"iFrame Test:\"></a>iFrame Test:</h1><iframe src=\"http://www.seu.edu.cn/english/main.htm\" width=\"100%\" height=\"500\" frameborder=\"0\" allowfullscreen></iframe>\n<h1 id=\"Picture-Test\"><a href=\"#Picture-Test\" class=\"headerlink\" title=\"Picture Test:\"></a>Picture Test:</h1><p><img src=\"/uploads/img/20160612/facebook.jpg\" alt=\"Facebook\"></p>\n<h1 id=\"Youtube-Test\"><a href=\"#Youtube-Test\" class=\"headerlink\" title=\"Youtube Test:\"></a>Youtube Test:</h1><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/https://youtu.be/QBJxGklvHRg\" frameborder=\"0\" allowfullscreen></iframe></div>\n<h1 id=\"Youku-Test\"><a href=\"#Youku-Test\" class=\"headerlink\" title=\"Youku Test:\"></a>Youku Test:</h1><div class=\"video-container\"><iframe height=\"480\" width=\"100%\" src=\"http://player.youku.com/embed/XMTU3NjExOTUwMA==\" frameborder=\"0\" allowfullscreen></iframe></div>\n","excerpt":"","more":"<p>This is my <strong>first blog</strong> after I established this site using <a href=\"https://hexo.io/\">Hexo</a> and <a href=\"https://github.com/stiekel/hexo-theme-random\">random</a>.<br>Here are some tests for Hexo:</p>\n<h1 id=\"Code-Test\"><a href=\"#Code-Test\" class=\"headerlink\" title=\"Code Test:\"></a>Code Test:</h1><figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo server</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<h1 id=\"Mathjax-Test\"><a href=\"#Mathjax-Test\" class=\"headerlink\" title=\"Mathjax Test:\"></a>Mathjax Test:</h1><p>$$\\sum_{i=1}^n a_i=0$$</p>\n<p>$$<br>\\begin{eqnarray}<br>f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2<br>\\end{eqnarray}<br>$$ </p>\n<p>$$<br>\\begin{eqnarray}<br>\\nabla\\cdot\\vec{E} &amp;=&amp; \\frac{\\rho}{\\epsilon_0} \\\\<br>\\nabla\\cdot\\vec{B} &amp;=&amp; 0 \\\\<br>\\nabla\\times\\vec{E} &amp;=&amp; -\\frac{\\partial B}{\\partial t} \\\\<br>\\nabla\\times\\vec{B} &amp;=&amp; \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)<br>\\end{eqnarray}<br>$$</p>\n<h1 id=\"PDF-Test\"><a href=\"#PDF-Test\" class=\"headerlink\" title=\"PDF Test:\"></a>PDF Test:</h1>\n\n\t<div class=\"row\">\n\t  <iframe src=\"http://nagland.github.io/viewer/web/viewer.html?val=http://tripleday.github.io/uploads/pdf/Clean%20Code.pdf\" style=\"width:100%; height:900px\"></iframe>\n\t</div>\n\n\n\n<h1 id=\"iFrame-Test\"><a href=\"#iFrame-Test\" class=\"headerlink\" title=\"iFrame Test:\"></a>iFrame Test:</h1><iframe src=\"http://www.seu.edu.cn/english/main.htm\" width=\"100%\" height=\"500\" frameborder=\"0\" allowfullscreen></iframe>\n<h1 id=\"Picture-Test\"><a href=\"#Picture-Test\" class=\"headerlink\" title=\"Picture Test:\"></a>Picture Test:</h1><p><img src=\"/uploads/img/20160612/facebook.jpg\" alt=\"Facebook\"></p>\n<h1 id=\"Youtube-Test\"><a href=\"#Youtube-Test\" class=\"headerlink\" title=\"Youtube Test:\"></a>Youtube Test:</h1><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/https://youtu.be/QBJxGklvHRg\" frameborder=\"0\" allowfullscreen></iframe></div>\n<h1 id=\"Youku-Test\"><a href=\"#Youku-Test\" class=\"headerlink\" title=\"Youku Test:\"></a>Youku Test:</h1><div class=\"video-container\"><iframe height=480 width=100% src=\"http://player.youku.com/embed/XMTU3NjExOTUwMA==\" frameborder=0 allowfullscreen></iframe></div>\n"},{"title":"使用python将图片转化为字符画","date":"2016-07-20T13:35:43.000Z","comments":1,"photos":["/uploads/img/20160720/cover.png"],"_content":"忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：\n![拿衣服](/uploads/img/20160720/mo.jpg)\n当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：[Emoji Mosaic](http://ericandrewlewis.github.io/emoji-mosaic/)。\n\n做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：[ericandrewlewis/emoji-mosaic](https://github.com/ericandrewlewis/emoji-mosaic)。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。\n\n代码很简单：\n```python\n#-*- coding: utf-8 -*-\nfrom PIL import Image\n \ngrey2char = ['@','#','$','%','&','?','*','o','/','{','[','(','|','!','^','~','-','_',':',';',',','.','`',' ']\ncount = len(grey2char)\n \ndef toText(image_file):\n   image_file = image_file.convert('L')# 转灰度\n   result = ''# 储存字符串\n   for h in range(0,  image_file.size[1]):# height\n      for w in range(0, image_file.size[0]):# width\n         gray = image_file.getpixel((w,h))\n         result += grey2char[int(gray/(255/(count-1)))]\n      result += '\\r\\n'\n   return result\n \nimage_file = Image.open(\"input.jpg\")# 打开图片\nimage_file = image_file.resize((int(image_file.size[0]), int(image_file.size[1]*0.55)))# 调整图片大小\n \noutput = open('output.txt','w')\noutput.write(toText(image_file))\noutput.close()\n```\n需要注意的是要安装依赖PIL。\n\n原图：\n![](/uploads/img/20160720/input.jpg)\n\n字符图：\n![](/uploads/img/20160720/cover.png)\n","source":"_posts/img2txt.md","raw":"title: 使用python将图片转化为字符画\ndate: 2016-07-20 21:35:43\ncomments: true\ntags: \n - PIL\n - python\ncategories: Fun\nphotos: \n - /uploads/img/20160720/cover.png\n---\n忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：\n![拿衣服](/uploads/img/20160720/mo.jpg)\n当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：[Emoji Mosaic](http://ericandrewlewis.github.io/emoji-mosaic/)。\n\n做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：[ericandrewlewis/emoji-mosaic](https://github.com/ericandrewlewis/emoji-mosaic)。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。\n\n代码很简单：\n```python\n#-*- coding: utf-8 -*-\nfrom PIL import Image\n \ngrey2char = ['@','#','$','%','&','?','*','o','/','{','[','(','|','!','^','~','-','_',':',';',',','.','`',' ']\ncount = len(grey2char)\n \ndef toText(image_file):\n   image_file = image_file.convert('L')# 转灰度\n   result = ''# 储存字符串\n   for h in range(0,  image_file.size[1]):# height\n      for w in range(0, image_file.size[0]):# width\n         gray = image_file.getpixel((w,h))\n         result += grey2char[int(gray/(255/(count-1)))]\n      result += '\\r\\n'\n   return result\n \nimage_file = Image.open(\"input.jpg\")# 打开图片\nimage_file = image_file.resize((int(image_file.size[0]), int(image_file.size[1]*0.55)))# 调整图片大小\n \noutput = open('output.txt','w')\noutput.write(toText(image_file))\noutput.close()\n```\n需要注意的是要安装依赖PIL。\n\n原图：\n![](/uploads/img/20160720/input.jpg)\n\n字符图：\n![](/uploads/img/20160720/cover.png)\n","slug":"img2txt","published":1,"updated":"2016-07-20T14:20:30.941Z","layout":"post","link":"","_id":"ciwm6dced0008dgc5olg7cbbf","content":"<p>忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：<br><img src=\"/uploads/img/20160720/mo.jpg\" alt=\"拿衣服\"><br>当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：<a href=\"http://ericandrewlewis.github.io/emoji-mosaic/\" target=\"_blank\" rel=\"external\">Emoji Mosaic</a>。</p>\n<p>做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：<a href=\"https://github.com/ericandrewlewis/emoji-mosaic\" target=\"_blank\" rel=\"external\">ericandrewlewis/emoji-mosaic</a>。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。</p>\n<p>代码很简单：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"> </span><br><span class=\"line\">grey2char = [<span class=\"string\">'@'</span>,<span class=\"string\">'#'</span>,<span class=\"string\">'$'</span>,<span class=\"string\">'%'</span>,<span class=\"string\">'&amp;'</span>,<span class=\"string\">'?'</span>,<span class=\"string\">'*'</span>,<span class=\"string\">'o'</span>,<span class=\"string\">'/'</span>,<span class=\"string\">'&#123;'</span>,<span class=\"string\">'['</span>,<span class=\"string\">'('</span>,<span class=\"string\">'|'</span>,<span class=\"string\">'!'</span>,<span class=\"string\">'^'</span>,<span class=\"string\">'~'</span>,<span class=\"string\">'-'</span>,<span class=\"string\">'_'</span>,<span class=\"string\">':'</span>,<span class=\"string\">';'</span>,<span class=\"string\">','</span>,<span class=\"string\">'.'</span>,<span class=\"string\">'`'</span>,<span class=\"string\">' '</span>]</span><br><span class=\"line\">count = len(grey2char)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toText</span><span class=\"params\">(image_file)</span>:</span></span><br><span class=\"line\">   image_file = image_file.convert(<span class=\"string\">'L'</span>)<span class=\"comment\"># 转灰度</span></span><br><span class=\"line\">   result = <span class=\"string\">''</span><span class=\"comment\"># 储存字符串</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> h <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,  image_file.size[<span class=\"number\">1</span>]):<span class=\"comment\"># height</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, image_file.size[<span class=\"number\">0</span>]):<span class=\"comment\"># width</span></span><br><span class=\"line\">         gray = image_file.getpixel((w,h))</span><br><span class=\"line\">         result += grey2char[int(gray/(<span class=\"number\">255</span>/(count<span class=\"number\">-1</span>)))]</span><br><span class=\"line\">      result += <span class=\"string\">'\\r\\n'</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> result</span><br><span class=\"line\"> </span><br><span class=\"line\">image_file = Image.open(<span class=\"string\">\"input.jpg\"</span>)<span class=\"comment\"># 打开图片</span></span><br><span class=\"line\">image_file = image_file.resize((int(image_file.size[<span class=\"number\">0</span>]), int(image_file.size[<span class=\"number\">1</span>]*<span class=\"number\">0.55</span>)))<span class=\"comment\"># 调整图片大小</span></span><br><span class=\"line\"> </span><br><span class=\"line\">output = open(<span class=\"string\">'output.txt'</span>,<span class=\"string\">'w'</span>)</span><br><span class=\"line\">output.write(toText(image_file))</span><br><span class=\"line\">output.close()</span><br></pre></td></tr></table></figure></p>\n<p>需要注意的是要安装依赖PIL。</p>\n<p>原图：<br><img src=\"/uploads/img/20160720/input.jpg\" alt=\"\"></p>\n<p>字符图：<br><img src=\"/uploads/img/20160720/cover.png\" alt=\"\"></p>\n","excerpt":"","more":"<p>忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：<br><img src=\"/uploads/img/20160720/mo.jpg\" alt=\"拿衣服\"><br>当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：<a href=\"http://ericandrewlewis.github.io/emoji-mosaic/\">Emoji Mosaic</a>。</p>\n<p>做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：<a href=\"https://github.com/ericandrewlewis/emoji-mosaic\">ericandrewlewis/emoji-mosaic</a>。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。</p>\n<p>代码很简单：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"> </span><br><span class=\"line\">grey2char = [<span class=\"string\">'@'</span>,<span class=\"string\">'#'</span>,<span class=\"string\">'$'</span>,<span class=\"string\">'%'</span>,<span class=\"string\">'&amp;'</span>,<span class=\"string\">'?'</span>,<span class=\"string\">'*'</span>,<span class=\"string\">'o'</span>,<span class=\"string\">'/'</span>,<span class=\"string\">'&#123;'</span>,<span class=\"string\">'['</span>,<span class=\"string\">'('</span>,<span class=\"string\">'|'</span>,<span class=\"string\">'!'</span>,<span class=\"string\">'^'</span>,<span class=\"string\">'~'</span>,<span class=\"string\">'-'</span>,<span class=\"string\">'_'</span>,<span class=\"string\">':'</span>,<span class=\"string\">';'</span>,<span class=\"string\">','</span>,<span class=\"string\">'.'</span>,<span class=\"string\">'`'</span>,<span class=\"string\">' '</span>]</span><br><span class=\"line\">count = len(grey2char)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toText</span><span class=\"params\">(image_file)</span>:</span></span><br><span class=\"line\">   image_file = image_file.convert(<span class=\"string\">'L'</span>)<span class=\"comment\"># 转灰度</span></span><br><span class=\"line\">   result = <span class=\"string\">''</span><span class=\"comment\"># 储存字符串</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> h <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,  image_file.size[<span class=\"number\">1</span>]):<span class=\"comment\"># height</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, image_file.size[<span class=\"number\">0</span>]):<span class=\"comment\"># width</span></span><br><span class=\"line\">         gray = image_file.getpixel((w,h))</span><br><span class=\"line\">         result += grey2char[int(gray/(<span class=\"number\">255</span>/(count<span class=\"number\">-1</span>)))]</span><br><span class=\"line\">      result += <span class=\"string\">'\\r\\n'</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> result</span><br><span class=\"line\"> </span><br><span class=\"line\">image_file = Image.open(<span class=\"string\">\"input.jpg\"</span>)<span class=\"comment\"># 打开图片</span></span><br><span class=\"line\">image_file = image_file.resize((int(image_file.size[<span class=\"number\">0</span>]), int(image_file.size[<span class=\"number\">1</span>]*<span class=\"number\">0.55</span>)))<span class=\"comment\"># 调整图片大小</span></span><br><span class=\"line\"> </span><br><span class=\"line\">output = open(<span class=\"string\">'output.txt'</span>,<span class=\"string\">'w'</span>)</span><br><span class=\"line\">output.write(toText(image_file))</span><br><span class=\"line\">output.close()</span><br></pre></td></tr></table></figure></p>\n<p>需要注意的是要安装依赖PIL。</p>\n<p>原图：<br><img src=\"/uploads/img/20160720/input.jpg\" alt=\"\"></p>\n<p>字符图：<br><img src=\"/uploads/img/20160720/cover.png\" alt=\"\"></p>\n"},{"title":"HMM、MEMM和CRF的学习总结","date":"2016-07-14T08:03:45.000Z","comments":1,"photos":["/uploads/img/20160714/cover.png"],"_content":"最近一直在学习NLP里最基础的几个语言模型：**隐马尔科夫模型**（Hidden Markov Model，HMM）、**最大熵马尔科夫模型**（Maximum Entropy Markov Model，MEMM）和**条件随机场**（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的**《统计学习方法》**和吴军老师的**《数学之美》**。如需这两本书的电子版可以给我留言。\n\n我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。\n\n# HMM\n\n下图是《统计学习方法》中的描述：\n![隐马尔科夫模型](/uploads/img/20160714/hmm.png)\nHMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种**生成模型**（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。\n\n如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：\n- [如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)\n- [隐马尔可夫模型（HMM）攻略](http://blog.csdn.net/likelet/article/details/7056068)\n\n# Maximum Entropy Model\n\n首先贴一下关于最大熵模型的定义：\n![最大熵模型](/uploads/img/20160714/me.png)\n最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是**特征函数**，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出**最不偏倚**的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。\n\n最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。\n\n最大熵模型的**优点**：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。\n\n最大熵模型的**不足**：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。\n\n# MEMM\n\n最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种**生成模型**（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。\n![最大熵马尔科夫模型](/uploads/img/20160714/memm.png)\n可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。\n\nHMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在**标注偏置问题**（Label Bias Problem）。\n\n关于标注偏置问题，网上最多的是下面这个例子解释：\n![](/uploads/img/20160714/label-bias-1.png)\n路径1-1-1-1的概率：0.4\\*0.45\\*0.5=0.09\n路径2-2-2-2的概率：0.018\n路径1-2-1-2的概率：0.06\n路径1-1-2-2的概率：0.066\n由此可得最优路径为：1-1-1-1\n![](/uploads/img/20160714/label-bias-2.png)\n而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。\n例子的出处参见[标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较](http://blog.csdn.net/lskyne/article/details/8669301)\n\n# CRF\n\n![线性链条件随机场模型](/uploads/img/20160714/crf-1.png)\n这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于**判别模型**（Discrimitive Model）。\n![线性链条件随机场模型图示](/uploads/img/20160714/crf-2.png)\n上式中也同样有$f_i$**特征函数**。之前我对模型中的特征函数一直不太理解。大家可以参考[中文分词入门之字标注法4](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954)这篇文章。文章主要介绍借用条件随机场工具“[CRF++: Yet Another CRF toolkit](http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip)”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客[52nlp](http://www.52nlp.cn/)。\n《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。\n\nCRF模型的**优点**：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。\n\nCRF模型的**不足**：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。\n\n更多一些详细的CRF解释可以参考知乎的相关问题[如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？](https://www.zhihu.com/question/35866596)\n\n# MEMM与CRF区别\n\n上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。\n\nMEMM的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &=& \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\\np(y_i | x_1, \\ldots, x_T, y_{i-1}) &=&\n\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}\n{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}\n\\end{eqnarray\\*}\n$$\n线性链CRF的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y|x) &=& \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}\n\\\\\n&=& \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}\n{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }\n\\end{eqnarray\\*}\n$$\n不同点：\n- 首先，CRF是**判别模型**，而MEMM我个人理解是**生成模型**。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个**不大于1**的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。\n- MEMM和CRF的**归一化位置**不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。\n- MEMM在用**viterbi算法**求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。\n\n关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：\n- [MEMM和CRF有什么不同？](https://www.zhihu.com/question/30869789)\n- [统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 ](http://blog.sina.com.cn/s/blog_8af106960102v0v1.html)\n\n# 写在最后\n\n关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为[An introduction to conditional random fields](http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)。\n\n以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。\n\n","source":"_posts/hmm-memm-crf.md","raw":"title: HMM、MEMM和CRF的学习总结\ndate: 2016-07-14 16:03:45\ncomments: true\ntags: \n - HMM\n - MEMM\n - CRF\ncategories: NLP\nphotos: \n - /uploads/img/20160714/cover.png\n---\n最近一直在学习NLP里最基础的几个语言模型：**隐马尔科夫模型**（Hidden Markov Model，HMM）、**最大熵马尔科夫模型**（Maximum Entropy Markov Model，MEMM）和**条件随机场**（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的**《统计学习方法》**和吴军老师的**《数学之美》**。如需这两本书的电子版可以给我留言。\n\n我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。\n\n# HMM\n\n下图是《统计学习方法》中的描述：\n![隐马尔科夫模型](/uploads/img/20160714/hmm.png)\nHMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种**生成模型**（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。\n\n如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：\n- [如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)\n- [隐马尔可夫模型（HMM）攻略](http://blog.csdn.net/likelet/article/details/7056068)\n\n# Maximum Entropy Model\n\n首先贴一下关于最大熵模型的定义：\n![最大熵模型](/uploads/img/20160714/me.png)\n最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是**特征函数**，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出**最不偏倚**的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。\n\n最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。\n\n最大熵模型的**优点**：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。\n\n最大熵模型的**不足**：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。\n\n# MEMM\n\n最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种**生成模型**（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。\n![最大熵马尔科夫模型](/uploads/img/20160714/memm.png)\n可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。\n\nHMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在**标注偏置问题**（Label Bias Problem）。\n\n关于标注偏置问题，网上最多的是下面这个例子解释：\n![](/uploads/img/20160714/label-bias-1.png)\n路径1-1-1-1的概率：0.4\\*0.45\\*0.5=0.09\n路径2-2-2-2的概率：0.018\n路径1-2-1-2的概率：0.06\n路径1-1-2-2的概率：0.066\n由此可得最优路径为：1-1-1-1\n![](/uploads/img/20160714/label-bias-2.png)\n而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。\n例子的出处参见[标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较](http://blog.csdn.net/lskyne/article/details/8669301)\n\n# CRF\n\n![线性链条件随机场模型](/uploads/img/20160714/crf-1.png)\n这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于**判别模型**（Discrimitive Model）。\n![线性链条件随机场模型图示](/uploads/img/20160714/crf-2.png)\n上式中也同样有$f_i$**特征函数**。之前我对模型中的特征函数一直不太理解。大家可以参考[中文分词入门之字标注法4](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954)这篇文章。文章主要介绍借用条件随机场工具“[CRF++: Yet Another CRF toolkit](http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip)”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客[52nlp](http://www.52nlp.cn/)。\n《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。\n\nCRF模型的**优点**：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。\n\nCRF模型的**不足**：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。\n\n更多一些详细的CRF解释可以参考知乎的相关问题[如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？](https://www.zhihu.com/question/35866596)\n\n# MEMM与CRF区别\n\n上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。\n\nMEMM的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &=& \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\\np(y_i | x_1, \\ldots, x_T, y_{i-1}) &=&\n\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}\n{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}\n\\end{eqnarray\\*}\n$$\n线性链CRF的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y|x) &=& \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}\n\\\\\n&=& \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}\n{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }\n\\end{eqnarray\\*}\n$$\n不同点：\n- 首先，CRF是**判别模型**，而MEMM我个人理解是**生成模型**。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个**不大于1**的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。\n- MEMM和CRF的**归一化位置**不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。\n- MEMM在用**viterbi算法**求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。\n\n关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：\n- [MEMM和CRF有什么不同？](https://www.zhihu.com/question/30869789)\n- [统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 ](http://blog.sina.com.cn/s/blog_8af106960102v0v1.html)\n\n# 写在最后\n\n关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为[An introduction to conditional random fields](http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)。\n\n以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。\n\n","slug":"hmm-memm-crf","published":1,"updated":"2016-07-21T01:42:35.744Z","layout":"post","link":"","_id":"ciwm6dceg000adgc5xckrfi11","content":"<p>最近一直在学习NLP里最基础的几个语言模型：<strong>隐马尔科夫模型</strong>（Hidden Markov Model，HMM）、<strong>最大熵马尔科夫模型</strong>（Maximum Entropy Markov Model，MEMM）和<strong>条件随机场</strong>（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的<strong>《统计学习方法》</strong>和吴军老师的<strong>《数学之美》</strong>。如需这两本书的电子版可以给我留言。</p>\n<p>我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。</p>\n<h1 id=\"HMM\"><a href=\"#HMM\" class=\"headerlink\" title=\"HMM\"></a>HMM</h1><p>下图是《统计学习方法》中的描述：<br><img src=\"/uploads/img/20160714/hmm.png\" alt=\"隐马尔科夫模型\"><br>HMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种<strong>生成模型</strong>（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。</p>\n<p>如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/20962240\" target=\"_blank\" rel=\"external\">如何用简单易懂的例子解释隐马尔可夫模型？</a></li>\n<li><a href=\"http://blog.csdn.net/likelet/article/details/7056068\" target=\"_blank\" rel=\"external\">隐马尔可夫模型（HMM）攻略</a></li>\n</ul>\n<h1 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h1><p>首先贴一下关于最大熵模型的定义：<br><img src=\"/uploads/img/20160714/me.png\" alt=\"最大熵模型\"><br>最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是<strong>特征函数</strong>，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出<strong>最不偏倚</strong>的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。</p>\n<p>最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。</p>\n<p>最大熵模型的<strong>优点</strong>：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。</p>\n<p>最大熵模型的<strong>不足</strong>：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。</p>\n<h1 id=\"MEMM\"><a href=\"#MEMM\" class=\"headerlink\" title=\"MEMM\"></a>MEMM</h1><p>最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种<strong>生成模型</strong>（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。<br><img src=\"/uploads/img/20160714/memm.png\" alt=\"最大熵马尔科夫模型\"><br>可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。</p>\n<p>HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在<strong>标注偏置问题</strong>（Label Bias Problem）。</p>\n<p>关于标注偏置问题，网上最多的是下面这个例子解释：<br><img src=\"/uploads/img/20160714/label-bias-1.png\" alt=\"\"><br>路径1-1-1-1的概率：0.4*0.45*0.5=0.09<br>路径2-2-2-2的概率：0.018<br>路径1-2-1-2的概率：0.06<br>路径1-1-2-2的概率：0.066<br>由此可得最优路径为：1-1-1-1<br><img src=\"/uploads/img/20160714/label-bias-2.png\" alt=\"\"><br>而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。<br>例子的出处参见<a href=\"http://blog.csdn.net/lskyne/article/details/8669301\" target=\"_blank\" rel=\"external\">标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较</a></p>\n<h1 id=\"CRF\"><a href=\"#CRF\" class=\"headerlink\" title=\"CRF\"></a>CRF</h1><p><img src=\"/uploads/img/20160714/crf-1.png\" alt=\"线性链条件随机场模型\"><br>这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于<strong>判别模型</strong>（Discrimitive Model）。<br><img src=\"/uploads/img/20160714/crf-2.png\" alt=\"线性链条件随机场模型图示\"><br>上式中也同样有$f_i$<strong>特征函数</strong>。之前我对模型中的特征函数一直不太理解。大家可以参考<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954\" target=\"_blank\" rel=\"external\">中文分词入门之字标注法4</a>这篇文章。文章主要介绍借用条件随机场工具“<a href=\"http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip\" target=\"_blank\" rel=\"external\">CRF++: Yet Another CRF toolkit</a>”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客<a href=\"http://www.52nlp.cn/\" target=\"_blank\" rel=\"external\">52nlp</a>。<br>《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。</p>\n<p>CRF模型的<strong>优点</strong>：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。</p>\n<p>CRF模型的<strong>不足</strong>：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。</p>\n<p>更多一些详细的CRF解释可以参考知乎的相关问题<a href=\"https://www.zhihu.com/question/35866596\" target=\"_blank\" rel=\"external\">如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？</a></p>\n<h1 id=\"MEMM与CRF区别\"><a href=\"#MEMM与CRF区别\" class=\"headerlink\" title=\"MEMM与CRF区别\"></a>MEMM与CRF区别</h1><p>上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。</p>\n<p>MEMM的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &amp;=&amp; \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\<br>p(y_i | x_1, \\ldots, x_T, y_{i-1}) &amp;=&amp;<br>\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}<br>{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}<br>\\end{eqnarray*}<br>$$<br>线性链CRF的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y|x) &amp;=&amp; \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}<br>\\\\<br>&amp;=&amp; \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}<br>{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }<br>\\end{eqnarray*}<br>$$<br>不同点：</p>\n<ul>\n<li>首先，CRF是<strong>判别模型</strong>，而MEMM我个人理解是<strong>生成模型</strong>。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个<strong>不大于1</strong>的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。</li>\n<li>MEMM和CRF的<strong>归一化位置</strong>不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。</li>\n<li>MEMM在用<strong>viterbi算法</strong>求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。</li>\n</ul>\n<p>关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/30869789\" target=\"_blank\" rel=\"external\">MEMM和CRF有什么不同？</a></li>\n<li><a href=\"http://blog.sina.com.cn/s/blog_8af106960102v0v1.html\" target=\"_blank\" rel=\"external\">统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 </a></li>\n</ul>\n<h1 id=\"写在最后\"><a href=\"#写在最后\" class=\"headerlink\" title=\"写在最后\"></a>写在最后</h1><p>关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为<a href=\"http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf\" target=\"_blank\" rel=\"external\">An introduction to conditional random fields</a>。</p>\n<p>以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。</p>\n","excerpt":"","more":"<p>最近一直在学习NLP里最基础的几个语言模型：<strong>隐马尔科夫模型</strong>（Hidden Markov Model，HMM）、<strong>最大熵马尔科夫模型</strong>（Maximum Entropy Markov Model，MEMM）和<strong>条件随机场</strong>（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的<strong>《统计学习方法》</strong>和吴军老师的<strong>《数学之美》</strong>。如需这两本书的电子版可以给我留言。</p>\n<p>我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。</p>\n<h1 id=\"HMM\"><a href=\"#HMM\" class=\"headerlink\" title=\"HMM\"></a>HMM</h1><p>下图是《统计学习方法》中的描述：<br><img src=\"/uploads/img/20160714/hmm.png\" alt=\"隐马尔科夫模型\"><br>HMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种<strong>生成模型</strong>（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。</p>\n<p>如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/20962240\">如何用简单易懂的例子解释隐马尔可夫模型？</a></li>\n<li><a href=\"http://blog.csdn.net/likelet/article/details/7056068\">隐马尔可夫模型（HMM）攻略</a></li>\n</ul>\n<h1 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h1><p>首先贴一下关于最大熵模型的定义：<br><img src=\"/uploads/img/20160714/me.png\" alt=\"最大熵模型\"><br>最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是<strong>特征函数</strong>，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出<strong>最不偏倚</strong>的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。</p>\n<p>最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。</p>\n<p>最大熵模型的<strong>优点</strong>：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。</p>\n<p>最大熵模型的<strong>不足</strong>：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。</p>\n<h1 id=\"MEMM\"><a href=\"#MEMM\" class=\"headerlink\" title=\"MEMM\"></a>MEMM</h1><p>最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种<strong>生成模型</strong>（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。<br><img src=\"/uploads/img/20160714/memm.png\" alt=\"最大熵马尔科夫模型\"><br>可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。</p>\n<p>HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在<strong>标注偏置问题</strong>（Label Bias Problem）。</p>\n<p>关于标注偏置问题，网上最多的是下面这个例子解释：<br><img src=\"/uploads/img/20160714/label-bias-1.png\" alt=\"\"><br>路径1-1-1-1的概率：0.4*0.45*0.5=0.09<br>路径2-2-2-2的概率：0.018<br>路径1-2-1-2的概率：0.06<br>路径1-1-2-2的概率：0.066<br>由此可得最优路径为：1-1-1-1<br><img src=\"/uploads/img/20160714/label-bias-2.png\" alt=\"\"><br>而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。<br>例子的出处参见<a href=\"http://blog.csdn.net/lskyne/article/details/8669301\">标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较</a></p>\n<h1 id=\"CRF\"><a href=\"#CRF\" class=\"headerlink\" title=\"CRF\"></a>CRF</h1><p><img src=\"/uploads/img/20160714/crf-1.png\" alt=\"线性链条件随机场模型\"><br>这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于<strong>判别模型</strong>（Discrimitive Model）。<br><img src=\"/uploads/img/20160714/crf-2.png\" alt=\"线性链条件随机场模型图示\"><br>上式中也同样有$f_i$<strong>特征函数</strong>。之前我对模型中的特征函数一直不太理解。大家可以参考<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954\">中文分词入门之字标注法4</a>这篇文章。文章主要介绍借用条件随机场工具“<a href=\"http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip\">CRF++: Yet Another CRF toolkit</a>”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客<a href=\"http://www.52nlp.cn/\">52nlp</a>。<br>《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。</p>\n<p>CRF模型的<strong>优点</strong>：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。</p>\n<p>CRF模型的<strong>不足</strong>：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。</p>\n<p>更多一些详细的CRF解释可以参考知乎的相关问题<a href=\"https://www.zhihu.com/question/35866596\">如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？</a></p>\n<h1 id=\"MEMM与CRF区别\"><a href=\"#MEMM与CRF区别\" class=\"headerlink\" title=\"MEMM与CRF区别\"></a>MEMM与CRF区别</h1><p>上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。</p>\n<p>MEMM的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &amp;=&amp; \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\<br>p(y_i | x_1, \\ldots, x_T, y_{i-1}) &amp;=&amp;<br>\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}<br>{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}<br>\\end{eqnarray*}<br>$$<br>线性链CRF的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y|x) &amp;=&amp; \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}<br>\\\\<br>&amp;=&amp; \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}<br>{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }<br>\\end{eqnarray*}<br>$$<br>不同点：</p>\n<ul>\n<li>首先，CRF是<strong>判别模型</strong>，而MEMM我个人理解是<strong>生成模型</strong>。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个<strong>不大于1</strong>的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。</li>\n<li>MEMM和CRF的<strong>归一化位置</strong>不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。</li>\n<li>MEMM在用<strong>viterbi算法</strong>求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。</li>\n</ul>\n<p>关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/30869789\">MEMM和CRF有什么不同？</a></li>\n<li><a href=\"http://blog.sina.com.cn/s/blog_8af106960102v0v1.html\">统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 </a></li>\n</ul>\n<h1 id=\"写在最后\"><a href=\"#写在最后\" class=\"headerlink\" title=\"写在最后\"></a>写在最后</h1><p>关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为<a href=\"http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf\">An introduction to conditional random fields</a>。</p>\n<p>以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。</p>\n"},{"title":"基于Neo4j的知乎关系爬虫","date":"2016-06-29T03:15:56.000Z","comments":1,"photos":["/uploads/img/20160629/cover.jpg"],"_content":"前两天做了一个爬取知乎用户**follow**关系的爬虫。做这个爬虫是受一个知乎专栏的启发[Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系](https://zhuanlan.zhihu.com/p/20546546)，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个**六度分隔理论**的直观展现，也是我在做爬虫时的意外验证。\n# 环境安装\n\n首先交代一下爬虫所用到的数据库和环境：\n## MongoDB\nMongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。\n在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，`pip install pymongo`就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具[Robomongo](https://robomongo.org/)。\n\n## Neo4j\nNeo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。\n关于Neo4j的安装，可以参考这篇博客[Neo4j介绍与使用](http://blog.csdn.net/dyllove98/article/details/8635965)。Win7环境下，官网[下载](https://neo4j.com/download/)可以一键安装Neo4j。\n\nNeo4j使用类似SQL的查询语言**Cypher**，关于Cypher的使用和简单demo，可以参考[Cypher查询语言--Neo4j中的SQL](http://www.uml.org.cn/sjjm/201203063.asp)。当然，为了减少学习Cypher的时间成本，我在python环境中安装了**py2neo**，`pip install py2neo`。\n\npy2neo的handbook见[The Py2neo v3 Handbook](http://py2neo.org/v3/)。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。\n\n## python依赖\n* requests\nrequests是一个非常好用的网络依赖包，API文档见[Requests: HTTP for Humans](http://www.python-requests.org/en/master/)。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。\n* BeautifulSoup\nBeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)。\n\n# 爬虫概要\n## 目的\n我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V---轮子哥（**vczh**）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“**三天三夜**”。\n## 开发思路\n爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是**BFS**的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num**不超过100**的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。\n## 代码相关\n整个爬虫的代码我push到Github上，附上[链接](https://github.com/tripleday/zhihu_link)。\n贴上几个想到的小细节：\n* 这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装**EditThisCookie**插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。\n* 知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的**data-id**，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。\n* BeautifulSoup的官方文档里的一张解析器对比表格\n\n![BeautifulSoup解析器对比](/uploads/img/20160629/bs.png)\n实际使用中，在解析`https://www.zhihu.com/people/hong-ming-da`这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。\n* 代码中有关知乎爬虫的代码，我是在[egrcc/zhihu-python](https://github.com/egrcc/zhihu-python)的基础上改动的，非常感谢原作者的分享。\n* 其他的细节想到后再补充。\n\n# 爬取结果\n\n爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：\n![部分用户关系图](/uploads/img/20160629/whole.png)\n\n在命令行执行Cypher语句：`MATCH (a {_id : '0970f947b898ecc0ec035f9126dd4e08'}), (b {_id : 'bd648b6ef0f14880a522e09ce2752465'}), p = allShortestPaths( (a)-[*..200]->(b) ) RETURN p`可以得到轮子哥到我的最短路径：\n![最短路径图](/uploads/img/20160629/shortestpath.png)\n\n可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 **6** 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。\n","source":"_posts/zhihu-link.md","raw":"title: 基于Neo4j的知乎关系爬虫\ndate: 2016-06-29 11:15:56\ncomments: true\ntags: \n - zhihu\n - Neo4j\n - python\ncategories: Crawler\nphotos: \n - /uploads/img/20160629/cover.jpg\n---\n前两天做了一个爬取知乎用户**follow**关系的爬虫。做这个爬虫是受一个知乎专栏的启发[Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系](https://zhuanlan.zhihu.com/p/20546546)，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个**六度分隔理论**的直观展现，也是我在做爬虫时的意外验证。\n# 环境安装\n\n首先交代一下爬虫所用到的数据库和环境：\n## MongoDB\nMongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。\n在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，`pip install pymongo`就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具[Robomongo](https://robomongo.org/)。\n\n## Neo4j\nNeo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。\n关于Neo4j的安装，可以参考这篇博客[Neo4j介绍与使用](http://blog.csdn.net/dyllove98/article/details/8635965)。Win7环境下，官网[下载](https://neo4j.com/download/)可以一键安装Neo4j。\n\nNeo4j使用类似SQL的查询语言**Cypher**，关于Cypher的使用和简单demo，可以参考[Cypher查询语言--Neo4j中的SQL](http://www.uml.org.cn/sjjm/201203063.asp)。当然，为了减少学习Cypher的时间成本，我在python环境中安装了**py2neo**，`pip install py2neo`。\n\npy2neo的handbook见[The Py2neo v3 Handbook](http://py2neo.org/v3/)。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。\n\n## python依赖\n* requests\nrequests是一个非常好用的网络依赖包，API文档见[Requests: HTTP for Humans](http://www.python-requests.org/en/master/)。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。\n* BeautifulSoup\nBeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)。\n\n# 爬虫概要\n## 目的\n我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V---轮子哥（**vczh**）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“**三天三夜**”。\n## 开发思路\n爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是**BFS**的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num**不超过100**的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。\n## 代码相关\n整个爬虫的代码我push到Github上，附上[链接](https://github.com/tripleday/zhihu_link)。\n贴上几个想到的小细节：\n* 这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装**EditThisCookie**插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。\n* 知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的**data-id**，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。\n* BeautifulSoup的官方文档里的一张解析器对比表格\n\n![BeautifulSoup解析器对比](/uploads/img/20160629/bs.png)\n实际使用中，在解析`https://www.zhihu.com/people/hong-ming-da`这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。\n* 代码中有关知乎爬虫的代码，我是在[egrcc/zhihu-python](https://github.com/egrcc/zhihu-python)的基础上改动的，非常感谢原作者的分享。\n* 其他的细节想到后再补充。\n\n# 爬取结果\n\n爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：\n![部分用户关系图](/uploads/img/20160629/whole.png)\n\n在命令行执行Cypher语句：`MATCH (a {_id : '0970f947b898ecc0ec035f9126dd4e08'}), (b {_id : 'bd648b6ef0f14880a522e09ce2752465'}), p = allShortestPaths( (a)-[*..200]->(b) ) RETURN p`可以得到轮子哥到我的最短路径：\n![最短路径图](/uploads/img/20160629/shortestpath.png)\n\n可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 **6** 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。\n","slug":"zhihu-link","published":1,"updated":"2016-07-20T13:49:47.806Z","layout":"post","link":"","_id":"ciwm6dcen000cdgc5kvfxvrz0","content":"<p>前两天做了一个爬取知乎用户<strong>follow</strong>关系的爬虫。做这个爬虫是受一个知乎专栏的启发<a href=\"https://zhuanlan.zhihu.com/p/20546546\" target=\"_blank\" rel=\"external\">Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系</a>，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个<strong>六度分隔理论</strong>的直观展现，也是我在做爬虫时的意外验证。</p>\n<h1 id=\"环境安装\"><a href=\"#环境安装\" class=\"headerlink\" title=\"环境安装\"></a>环境安装</h1><p>首先交代一下爬虫所用到的数据库和环境：</p>\n<h2 id=\"MongoDB\"><a href=\"#MongoDB\" class=\"headerlink\" title=\"MongoDB\"></a>MongoDB</h2><p>MongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。<br>在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，<code>pip install pymongo</code>就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具<a href=\"https://robomongo.org/\" target=\"_blank\" rel=\"external\">Robomongo</a>。</p>\n<h2 id=\"Neo4j\"><a href=\"#Neo4j\" class=\"headerlink\" title=\"Neo4j\"></a>Neo4j</h2><p>Neo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。<br>关于Neo4j的安装，可以参考这篇博客<a href=\"http://blog.csdn.net/dyllove98/article/details/8635965\" target=\"_blank\" rel=\"external\">Neo4j介绍与使用</a>。Win7环境下，官网<a href=\"https://neo4j.com/download/\" target=\"_blank\" rel=\"external\">下载</a>可以一键安装Neo4j。</p>\n<p>Neo4j使用类似SQL的查询语言<strong>Cypher</strong>，关于Cypher的使用和简单demo，可以参考<a href=\"http://www.uml.org.cn/sjjm/201203063.asp\" target=\"_blank\" rel=\"external\">Cypher查询语言–Neo4j中的SQL</a>。当然，为了减少学习Cypher的时间成本，我在python环境中安装了<strong>py2neo</strong>，<code>pip install py2neo</code>。</p>\n<p>py2neo的handbook见<a href=\"http://py2neo.org/v3/\" target=\"_blank\" rel=\"external\">The Py2neo v3 Handbook</a>。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。</p>\n<h2 id=\"python依赖\"><a href=\"#python依赖\" class=\"headerlink\" title=\"python依赖\"></a>python依赖</h2><ul>\n<li>requests<br>requests是一个非常好用的网络依赖包，API文档见<a href=\"http://www.python-requests.org/en/master/\" target=\"_blank\" rel=\"external\">Requests: HTTP for Humans</a>。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。</li>\n<li>BeautifulSoup<br>BeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见<a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\" target=\"_blank\" rel=\"external\">Beautiful Soup 4.2.0 文档</a>。</li>\n</ul>\n<h1 id=\"爬虫概要\"><a href=\"#爬虫概要\" class=\"headerlink\" title=\"爬虫概要\"></a>爬虫概要</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V—轮子哥（<strong>vczh</strong>）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“<strong>三天三夜</strong>”。</p>\n<h2 id=\"开发思路\"><a href=\"#开发思路\" class=\"headerlink\" title=\"开发思路\"></a>开发思路</h2><p>爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是<strong>BFS</strong>的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num<strong>不超过100</strong>的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。</p>\n<h2 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h2><p>整个爬虫的代码我push到Github上，附上<a href=\"https://github.com/tripleday/zhihu_link\" target=\"_blank\" rel=\"external\">链接</a>。<br>贴上几个想到的小细节：</p>\n<ul>\n<li>这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装<strong>EditThisCookie</strong>插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。</li>\n<li>知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的<strong>data-id</strong>，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。</li>\n<li>BeautifulSoup的官方文档里的一张解析器对比表格</li>\n</ul>\n<p><img src=\"/uploads/img/20160629/bs.png\" alt=\"BeautifulSoup解析器对比\"><br>实际使用中，在解析<code>https://www.zhihu.com/people/hong-ming-da</code>这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。</p>\n<ul>\n<li>代码中有关知乎爬虫的代码，我是在<a href=\"https://github.com/egrcc/zhihu-python\" target=\"_blank\" rel=\"external\">egrcc/zhihu-python</a>的基础上改动的，非常感谢原作者的分享。</li>\n<li>其他的细节想到后再补充。</li>\n</ul>\n<h1 id=\"爬取结果\"><a href=\"#爬取结果\" class=\"headerlink\" title=\"爬取结果\"></a>爬取结果</h1><p>爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：<br><img src=\"/uploads/img/20160629/whole.png\" alt=\"部分用户关系图\"></p>\n<p>在命令行执行Cypher语句：<code>MATCH (a {_id : &#39;0970f947b898ecc0ec035f9126dd4e08&#39;}), (b {_id : &#39;bd648b6ef0f14880a522e09ce2752465&#39;}), p = allShortestPaths( (a)-[*..200]-&gt;(b) ) RETURN p</code>可以得到轮子哥到我的最短路径：<br><img src=\"/uploads/img/20160629/shortestpath.png\" alt=\"最短路径图\"></p>\n<p>可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 <strong>6</strong> 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。</p>\n","excerpt":"","more":"<p>前两天做了一个爬取知乎用户<strong>follow</strong>关系的爬虫。做这个爬虫是受一个知乎专栏的启发<a href=\"https://zhuanlan.zhihu.com/p/20546546\">Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系</a>，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个<strong>六度分隔理论</strong>的直观展现，也是我在做爬虫时的意外验证。</p>\n<h1 id=\"环境安装\"><a href=\"#环境安装\" class=\"headerlink\" title=\"环境安装\"></a>环境安装</h1><p>首先交代一下爬虫所用到的数据库和环境：</p>\n<h2 id=\"MongoDB\"><a href=\"#MongoDB\" class=\"headerlink\" title=\"MongoDB\"></a>MongoDB</h2><p>MongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。<br>在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，<code>pip install pymongo</code>就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具<a href=\"https://robomongo.org/\">Robomongo</a>。</p>\n<h2 id=\"Neo4j\"><a href=\"#Neo4j\" class=\"headerlink\" title=\"Neo4j\"></a>Neo4j</h2><p>Neo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。<br>关于Neo4j的安装，可以参考这篇博客<a href=\"http://blog.csdn.net/dyllove98/article/details/8635965\">Neo4j介绍与使用</a>。Win7环境下，官网<a href=\"https://neo4j.com/download/\">下载</a>可以一键安装Neo4j。</p>\n<p>Neo4j使用类似SQL的查询语言<strong>Cypher</strong>，关于Cypher的使用和简单demo，可以参考<a href=\"http://www.uml.org.cn/sjjm/201203063.asp\">Cypher查询语言–Neo4j中的SQL</a>。当然，为了减少学习Cypher的时间成本，我在python环境中安装了<strong>py2neo</strong>，<code>pip install py2neo</code>。</p>\n<p>py2neo的handbook见<a href=\"http://py2neo.org/v3/\">The Py2neo v3 Handbook</a>。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。</p>\n<h2 id=\"python依赖\"><a href=\"#python依赖\" class=\"headerlink\" title=\"python依赖\"></a>python依赖</h2><ul>\n<li>requests<br>requests是一个非常好用的网络依赖包，API文档见<a href=\"http://www.python-requests.org/en/master/\">Requests: HTTP for Humans</a>。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。</li>\n<li>BeautifulSoup<br>BeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见<a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\">Beautiful Soup 4.2.0 文档</a>。</li>\n</ul>\n<h1 id=\"爬虫概要\"><a href=\"#爬虫概要\" class=\"headerlink\" title=\"爬虫概要\"></a>爬虫概要</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V—轮子哥（<strong>vczh</strong>）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“<strong>三天三夜</strong>”。</p>\n<h2 id=\"开发思路\"><a href=\"#开发思路\" class=\"headerlink\" title=\"开发思路\"></a>开发思路</h2><p>爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是<strong>BFS</strong>的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num<strong>不超过100</strong>的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。</p>\n<h2 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h2><p>整个爬虫的代码我push到Github上，附上<a href=\"https://github.com/tripleday/zhihu_link\">链接</a>。<br>贴上几个想到的小细节：</p>\n<ul>\n<li>这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装<strong>EditThisCookie</strong>插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。</li>\n<li>知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的<strong>data-id</strong>，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。</li>\n<li>BeautifulSoup的官方文档里的一张解析器对比表格</li>\n</ul>\n<p><img src=\"/uploads/img/20160629/bs.png\" alt=\"BeautifulSoup解析器对比\"><br>实际使用中，在解析<code>https://www.zhihu.com/people/hong-ming-da</code>这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。</p>\n<ul>\n<li>代码中有关知乎爬虫的代码，我是在<a href=\"https://github.com/egrcc/zhihu-python\">egrcc/zhihu-python</a>的基础上改动的，非常感谢原作者的分享。</li>\n<li>其他的细节想到后再补充。</li>\n</ul>\n<h1 id=\"爬取结果\"><a href=\"#爬取结果\" class=\"headerlink\" title=\"爬取结果\"></a>爬取结果</h1><p>爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：<br><img src=\"/uploads/img/20160629/whole.png\" alt=\"部分用户关系图\"></p>\n<p>在命令行执行Cypher语句：<code>MATCH (a {_id : &#39;0970f947b898ecc0ec035f9126dd4e08&#39;}), (b {_id : &#39;bd648b6ef0f14880a522e09ce2752465&#39;}), p = allShortestPaths( (a)-[*..200]-&gt;(b) ) RETURN p</code>可以得到轮子哥到我的最短路径：<br><img src=\"/uploads/img/20160629/shortestpath.png\" alt=\"最短路径图\"></p>\n<p>可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 <strong>6</strong> 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。</p>\n"},{"title":"NLP中使用HMM进行tag、seg和ner","date":"2016-07-28T13:54:17.000Z","comments":1,"photos":["/uploads/img/20160728/cover.png"],"_content":"看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称**tag**）、分词（Segmentation，以下简称**seg**）和命名实体识别（Named Entity Recognition，以下简称**ner**）。\n\n具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文[HMM、MEMM和CRF的学习总结](http://tripleday.github.io/2016/07/14/hmm-memm-crf/)和里面提供的一些链接。\n\n# 代码相关\n\n整个HMM的代码和相关测试数据已上传至Github上，附上[链接](https://github.com/tripleday/simple_HMM)。整个代码的实现有部分学习参考博客[python词法分析(分词+词性标注）](http://blog.csdn.net/soundfuture/article/details/4135216)，感谢博主的分享。\n\n* 文件图如下：\n![](/uploads/img/20160728/file.png)\n这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见[Chunking](http://www.cnts.ua.ac.be/conll2000/chunking/)，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。\n\n* 其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。\n\n\n# tag\n\n* 数据说明\ntag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。\n  * conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，**第二列**是由Brill tagger标注的词性，**第三列**是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。\n  * pku的数据是中文语料，且已经进行过分词操作，它的词性标注与[近代汉语词类标注简表](http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a)相似但不完全相同。\n\n* 测试结果\n关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的**Brill tagger**的准确率约为94.09%，对第三列的**WSJ corpus**的准确率约为87.93%。pku的准确率约为93.0%。\n\n* 结果分析\n  * 本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。\n  * 我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。\n\n# seg\n\n* 处理思路\nseg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇[中文分词入门之字标注法3](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953)。\n\n* 测试结果\n对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。\n\n# ner\n\n* 数据说明\nner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。\n\n* 测试结果\n对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。\n\n* 结果分析\n从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。\n\n\n","source":"_posts/hmm4nlp.md","raw":"title: NLP中使用HMM进行tag、seg和ner\ndate: 2016-07-28 21:54:17\ncomments: true\ntags: \n - HMM\n - python\ncategories: NLP\nphotos: \n - /uploads/img/20160728/cover.png\n---\n看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称**tag**）、分词（Segmentation，以下简称**seg**）和命名实体识别（Named Entity Recognition，以下简称**ner**）。\n\n具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文[HMM、MEMM和CRF的学习总结](http://tripleday.github.io/2016/07/14/hmm-memm-crf/)和里面提供的一些链接。\n\n# 代码相关\n\n整个HMM的代码和相关测试数据已上传至Github上，附上[链接](https://github.com/tripleday/simple_HMM)。整个代码的实现有部分学习参考博客[python词法分析(分词+词性标注）](http://blog.csdn.net/soundfuture/article/details/4135216)，感谢博主的分享。\n\n* 文件图如下：\n![](/uploads/img/20160728/file.png)\n这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见[Chunking](http://www.cnts.ua.ac.be/conll2000/chunking/)，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。\n\n* 其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。\n\n\n# tag\n\n* 数据说明\ntag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。\n  * conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，**第二列**是由Brill tagger标注的词性，**第三列**是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。\n  * pku的数据是中文语料，且已经进行过分词操作，它的词性标注与[近代汉语词类标注简表](http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a)相似但不完全相同。\n\n* 测试结果\n关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的**Brill tagger**的准确率约为94.09%，对第三列的**WSJ corpus**的准确率约为87.93%。pku的准确率约为93.0%。\n\n* 结果分析\n  * 本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。\n  * 我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。\n\n# seg\n\n* 处理思路\nseg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇[中文分词入门之字标注法3](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953)。\n\n* 测试结果\n对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。\n\n# ner\n\n* 数据说明\nner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。\n\n* 测试结果\n对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。\n\n* 结果分析\n从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。\n\n\n","slug":"hmm4nlp","published":1,"updated":"2016-09-02T08:44:17.690Z","layout":"post","link":"","_id":"ciwm6dchy001kdgc5egi128n3","content":"<p>看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称<strong>tag</strong>）、分词（Segmentation，以下简称<strong>seg</strong>）和命名实体识别（Named Entity Recognition，以下简称<strong>ner</strong>）。</p>\n<p>具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文<a href=\"http://tripleday.github.io/2016/07/14/hmm-memm-crf/\">HMM、MEMM和CRF的学习总结</a>和里面提供的一些链接。</p>\n<h1 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h1><p>整个HMM的代码和相关测试数据已上传至Github上，附上<a href=\"https://github.com/tripleday/simple_HMM\" target=\"_blank\" rel=\"external\">链接</a>。整个代码的实现有部分学习参考博客<a href=\"http://blog.csdn.net/soundfuture/article/details/4135216\" target=\"_blank\" rel=\"external\">python词法分析(分词+词性标注）</a>，感谢博主的分享。</p>\n<ul>\n<li><p>文件图如下：<br><img src=\"/uploads/img/20160728/file.png\" alt=\"\"><br>这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见<a href=\"http://www.cnts.ua.ac.be/conll2000/chunking/\" target=\"_blank\" rel=\"external\">Chunking</a>，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。</p>\n</li>\n<li><p>其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。</p>\n</li>\n</ul>\n<h1 id=\"tag\"><a href=\"#tag\" class=\"headerlink\" title=\"tag\"></a>tag</h1><ul>\n<li><p>数据说明<br>tag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。</p>\n<ul>\n<li>conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，<strong>第二列</strong>是由Brill tagger标注的词性，<strong>第三列</strong>是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。</li>\n<li>pku的数据是中文语料，且已经进行过分词操作，它的词性标注与<a href=\"http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a\" target=\"_blank\" rel=\"external\">近代汉语词类标注简表</a>相似但不完全相同。</li>\n</ul>\n</li>\n<li><p>测试结果<br>关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的<strong>Brill tagger</strong>的准确率约为94.09%，对第三列的<strong>WSJ corpus</strong>的准确率约为87.93%。pku的准确率约为93.0%。</p>\n</li>\n<li><p>结果分析</p>\n<ul>\n<li>本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。</li>\n<li>我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"seg\"><a href=\"#seg\" class=\"headerlink\" title=\"seg\"></a>seg</h1><ul>\n<li><p>处理思路<br>seg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953\" target=\"_blank\" rel=\"external\">中文分词入门之字标注法3</a>。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。</p>\n</li>\n</ul>\n<h1 id=\"ner\"><a href=\"#ner\" class=\"headerlink\" title=\"ner\"></a>ner</h1><ul>\n<li><p>数据说明<br>ner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。</p>\n</li>\n<li><p>结果分析<br>从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。</p>\n</li>\n</ul>\n","excerpt":"","more":"<p>看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称<strong>tag</strong>）、分词（Segmentation，以下简称<strong>seg</strong>）和命名实体识别（Named Entity Recognition，以下简称<strong>ner</strong>）。</p>\n<p>具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文<a href=\"http://tripleday.github.io/2016/07/14/hmm-memm-crf/\">HMM、MEMM和CRF的学习总结</a>和里面提供的一些链接。</p>\n<h1 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h1><p>整个HMM的代码和相关测试数据已上传至Github上，附上<a href=\"https://github.com/tripleday/simple_HMM\">链接</a>。整个代码的实现有部分学习参考博客<a href=\"http://blog.csdn.net/soundfuture/article/details/4135216\">python词法分析(分词+词性标注）</a>，感谢博主的分享。</p>\n<ul>\n<li><p>文件图如下：<br><img src=\"/uploads/img/20160728/file.png\" alt=\"\"><br>这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见<a href=\"http://www.cnts.ua.ac.be/conll2000/chunking/\">Chunking</a>，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。</p>\n</li>\n<li><p>其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。</p>\n</li>\n</ul>\n<h1 id=\"tag\"><a href=\"#tag\" class=\"headerlink\" title=\"tag\"></a>tag</h1><ul>\n<li><p>数据说明<br>tag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。</p>\n<ul>\n<li>conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，<strong>第二列</strong>是由Brill tagger标注的词性，<strong>第三列</strong>是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。</li>\n<li>pku的数据是中文语料，且已经进行过分词操作，它的词性标注与<a href=\"http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a\">近代汉语词类标注简表</a>相似但不完全相同。</li>\n</ul>\n</li>\n<li><p>测试结果<br>关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的<strong>Brill tagger</strong>的准确率约为94.09%，对第三列的<strong>WSJ corpus</strong>的准确率约为87.93%。pku的准确率约为93.0%。</p>\n</li>\n<li><p>结果分析</p>\n<ul>\n<li>本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。</li>\n<li>我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"seg\"><a href=\"#seg\" class=\"headerlink\" title=\"seg\"></a>seg</h1><ul>\n<li><p>处理思路<br>seg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953\">中文分词入门之字标注法3</a>。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。</p>\n</li>\n</ul>\n<h1 id=\"ner\"><a href=\"#ner\" class=\"headerlink\" title=\"ner\"></a>ner</h1><ul>\n<li><p>数据说明<br>ner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。</p>\n</li>\n<li><p>结果分析<br>从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。</p>\n</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ciwm6dcdv0000dgc5gldjqowe","category_id":"ciwm6dce90005dgc5s52csjon","_id":"ciwm6dcep000ddgc52g2hc16h"},{"post_id":"ciwm6dcen000cdgc5kvfxvrz0","category_id":"ciwm6dce90005dgc5s52csjon","_id":"ciwm6dces000gdgc5bd4nm4vc"},{"post_id":"ciwm6dce30002dgc5i6iiqtdn","category_id":"ciwm6dcel000bdgc5h6gu3fe2","_id":"ciwm6dcev000jdgc5loey5yzd"},{"post_id":"ciwm6dcea0006dgc55zrq0azk","category_id":"ciwm6dceq000fdgc5ajjj227m","_id":"ciwm6dcex000kdgc5y8ybp895"},{"post_id":"ciwm6dced0008dgc5olg7cbbf","category_id":"ciwm6dcet000idgc5pmddylxv","_id":"ciwm6dcf1000pdgc561lxmcus"},{"post_id":"ciwm6dceg000adgc5xckrfi11","category_id":"ciwm6dcez000mdgc5oxruzfr0","_id":"ciwm6dcf3000udgc5zfn5qawd"},{"post_id":"ciwm6dchy001kdgc5egi128n3","category_id":"ciwm6dcez000mdgc5oxruzfr0","_id":"ciwm6dci2001ndgc5hrgqcow1"}],"PostTag":[{"post_id":"ciwm6dcdv0000dgc5gldjqowe","tag_id":"ciwm6dce70004dgc5a1g7jx2g","_id":"ciwm6dcf1000ndgc5gzh84qlf"},{"post_id":"ciwm6dcdv0000dgc5gldjqowe","tag_id":"ciwm6dcef0009dgc5gnilicn0","_id":"ciwm6dcf1000odgc5a329w1rn"},{"post_id":"ciwm6dcdv0000dgc5gldjqowe","tag_id":"ciwm6dcep000edgc5fmel44yl","_id":"ciwm6dcf2000rdgc5n9yyw84z"},{"post_id":"ciwm6dcdv0000dgc5gldjqowe","tag_id":"ciwm6dcet000hdgc55r5i0bkg","_id":"ciwm6dcf2000sdgc5uth72s98"},{"post_id":"ciwm6dce30002dgc5i6iiqtdn","tag_id":"ciwm6dcep000edgc5fmel44yl","_id":"ciwm6dcf5000xdgc5pt4guvxm"},{"post_id":"ciwm6dce30002dgc5i6iiqtdn","tag_id":"ciwm6dcef0009dgc5gnilicn0","_id":"ciwm6dcf5000ydgc5f1h76phq"},{"post_id":"ciwm6dce30002dgc5i6iiqtdn","tag_id":"ciwm6dcf2000tdgc5sqo6d3ga","_id":"ciwm6dcf60010dgc5n58w0fap"},{"post_id":"ciwm6dce30002dgc5i6iiqtdn","tag_id":"ciwm6dcet000hdgc55r5i0bkg","_id":"ciwm6dcf60011dgc5uamgncv4"},{"post_id":"ciwm6dcea0006dgc55zrq0azk","tag_id":"ciwm6dcf3000wdgc5arjcb24w","_id":"ciwm6dcf70013dgc5ox9mpaqw"},{"post_id":"ciwm6dcea0006dgc55zrq0azk","tag_id":"ciwm6dcf5000zdgc5k0m302jw","_id":"ciwm6dcf70014dgc5dq1c51za"},{"post_id":"ciwm6dced0008dgc5olg7cbbf","tag_id":"ciwm6dcf60012dgc55ui6igo3","_id":"ciwm6dcf80017dgc5apexxk7z"},{"post_id":"ciwm6dced0008dgc5olg7cbbf","tag_id":"ciwm6dcet000hdgc55r5i0bkg","_id":"ciwm6dcf80018dgc5ub9zzb79"},{"post_id":"ciwm6dceg000adgc5xckrfi11","tag_id":"ciwm6dcf70016dgc5zzu5psx6","_id":"ciwm6dcfa001cdgc5xtaw91mk"},{"post_id":"ciwm6dceg000adgc5xckrfi11","tag_id":"ciwm6dcf90019dgc58i5gc8tf","_id":"ciwm6dcfb001ddgc5bg153yy2"},{"post_id":"ciwm6dceg000adgc5xckrfi11","tag_id":"ciwm6dcf9001adgc5oatleckr","_id":"ciwm6dcfb001fdgc5lngw7q1k"},{"post_id":"ciwm6dcen000cdgc5kvfxvrz0","tag_id":"ciwm6dcf9001bdgc5rjf9ruic","_id":"ciwm6dcfe001hdgc5yq4usdlf"},{"post_id":"ciwm6dcen000cdgc5kvfxvrz0","tag_id":"ciwm6dcfb001edgc5mivcvvj1","_id":"ciwm6dcfe001idgc5hbidcj92"},{"post_id":"ciwm6dcen000cdgc5kvfxvrz0","tag_id":"ciwm6dcet000hdgc55r5i0bkg","_id":"ciwm6dcfe001jdgc5q849ok7v"},{"post_id":"ciwm6dchy001kdgc5egi128n3","tag_id":"ciwm6dcf70016dgc5zzu5psx6","_id":"ciwm6dci1001ldgc5raqmwkzr"},{"post_id":"ciwm6dchy001kdgc5egi128n3","tag_id":"ciwm6dcet000hdgc55r5i0bkg","_id":"ciwm6dci1001mdgc5pmzeqiio"}],"Tag":[{"name":"Graphite","_id":"ciwm6dce70004dgc5a1g7jx2g"},{"name":"Scrapy","_id":"ciwm6dcef0009dgc5gnilicn0"},{"name":"CentOS","_id":"ciwm6dcep000edgc5fmel44yl"},{"name":"python","_id":"ciwm6dcet000hdgc55r5i0bkg"},{"name":"ghost.py","_id":"ciwm6dcf2000tdgc5sqo6d3ga"},{"name":"English","_id":"ciwm6dcf3000wdgc5arjcb24w"},{"name":"Hexo","_id":"ciwm6dcf5000zdgc5k0m302jw"},{"name":"PIL","_id":"ciwm6dcf60012dgc55ui6igo3"},{"name":"HMM","_id":"ciwm6dcf70016dgc5zzu5psx6"},{"name":"MEMM","_id":"ciwm6dcf90019dgc58i5gc8tf"},{"name":"CRF","_id":"ciwm6dcf9001adgc5oatleckr"},{"name":"zhihu","_id":"ciwm6dcf9001bdgc5rjf9ruic"},{"name":"Neo4j","_id":"ciwm6dcfb001edgc5mivcvvj1"}]}}