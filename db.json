{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/random/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/random/source/css/blank.gif","path":"css/blank.gif","modified":1,"renderable":1},{"_id":"themes/random/source/css/fancybox_loading.gif","path":"css/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/random/source/css/fancybox_loading@2x.gif","path":"css/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/random/source/css/fancybox_sprite@2x.png","path":"css/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/random/source/css/highlight-railscasts.css","path":"css/highlight-railscasts.css","modified":1,"renderable":1},{"_id":"themes/random/source/css/jquery.fancybox-thumbs.css","path":"css/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/random/source/css/jquery.fancybox.css","path":"css/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/random/source/css/next.png","path":"css/next.png","modified":1,"renderable":1},{"_id":"themes/random/source/css/plyr.css","path":"css/plyr.css","modified":1,"renderable":1},{"_id":"themes/random/source/css/prev.png","path":"css/prev.png","modified":1,"renderable":1},{"_id":"themes/random/source/css/random.styl","path":"css/random.styl","modified":1,"renderable":1},{"_id":"themes/random/source/css/sprite.svg","path":"css/sprite.svg","modified":1,"renderable":1},{"_id":"themes/random/source/css/vegas.min.css","path":"css/vegas.min.css","modified":1,"renderable":1},{"_id":"themes/random/source/js/highlight.pack.js","path":"js/highlight.pack.js","modified":1,"renderable":1},{"_id":"themes/random/source/js/jquery.fancybox-thumbs.js","path":"js/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/random/source/js/jquery.fancybox.pack.js","path":"js/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/random/source/js/jquery.mousewheel.pack.js","path":"js/jquery.mousewheel.pack.js","modified":1,"renderable":1},{"_id":"themes/random/source/js/plyr.js","path":"js/plyr.js","modified":1,"renderable":1},{"_id":"themes/random/source/js/random.js","path":"js/random.js","modified":1,"renderable":1},{"_id":"themes/random/source/js/vegas.min.js","path":"js/vegas.min.js","modified":1,"renderable":1},{"_id":"themes/random/source/js/jquery-2.2.3.min.js","path":"js/jquery-2.2.3.min.js","modified":1,"renderable":1},{"_id":"themes/random/source/css/fancybox_overlay.png","path":"css/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/random/source/css/fancybox_sprite.png","path":"css/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.css","path":"css/iconfont/iconfont.css","modified":1,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.eot","path":"css/iconfont/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.svg","path":"css/iconfont/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.woff","path":"css/iconfont/iconfont.woff","modified":1,"renderable":1},{"_id":"themes/random/source/css/iconfont/iconfont.ttf","path":"css/iconfont/iconfont.ttf","modified":1,"renderable":1}],"Cache":[{"_id":"themes/random/.gitignore","hash":"6993cdad70ce92d2734ccd7a0a944e539a79e738","modified":1466575630716},{"_id":"themes/random/README.CN.md","hash":"197e736dc03c31a666b54b2c26f56c7e9c8169c6","modified":1466575630718},{"_id":"themes/random/README.md","hash":"26bc4bf3b66583fb8a9d400722de223b346d4586","modified":1466575630718},{"_id":"source/_posts/centos-scrapy-ghost.py.md","hash":"798fc36877a0777a3f6f3f42f777eb6813deaf95","modified":1502451080991},{"_id":"source/_posts/dynamic-crawling.md","hash":"5eb1c3f8b6ffd3244b625d7ff1689f76118e737b","modified":1526479989228},{"_id":"source/_posts/hello-world.md","hash":"fa5a4b86c2369466f2549fba5fb3be89bfc72209","modified":1502451015502},{"_id":"source/_posts/hmm4nlp.md","hash":"629cf25ba022b2d9a875038081628f86331e2e43","modified":1472805857690},{"_id":"themes/random/_config.yml","hash":"35268892de097360292f64a73530eab418c0f144","modified":1526096472511},{"_id":"themes/random/LICENSE","hash":"03bec2e3a3b7bad96e2688d57197c99b494dcdb5","modified":1466575630717},{"_id":"source/_posts/img2txt.md","hash":"4ff151ea11a5ebaf45d4cc58a2b550722bebf24e","modified":1469024430941},{"_id":"source/_posts/sparse-nmf.md","hash":"3cd21b9f2d9477536e1c38c49579a28b686da4e9","modified":1503057788890},{"_id":"source/categories/index.md","hash":"174e63bdf5768ee16d002dad34de812b200f4ae2","modified":1466125557734},{"_id":"source/tags/index.md","hash":"63773e15ffc42a9c4488f47fbdaf1a5f978a5c79","modified":1466125557735},{"_id":"themes/random/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1466575630522},{"_id":"themes/random/.git/config","hash":"a50b6632189a332eb4f3128bbbf30a6278addf86","modified":1481552895974},{"_id":"themes/random/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1466575625530},{"_id":"source/_posts/hmm-memm-crf.md","hash":"88105118fd132feef3932ef498d9b0a8e4b6b179","modified":1469065355744},{"_id":"source/about/index.md","hash":"bfaa941e6d4ad0cc57f630af4e699437c51e2f9b","modified":1526451353144},{"_id":"themes/random/.git/index","hash":"f68802f2b1911c3dd32549ce9196dc35efba9e0a","modified":1466585131672},{"_id":"themes/random/.git/packed-refs","hash":"fa836080405cb6079c1c640bd65cae6be761a7c7","modified":1466575630519},{"_id":"themes/random/languages/en.yml","hash":"1435cf69d9607a7912f647d861ec4a30a8c08dba","modified":1466575630720},{"_id":"themes/random/languages/zh-CN.yml","hash":"1f18252ba42703ccdecd9ce01ac57a829b99d9fe","modified":1466575630720},{"_id":"themes/random/source/favicon.ico","hash":"96b9a549337c2bec483c2879eeafa4d1f8748fed","modified":1466575630767},{"_id":"source/_posts/zhihu-link.md","hash":"5407fa07f57a425c7c414dc3bccff8fe60e45ceb","modified":1469022587806},{"_id":"themes/random/layout/archive.swig","hash":"23f96a38b04d116aedbed0820d0eb0c9a4b8622c","modified":1466575630722},{"_id":"source/_posts/graphite.md","hash":"ced5cc2f9a40721236b5222674f07af5dcc7913b","modified":1526116826503},{"_id":"themes/random/layout/category.swig","hash":"58492ccd8a863b06742f89dcac97782dfd86e43a","modified":1466575630722},{"_id":"themes/random/layout/tag.swig","hash":"340ca3d9dceff6827865ffceabbfbd6eb75a927f","modified":1466575630731},{"_id":"themes/random/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1466575625555},{"_id":"themes/random/layout/index.swig","hash":"411520dcdcf83f16e19cbf039f6aa544a6c332d3","modified":1466575630730},{"_id":"themes/random/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1466575625603},{"_id":"themes/random/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1466575625603},{"_id":"themes/random/layout/page.swig","hash":"146e6587c08e6bea08eec98b45b8e0438793cfd5","modified":1466575630730},{"_id":"themes/random/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1466575625588},{"_id":"themes/random/layout/post.swig","hash":"10570f6c48f4accf65cff253639b03ee5cd2e993","modified":1466575630731},{"_id":"themes/random/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1466575625613},{"_id":"themes/random/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1466575625614},{"_id":"themes/random/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1466575625627},{"_id":"themes/random/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1466575625629},{"_id":"themes/random/.git/logs/HEAD","hash":"f10f4700b496f0c2825b977927dee73df01dc7f1","modified":1466575630524},{"_id":"themes/random/source/css/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1466575630732},{"_id":"themes/random/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1466575625626},{"_id":"themes/random/source/css/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1466575630733},{"_id":"themes/random/source/css/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1466575630733},{"_id":"themes/random/source/css/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1466575630735},{"_id":"themes/random/source/css/highlight-railscasts.css","hash":"1bb2dd8ccba3e33aa3fd419bad757b0710ca7bf3","modified":1466575630735},{"_id":"themes/random/source/css/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1466575630762},{"_id":"themes/random/source/css/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1466575630763},{"_id":"themes/random/source/css/next.png","hash":"1bf3e61fbe6858bd9d154e23a477a06307f80436","modified":1466575630763},{"_id":"themes/random/source/css/plyr.css","hash":"29202451dd2547739b2b763952ff58c8a4d11df9","modified":1466575630764},{"_id":"themes/random/source/css/prev.png","hash":"c35bf41b4597a371e80ffdaf338b5693a082a5f4","modified":1466575630764},{"_id":"themes/random/source/css/random.styl","hash":"9a72972f08947b9e4fde8f4d085a10bca97e53e7","modified":1466582214977},{"_id":"themes/random/source/css/sprite.svg","hash":"4ed6a0d335ce214ec00fc9e56867687798a53ee3","modified":1466575630765},{"_id":"themes/random/source/css/vegas.min.css","hash":"5810e20875386f98565b69de5ca8ee1d0a6d1feb","modified":1466575630766},{"_id":"themes/random/source/js/highlight.pack.js","hash":"8407be86478389b07f54296e976a1d6f0a6cf69a","modified":1466575630786},{"_id":"themes/random/source/js/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1466575630788},{"_id":"themes/random/source/js/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1466575630795},{"_id":"themes/random/source/js/jquery.mousewheel.pack.js","hash":"5d6f224e3080fd4066f8ef5c63d3f467e9d29e66","modified":1466575630796},{"_id":"themes/random/source/js/plyr.js","hash":"58ddde47173120b23f752872a535d7b3c9166ae8","modified":1466575630797},{"_id":"themes/random/source/js/random.js","hash":"2ff5837668e5a8fe5f53dcfe9d0b8c005e58ced3","modified":1466575630798},{"_id":"themes/random/source/js/vegas.min.js","hash":"49a911f3434d0d5a7d0372129e4a80b4b4bb3923","modified":1466575630798},{"_id":"themes/random/layout/includes/baidu-tongji.swig","hash":"3b88cbae8980212859e898d7b3c16e839f99cd80","modified":1466575630722},{"_id":"themes/random/layout/includes/disqus.swig","hash":"d81bc248e3926019e21deaac52123678ae6af55b","modified":1466575630723},{"_id":"themes/random/layout/includes/duoshuo.swig","hash":"3e97d0c6b0243bb16e3d76ba3f481338486322c9","modified":1466575630723},{"_id":"themes/random/layout/includes/footer.swig","hash":"1512b1b974bc7bd0cf7a04234c9ccf32500ed2c3","modified":1466575630724},{"_id":"themes/random/layout/includes/gallery.swig","hash":"7d749acb90d424f2d04587fca396e578bf1fabb5","modified":1466575630724},{"_id":"themes/random/layout/includes/google-analytics.swig","hash":"3e4cad2175c599edd6f3e66f0a738a57e6eeffd7","modified":1466575630725},{"_id":"themes/random/layout/includes/head.swig","hash":"6dfd7afb26d79729a90bf2dd3c453d43e751d58b","modified":1466575630725},{"_id":"themes/random/layout/includes/jiathis.swig","hash":"53fb1038a56b2428c6ff3ffae01203dddc112f6a","modified":1466575630726},{"_id":"themes/random/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1466575625628},{"_id":"themes/random/layout/includes/pagination.swig","hash":"1c4c841816c7fbfd4d34c32f05797264942d6a22","modified":1466575630727},{"_id":"themes/random/layout/includes/post-title-item.swig","hash":"10238491556e3104b5ee5ca52df3eb50029cc47d","modified":1466575630727},{"_id":"themes/random/layout/includes/recent-posts.swig","hash":"281446641c5d33762bd00d4fa28447aeb2fa5d18","modified":1466575630727},{"_id":"themes/random/layout/includes/side-pagination.swig","hash":"f3d4b832aece4a22eb9a5503063ccd384dc4538d","modified":1466575630728},{"_id":"themes/random/layout/includes/social-icon.swig","hash":"665510baf91266f9693cbcb7b4b47b6f6e6a4947","modified":1466575630728},{"_id":"themes/random/layout/includes/toc.swig","hash":"704b68b52663734e00e2bd4e21c51fab92868e12","modified":1466575630729},{"_id":"themes/random/layout/includes/user-card.swig","hash":"7c17b70595754638043b3eddee626420c27dffd7","modified":1466575630729},{"_id":"themes/random/layout/includes/uyan.swig","hash":"3803b211124a7967aebf5fb96870b0f8b4be3e6f","modified":1466575630729},{"_id":"themes/random/source/js/jquery-2.2.3.min.js","hash":"223a49c329f0f0a651d142be9dadc95008678d26","modified":1466575630788},{"_id":"themes/random/source/css/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1466575630734},{"_id":"themes/random/source/css/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1466575630734},{"_id":"themes/random/.git/refs/heads/master","hash":"24ea10a0bcde23d65d704386d054ddaa0a1edbf6","modified":1466575630523},{"_id":"themes/random/source/css/iconfont/iconfont.css","hash":"1955cec6e833e3b782bb6b150682ecbb3b0d9c1f","modified":1466575630736},{"_id":"themes/random/layout/includes/layout.swig","hash":"d0a884710325964ad236c8dde66be96d34c14dda","modified":1466575630726},{"_id":"themes/random/.git/objects/pack/pack-7f0245a058983d2de7f7843f803886e99be35bae.idx","hash":"b72944ea7a99711417311977337f808833882506","modified":1466575630190},{"_id":"themes/random/source/css/iconfont/iconfont.eot","hash":"b4d3e20bd54983f7d9f95e789e9e28d056592f11","modified":1466575630737},{"_id":"themes/random/source/css/iconfont/iconfont.svg","hash":"ed867bba0ead29c55997b7b9568ee5c0499a4379","modified":1466575630737},{"_id":"themes/random/source/css/iconfont/iconfont.woff","hash":"5c122ce49b9fc0c05c0e0f88da009469e5286a55","modified":1466575630738},{"_id":"themes/random/source/css/iconfont/iconfont.ttf","hash":"3a02ec5b0453b13944311361e95081f1c3085f6d","modified":1466575630738},{"_id":"themes/random/.git/logs/refs/heads/master","hash":"f10f4700b496f0c2825b977927dee73df01dc7f1","modified":1466575630523},{"_id":"themes/random/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1466575630522},{"_id":"themes/random/.git/logs/refs/remotes/origin/HEAD","hash":"f10f4700b496f0c2825b977927dee73df01dc7f1","modified":1466575630522},{"_id":"themes/random/.git/objects/pack/pack-7f0245a058983d2de7f7843f803886e99be35bae.pack","hash":"11b9bb61b2bb694d8533c34bd174ad23e01158b8","modified":1466575630239},{"_id":"public/search.xml","hash":"ece87e6932f52381ae74a6fb078c4f610310af87","modified":1526480087967},{"_id":"public/favicon.ico","hash":"96b9a549337c2bec483c2879eeafa4d1f8748fed","modified":1526480089494},{"_id":"public/css/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1526480089495},{"_id":"public/css/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1526480089495},{"_id":"public/css/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1526480089495},{"_id":"public/css/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1526480089495},{"_id":"public/css/next.png","hash":"1bf3e61fbe6858bd9d154e23a477a06307f80436","modified":1526480089495},{"_id":"public/css/prev.png","hash":"c35bf41b4597a371e80ffdaf338b5693a082a5f4","modified":1526480089495},{"_id":"public/css/sprite.svg","hash":"4ed6a0d335ce214ec00fc9e56867687798a53ee3","modified":1526480089496},{"_id":"public/css/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1526480089496},{"_id":"public/css/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1526480089496},{"_id":"public/css/iconfont/iconfont.eot","hash":"b4d3e20bd54983f7d9f95e789e9e28d056592f11","modified":1526480089496},{"_id":"public/css/iconfont/iconfont.svg","hash":"ed867bba0ead29c55997b7b9568ee5c0499a4379","modified":1526480089496},{"_id":"public/css/iconfont/iconfont.ttf","hash":"3a02ec5b0453b13944311361e95081f1c3085f6d","modified":1526480089496},{"_id":"public/css/iconfont/iconfont.woff","hash":"5c122ce49b9fc0c05c0e0f88da009469e5286a55","modified":1526480089497},{"_id":"public/about/index.html","hash":"8cf01d0d5e9f09269786d5713e642f925dcccf88","modified":1526480091319},{"_id":"public/categories/index.html","hash":"10fcb6461e8a1b3239481105b528e801d2aaeabb","modified":1526480091320},{"_id":"public/index.html","hash":"ed852e4d904a8f1f338dc88199f4b4fac8241646","modified":1526480091323},{"_id":"public/tags/index.html","hash":"7fe18ac9ee8dcd26e3381c6d8dff1c2072725806","modified":1526480091330},{"_id":"public/2016/07/20/img2txt/index.html","hash":"dbc59637e1e54f7656c6d674429439834cae04c0","modified":1526480091330},{"_id":"public/2016/06/16/centos-scrapy-ghost.py/index.html","hash":"bbb76ddb1db1acaa423ed1d4f35cb078e0c158ae","modified":1526480091330},{"_id":"public/2016/06/12/hello-world/index.html","hash":"e92ff7152e35eea18a337de18d34d475afec11c8","modified":1526480091330},{"_id":"public/archives/index.html","hash":"4a59361d232ff8a7f300e7ef150eb65b04e3aed1","modified":1526480091330},{"_id":"public/archives/2016/index.html","hash":"018c07c2a4461b1b3ada30c0ab9882d2e56c0dd6","modified":1526480091330},{"_id":"public/archives/2016/07/index.html","hash":"31939bdea8caae00a2df9996c718248057a25354","modified":1526480091330},{"_id":"public/archives/2017/index.html","hash":"ad4be32374615e173088afc18f0354b97d7afec9","modified":1526480091330},{"_id":"public/archives/2016/06/index.html","hash":"e141cdfe8fac0ad9e1e7fa2ceb164c59ee98e744","modified":1526480091331},{"_id":"public/archives/2017/01/index.html","hash":"d7b1f343621680ceb971edf5b4fb48ec2c74ee14","modified":1526480091331},{"_id":"public/archives/2016/10/index.html","hash":"0ebaebf757092900d7e59b447b29e68f0bf21784","modified":1526480091331},{"_id":"public/archives/2017/05/index.html","hash":"46aded57ac4895164fce4598c1aa02842ab149c6","modified":1526480091331},{"_id":"public/tags/scrapy/index.html","hash":"e5eb35df7edaa272fa74ca3cbc3eefea7d9e05cb","modified":1526480091331},{"_id":"public/tags/ghost-py/index.html","hash":"b391bb710f67d3a583550249088daa96b30a657d","modified":1526480091331},{"_id":"public/tags/python/index.html","hash":"8da13cdb7fe04840199f394484703a865ccb1012","modified":1526480091331},{"_id":"public/tags/English/index.html","hash":"f9d66ea3936e4621922aef8229680e726423e4cc","modified":1526480091331},{"_id":"public/tags/CentOS/index.html","hash":"a70b4d092be0188c3b1eb462f5e5db5ec73b7def","modified":1526480091331},{"_id":"public/tags/Hexo/index.html","hash":"ad75e9662248629948faf268708b0155b50e202c","modified":1526480091331},{"_id":"public/tags/NMF/index.html","hash":"e41adae042416a68df759d7f02298dd4d1dfe318","modified":1526480091331},{"_id":"public/tags/MEMM/index.html","hash":"0a4785ceb8b5aabf5489964d3858a75a10c09048","modified":1526480091331},{"_id":"public/tags/CRF/index.html","hash":"b840f41f28db4e720abef75b9339cb7a90a0d11c","modified":1526480091331},{"_id":"public/tags/Neo4j/index.html","hash":"1f3c8929ad538437007213199a003b2b4e4a1bb7","modified":1526480091332},{"_id":"public/tags/zhihu/index.html","hash":"3a605ad96b67c348c8b11b5793c80a96a57940a7","modified":1526480091332},{"_id":"public/tags/Graphite/index.html","hash":"ab62b99cb21c4aa29759ed31422d4f9319281b8e","modified":1526480091332},{"_id":"public/tags/PIL/index.html","hash":"b6b309c096ed713bfc8a63f8678dee6535fb2946","modified":1526480091332},{"_id":"public/tags/HMM/index.html","hash":"d1bcd89640e7f3b0dd0c77b2ab0340ca91df0091","modified":1526480091332},{"_id":"public/categories/Site-Est/index.html","hash":"0a43abba611c18b9470e89799bee976e6fafaa61","modified":1526480091332},{"_id":"public/categories/ML/index.html","hash":"461b10451ba9cddf20535604cc893711ebdfcb50","modified":1526480091332},{"_id":"public/categories/Fun/index.html","hash":"85d1cb5745c40968c1b2ea533f27e7c02581386e","modified":1526480091332},{"_id":"public/categories/Crawler/index.html","hash":"f2dced0f79cd8bf1bddb57eb77f7fd08adb3eeff","modified":1526480091332},{"_id":"public/categories/NLP/index.html","hash":"2cf9a5ce76155357b4e54132b951a83575759ef6","modified":1526480091332},{"_id":"public/css/highlight-railscasts.css","hash":"a6d2043478fae5915926914cbd96fe9b706d98a6","modified":1526480091332},{"_id":"public/css/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1526480091333},{"_id":"public/css/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1526480091333},{"_id":"public/css/plyr.css","hash":"29202451dd2547739b2b763952ff58c8a4d11df9","modified":1526480091333},{"_id":"public/css/random.css","hash":"264edb971f706a5717b3dc728df3654d74023f8f","modified":1526480091333},{"_id":"public/css/vegas.min.css","hash":"5810e20875386f98565b69de5ca8ee1d0a6d1feb","modified":1526480091333},{"_id":"public/js/jquery.mousewheel.pack.js","hash":"1e1b44eb7cfade680c52d8748846425ecd809bfd","modified":1526480091333},{"_id":"public/js/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1526480091333},{"_id":"public/js/random.js","hash":"e678af85471e86c3e25a7709496d517e50008d7a","modified":1526480091333},{"_id":"public/css/iconfont/iconfont.css","hash":"4d5f8113307fbb6b99df0ee2ce5817af5137ec27","modified":1526480091333},{"_id":"public/js/vegas.min.js","hash":"8d24ba5346a600c4b77ef39d68ee924ab48c8790","modified":1526480091334},{"_id":"public/2017/05/12/dynamic-crawling/index.html","hash":"cb400f2799e4ee11b8592b67562c1a42015078c5","modified":1526480091334},{"_id":"public/2017/01/12/sparse-nmf/index.html","hash":"1546f5081132d1a38600543598c34228d108c826","modified":1526480091334},{"_id":"public/2016/10/06/graphite/index.html","hash":"b6041ad6ada8022f71783f59db8d0c58cf7cadcc","modified":1526480091334},{"_id":"public/2016/07/28/hmm4nlp/index.html","hash":"a7b7f0672b4c4c5708186a5d9a53b9b8690c438b","modified":1526480091334},{"_id":"public/2016/07/14/hmm-memm-crf/index.html","hash":"b18cd3a7333dd3e790674ed64711c6cda52fb9b0","modified":1526480091334},{"_id":"public/2016/06/29/zhihu-link/index.html","hash":"58591605f2fed4ce2cf98b4bebb30e64c85ff0e9","modified":1526480091334},{"_id":"public/js/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1526480091334},{"_id":"public/js/highlight.pack.js","hash":"87868df3be2e575e47d6218ea08c0e5922808318","modified":1526480091334},{"_id":"public/js/plyr.js","hash":"11e09c25a5821fc08880b8ab1f691ad58780bde3","modified":1526480091334},{"_id":"public/js/jquery-2.2.3.min.js","hash":"e3dbb65f2b541d842b50d37304b0102a2d5f2387","modified":1526480091335}],"Category":[{"name":"Crawler","_id":"cjh96tx2q0005hoc5sugiw1dd"},{"name":"Site Est","_id":"cjh96tx3a000ehoc5ndj8gtz1"},{"name":"NLP","_id":"cjh96tx3e000hhoc5gh66aa6q"},{"name":"Fun","_id":"cjh96tx3g000khoc5kol1plpn"},{"name":"ML","_id":"cjh96tx3j000qhoc58ms4nbe0"}],"Data":[],"Page":[{"title":"categories","date":"2016-06-11T13:59:08.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-06-11 21:59:08\ntype: \"categories\"\ncomments: false\n---\n","updated":"2016-06-17T01:05:57.734Z","path":"categories/index.html","layout":"page","_id":"cjh96tx2f0001hoc5li20sy3e","content":"","excerpt":"","more":""},{"title":"tags","date":"2016-06-11T13:57:52.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2016-06-11 21:57:52\ntype: \"tags\"\ncomments: false\n---\n","updated":"2016-06-17T01:05:57.735Z","path":"tags/index.html","layout":"page","_id":"cjh96tx2l0003hoc5l4lefxg7","content":"","excerpt":"","more":""},{"title":"About","date":"2016-06-12T11:19:01.000Z","type":"about","comments":0,"photos":["https://avatars3.githubusercontent.com/u/16516510?v=3&s=460"],"_content":"\n# About Me\nMy name is Haotian WU. I'm a postgraduate student in [School of Computer Science & Engineering](http://cse.seu.edu.cn/en/index.html) at [Southeast University](http://www.seu.edu.cn/english/main.htm) and will graduate in Summer 2018.\n* Research Interests:\n  * Blockchain\n  * Opportunistic Networks\n  * Web Information Retrieval\n* Sports: \n  * Badminton\n  * Swimming\n\n# Contacts\nE-mail address: haotianwuseu # gmail # com.\n\nThe Base64 code of my QQ number is: \n`MTc3MjQ4MzM0Mg==`\n\n# Links\n[1024ss.com](http://1024ss.com): 1024 Search Engine\n[Heiming's Blog](http://heimingx.cn): Haiming Xu's blog\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2016-06-12 19:19:01\ntype: \"about\"\ncomments: false\nphotos:\n - https://avatars3.githubusercontent.com/u/16516510?v=3&s=460\n---\n\n# About Me\nMy name is Haotian WU. I'm a postgraduate student in [School of Computer Science & Engineering](http://cse.seu.edu.cn/en/index.html) at [Southeast University](http://www.seu.edu.cn/english/main.htm) and will graduate in Summer 2018.\n* Research Interests:\n  * Blockchain\n  * Opportunistic Networks\n  * Web Information Retrieval\n* Sports: \n  * Badminton\n  * Swimming\n\n# Contacts\nE-mail address: haotianwuseu # gmail # com.\n\nThe Base64 code of my QQ number is: \n`MTc3MjQ4MzM0Mg==`\n\n# Links\n[1024ss.com](http://1024ss.com): 1024 Search Engine\n[Heiming's Blog](http://heimingx.cn): Haiming Xu's blog\n","updated":"2018-05-16T06:15:53.144Z","path":"about/index.html","layout":"page","_id":"cjh96tx5h001lhoc5456vob12","content":"<h1 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h1><p>My name is Haotian WU. I’m a postgraduate student in <a href=\"http://cse.seu.edu.cn/en/index.html\" target=\"_blank\" rel=\"external\">School of Computer Science &amp; Engineering</a> at <a href=\"http://www.seu.edu.cn/english/main.htm\" target=\"_blank\" rel=\"external\">Southeast University</a> and will graduate in Summer 2018.</p>\n<ul>\n<li>Research Interests:<ul>\n<li>Blockchain</li>\n<li>Opportunistic Networks</li>\n<li>Web Information Retrieval</li>\n</ul>\n</li>\n<li>Sports: <ul>\n<li>Badminton</li>\n<li>Swimming</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Contacts\"><a href=\"#Contacts\" class=\"headerlink\" title=\"Contacts\"></a>Contacts</h1><p>E-mail address: haotianwuseu # gmail # com.</p>\n<p>The Base64 code of my QQ number is:<br><code>MTc3MjQ4MzM0Mg==</code></p>\n<h1 id=\"Links\"><a href=\"#Links\" class=\"headerlink\" title=\"Links\"></a>Links</h1><p><a href=\"http://1024ss.com\" target=\"_blank\" rel=\"external\">1024ss.com</a>: 1024 Search Engine<br><a href=\"http://heimingx.cn\" target=\"_blank\" rel=\"external\">Heiming’s Blog</a>: Haiming Xu’s blog</p>\n","excerpt":"","more":"<h1 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h1><p>My name is Haotian WU. I’m a postgraduate student in <a href=\"http://cse.seu.edu.cn/en/index.html\">School of Computer Science &amp; Engineering</a> at <a href=\"http://www.seu.edu.cn/english/main.htm\">Southeast University</a> and will graduate in Summer 2018.</p>\n<ul>\n<li>Research Interests:<ul>\n<li>Blockchain</li>\n<li>Opportunistic Networks</li>\n<li>Web Information Retrieval</li>\n</ul>\n</li>\n<li>Sports: <ul>\n<li>Badminton</li>\n<li>Swimming</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Contacts\"><a href=\"#Contacts\" class=\"headerlink\" title=\"Contacts\"></a>Contacts</h1><p>E-mail address: haotianwuseu # gmail # com.</p>\n<p>The Base64 code of my QQ number is:<br><code>MTc3MjQ4MzM0Mg==</code></p>\n<h1 id=\"Links\"><a href=\"#Links\" class=\"headerlink\" title=\"Links\"></a>Links</h1><p><a href=\"http://1024ss.com\">1024ss.com</a>: 1024 Search Engine<br><a href=\"http://heimingx.cn\">Heiming’s Blog</a>: Haiming Xu’s blog</p>\n"}],"Post":[{"title":"CentOS 6.5 下scrapy与ghost.py的安装干货","date":"2016-06-16T05:34:08.000Z","comments":1,"_content":"之前一直在做scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。\n在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：\n* [Python 爬虫如何获取 JS 生成的 URL 和网页内容？](https://www.zhihu.com/question/21471960)\n* [Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？](https://www.zhihu.com/question/36450326)\n\n一些前人的技术博客如：\n* 开源中国上[斑ban](http://my.oschina.net/u/1024140?ft=blog)的[《使用python，scrapy写（定制）爬虫的经验，资料，杂》](http://my.oschina.net/u/1024140/blog/188154)。\n这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。\n\n上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。\n\nghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对**PyQt4**或者**PySide**的一个封装，需要安装其中一个才能运行。\n\n# PySide\n当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。\n下面是我在CentOS 6.5和python2.7.11的环境上安装scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF[源地址](http://tripleday.cn/uploads/pdf/CentOS-scrapy-ghost.py.pdf)。\n{% pdf http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf %}\n上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。\n\n# PyQt4\n我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。\n\n安装PyQt4之前是需要安装**SIP**的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。\n\n在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。\n贴一个能够成功安装的博客链接：[CentOS7.1下python2.7.10安装PyQt4](http://blog.csdn.net/dgatiger/article/details/50331361)\n\n文中SIP的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz\ntar xvf sip-4.17.tar.gz\ncd sip-4.17\npython configure.py\nmake & make install & make clean\n```\nPyQt的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz\ntar xvf PyQt-x11-gpl-4.11.4.tar.gz\ncd PyQt-x11-gpl-4.11.4\npython configure.py -q  /usr/lib64/qt4/bin/qmake\nmake & make install & make clean\n```\n这篇博客里用的是**sip-4.17**和**PyQt-4.11.4**是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。\n","source":"_posts/centos-scrapy-ghost.py.md","raw":"title: CentOS 6.5 下scrapy与ghost.py的安装干货\ndate: 2016-06-16 13:34:08\ncomments: true\ntags: \n - CentOS\n - scrapy\n - ghost.py\n - python\ncategories: Crawler\n---\n之前一直在做scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。\n在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：\n* [Python 爬虫如何获取 JS 生成的 URL 和网页内容？](https://www.zhihu.com/question/21471960)\n* [Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？](https://www.zhihu.com/question/36450326)\n\n一些前人的技术博客如：\n* 开源中国上[斑ban](http://my.oschina.net/u/1024140?ft=blog)的[《使用python，scrapy写（定制）爬虫的经验，资料，杂》](http://my.oschina.net/u/1024140/blog/188154)。\n这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。\n\n上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。\n\nghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对**PyQt4**或者**PySide**的一个封装，需要安装其中一个才能运行。\n\n# PySide\n当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。\n下面是我在CentOS 6.5和python2.7.11的环境上安装scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF[源地址](http://tripleday.cn/uploads/pdf/CentOS-scrapy-ghost.py.pdf)。\n{% pdf http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf %}\n上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。\n\n# PyQt4\n我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。\n\n安装PyQt4之前是需要安装**SIP**的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。\n\n在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。\n贴一个能够成功安装的博客链接：[CentOS7.1下python2.7.10安装PyQt4](http://blog.csdn.net/dgatiger/article/details/50331361)\n\n文中SIP的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz\ntar xvf sip-4.17.tar.gz\ncd sip-4.17\npython configure.py\nmake & make install & make clean\n```\nPyQt的安装代码：\n```sh\nwget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz\ntar xvf PyQt-x11-gpl-4.11.4.tar.gz\ncd PyQt-x11-gpl-4.11.4\npython configure.py -q  /usr/lib64/qt4/bin/qmake\nmake & make install & make clean\n```\n这篇博客里用的是**sip-4.17**和**PyQt-4.11.4**是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。\n","slug":"centos-scrapy-ghost.py","published":1,"updated":"2017-08-11T11:31:20.991Z","layout":"post","photos":[],"link":"","_id":"cjh96tx220000hoc54mj5s4ew","content":"<p>之前一直在做scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。<br>在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/21471960\" target=\"_blank\" rel=\"external\">Python 爬虫如何获取 JS 生成的 URL 和网页内容？</a></li>\n<li><a href=\"https://www.zhihu.com/question/36450326\" target=\"_blank\" rel=\"external\">Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？</a></li>\n</ul>\n<p>一些前人的技术博客如：</p>\n<ul>\n<li>开源中国上<a href=\"http://my.oschina.net/u/1024140?ft=blog\" target=\"_blank\" rel=\"external\">斑ban</a>的<a href=\"http://my.oschina.net/u/1024140/blog/188154\" target=\"_blank\" rel=\"external\">《使用python，scrapy写（定制）爬虫的经验，资料，杂》</a>。<br>这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。</li>\n</ul>\n<p>上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。</p>\n<p>ghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对<strong>PyQt4</strong>或者<strong>PySide</strong>的一个封装，需要安装其中一个才能运行。</p>\n<h1 id=\"PySide\"><a href=\"#PySide\" class=\"headerlink\" title=\"PySide\"></a>PySide</h1><p>当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。<br>下面是我在CentOS 6.5和python2.7.11的环境上安装scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF<a href=\"http://tripleday.cn/uploads/pdf/CentOS-scrapy-ghost.py.pdf\" target=\"_blank\" rel=\"external\">源地址</a>。<br>\n\n\t<div class=\"row\">\n    <embed src=\"http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n<br>上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。</p>\n<h1 id=\"PyQt4\"><a href=\"#PyQt4\" class=\"headerlink\" title=\"PyQt4\"></a>PyQt4</h1><p>我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。</p>\n<p>安装PyQt4之前是需要安装<strong>SIP</strong>的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。</p>\n<p>在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。<br>贴一个能够成功安装的博客链接：<a href=\"http://blog.csdn.net/dgatiger/article/details/50331361\" target=\"_blank\" rel=\"external\">CentOS7.1下python2.7.10安装PyQt4</a></p>\n<p>文中SIP的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz</span><br><span class=\"line\">tar xvf sip-4.17.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> sip-4.17</span><br><span class=\"line\">python configure.py</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>PyQt的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\">tar xvf PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> PyQt-x11-gpl-4.11.4</span><br><span class=\"line\">python configure.py -q  /usr/lib64/qt4/bin/qmake</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>这篇博客里用的是<strong>sip-4.17</strong>和<strong>PyQt-4.11.4</strong>是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。</p>\n","excerpt":"","more":"<p>之前一直在做scrapy中关于网页动态内容的获取，主要目标是想获得javascript渲染后的网页html源码。<br>在转向使用ghost.py来做脚本解析之前的挖坑爬坑过程中，我已经造访过我所知的大大小小各种论坛、博客以及贴吧和知乎。其中有大方向上指导意义的有知乎里的相关问题：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/21471960\">Python 爬虫如何获取 JS 生成的 URL 和网页内容？</a></li>\n<li><a href=\"https://www.zhihu.com/question/36450326\">Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？</a></li>\n</ul>\n<p>一些前人的技术博客如：</p>\n<ul>\n<li>开源中国上<a href=\"http://my.oschina.net/u/1024140?ft=blog\">斑ban</a>的<a href=\"http://my.oschina.net/u/1024140/blog/188154\">《使用python，scrapy写（定制）爬虫的经验，资料，杂》</a>。<br>这篇博客里的总结涉及到爬虫的很多方面，看后受益匪浅，作者乃真大神，有很丰富的爬虫经验。</li>\n</ul>\n<p>上述几个干货里提到的方法，我基本都去了解了一下，也照着其中的几个方向挖过坑，过些时间我把我在这方面爬的所有坑都总结到一篇博客里。</p>\n<p>ghost.py算是我掉坑里时间最长的，也是差点就成功的一个，到现在也弃了，弃的原因日后再说。其实，用ghost.py是在PyQt4的基础上转过去的，ghost.py是对<strong>PyQt4</strong>或者<strong>PySide</strong>的一个封装，需要安装其中一个才能运行。</p>\n<h1 id=\"PySide\"><a href=\"#PySide\" class=\"headerlink\" title=\"PySide\"></a>PySide</h1><p>当然挖坑的第一步就是安装环境了，win7上安装简便得多，但到linux下就没那么舒服了。<br>下面是我在CentOS 6.5和python2.7.11的环境上安装scrapy、PySide和Ghost.py过程中查到的有用资料的整合。如嫌下面的字太小，可戳此PDF<a href=\"http://tripleday.cn/uploads/pdf/CentOS-scrapy-ghost.py.pdf\">源地址</a>。<br>\n\n\t<div class=\"row\">\n    <embed src=\"http://tripleday.github.io/uploads/pdf/CentOS-scrapy-ghost.py.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n<br>上面的PDF里ghost.py用的是PySide。PySide和PyQt4的功能和API近乎一致，我的理解是：PyQt4是PySide的商业化版本，两者都是Qt进行维护。</p>\n<h1 id=\"PyQt4\"><a href=\"#PyQt4\" class=\"headerlink\" title=\"PyQt4\"></a>PyQt4</h1><p>我曾经在用PySide的时候遇到无法解决的Core Dump的bug，想转去试一下PyQt4看会不会好点，虽然结果是bug更频繁，但我还是列出安装PyQt4的一些小tips吧，希望后来人少走点弯路。</p>\n<p>安装PyQt4之前是需要安装<strong>SIP</strong>的。SIP是一个自动为C和C++库生成Python扩展模块的工具。为了方便开发PyQt，SIP于1998被“Riverbank Computing”公司创造出来。不过，SIP不专用于PyQt，而是适用于所有的C和C++库。但据说好像现在只有PyQt一直在坚持用SIP，很多别人家的项目在需要对C或C++封装调用的时候都用SWIG了。</p>\n<p>在安装过程中最让人抓狂的SIP和PyQt4的版本对应问题：某个固定版本的SIP只能支持少数几个版本的PyQt，有比较麻烦的兼容性问题。曾经在安装时，要么提示SIP版本过高，PyQt无法编译；要么PyQt版本过高，SIP不能支持。<br>贴一个能够成功安装的博客链接：<a href=\"http://blog.csdn.net/dgatiger/article/details/50331361\">CentOS7.1下python2.7.10安装PyQt4</a></p>\n<p>文中SIP的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/sip/sip-4.17/sip-4.17.tar.gz</span><br><span class=\"line\">tar xvf sip-4.17.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> sip-4.17</span><br><span class=\"line\">python configure.py</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>PyQt的安装代码：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://downloads.sourceforge.net/project/pyqt/PyQt4/PyQt-4.11.4/PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\">tar xvf PyQt-x11-gpl-4.11.4.tar.gz</span><br><span class=\"line\"><span class=\"built_in\">cd</span> PyQt-x11-gpl-4.11.4</span><br><span class=\"line\">python configure.py -q  /usr/lib64/qt4/bin/qmake</span><br><span class=\"line\">make &amp; make install &amp; make clean</span><br></pre></td></tr></table></figure></p>\n<p>这篇博客里用的是<strong>sip-4.17</strong>和<strong>PyQt-4.11.4</strong>是能够成功的一对版本。另外，Python2.7最高只能支持到PyQt4，PyQt5好像需要Python3.X的环境；同时Python2与Python3的兼容性也有些问题。所以为了避免版本上的麻烦，个人建议Python2还是老老实实用PyQt4，Python3的也使用对应的PyQt5。</p>\n"},{"title":"基于scrapy的动态网页采集方案总结","date":"2017-05-12T04:25:31.000Z","comments":1,"_content":"基于scrapy的动态网页采集一直是个难点，而且如果想达到工程级别的抓取，需要有个高效率的解决方案。我列出了几个曾经尝试过的方案和它们的特点。\n\n# 基于PyV8等脚本解析引擎\n\n这类方案的原理是利用开源浏览器项目中的脚本解释引擎，实现相关脚本片段的解析，从而获得动态页面主体内容与超连接网络地址。自行构建脚本环境的方法以JavaScript脚本解析引擎为基础，构造解析动态页面脚本程序片段的环境。由于JavaScript脚本解析引擎只对JavaScript语言的基本对象做了支持，无法解析动态页面嵌入的JavaScript片段所使用的浏览器DOM对象，因而在利用这些脚本解析引擎解析动态页面脚本片段时，需要自己实现DOM对象，使脚本解析引擎支持浏览器DOM对象。\n![网页中Javascript片段示例](/uploads/img/20170512/js.png)\n这种方法能够在非web环境下执行独立的Javascript语句，可以细粒度地控制网页中Javascript的执行。但由于实际抓取时缺少web环境，无法对如上图所示的DOM树等对象进行操作，因此这种方案很难适用。\n\n\n# 自动化测试工具 + 浏览器\n\n这类方案使用开源浏览器渲染整个动态页面，从浏览器输出结果中提取页面主体内容与超链接网络地址。Selenium是由thought Works公司开发的web自动化测试工具。它直接在浏览器中运行，就像真正的用户在操作一样。Selenium框架底层使用Javascript模拟真实用户对浏览器进行操作。当执测试脚本时，浏览器自动按照做出相应的操作，就仿佛真实用户一样，因而可以从终端用户的角度测试Web应用。下图是在并发数为4，抓取20条URL情况下各浏览器性能对比：\n![并发数4，抓取20条URL各浏览器性能对比](/uploads/img/20170512/browsers.png)\n这种方法能够完整得到浏览器渲染过的动态网页源码，但它需要调用外部程序，CPU和内存资源占用较高。由上图可以发现各个浏览器的用时、CPU占用和内存占用对实际应用来说是无法接受的；同时，该方案执行了很多和页面的文本内容和链接无关的脚本片段，针对性不够强，会使得整个系统的性能消耗过大，导致其效率低下，严重影响数据的采集速度。\n\n\n# 基于QtWebkit的下载中间件\n\nWebKit 是一个开源的浏览器引擎，相比于其他浏览器引擎来说，Webkit的优势在于高效稳定，兼容性好，且源码结构清晰，易于维护。而QtWebkit是集成了Qt接口的Webkit实现，底层由C++开发，本方案中使用的是该内核的Python封装PyQt。在具体实现中，我们单独构建一个独立的下载中间件，在process_response函数中对网页源码进行动态渲染，并将渲染后得到的HTML代码替换源码。\n\n它能够利用其中的脚本解析引擎执行页面中嵌入的JavaScript脚本程序片段；无需调用外部程序，系统资源占用较小。但这种方案很难适用，因为QtWebkit的调用方法与下载中间件的结构设计无法统一，导致爬虫退出时QtWebkit异常崩溃；下载中间件渲染网页时，其他请求会被阻塞，渲染效率较低。\n\n\n# 基于ghost.py的下载中间件\n\nghost.py是对QtWebkit进一步封装，封装后的调用方法与下载中间件的结构能够较好地结合，解决了爬虫退出时的崩溃问题；同时，ghost.py能够创建DOM对象，构造解析动态页面脚本程序片段的环境，支持单条Javascript语句的执行；在具体实现中，我们仍然构建一个独立的下载中间件，处理返回结果时进行动态渲染，并将渲染后得到的HTML代码替换源码。\n\n这种方法可以细粒度地控制渲染细节，比如禁用图片设置、资源加载超时设置等。但是，下载中间件是处于核心引擎和下载器之间，爬虫运行时只有一个示例对象，这样的系统定位导致网页渲染仍然是阻塞式的，执行效率较低。\n\n\n# 基于ghost.py的下载器\n\n改造scrapy的下载器，在下载器中使用ghost.py完成网页源码解压和动态网页渲染的步骤，这种方案的目的是尝试在下载器多线程并发下载的同时能够并发渲染，以期能够提高下载器的抓取效率。关于下载中间件渲染和下载器渲染的系统结构区别如下图\n![下载中间件与下载器示意图](/uploads/img/20170512/middle&downloader.png)\n我们的测试结果表明这种方案下爬虫的CPU使用率较低，下载器的线程无法利用多核执行；网络I/O和动态解析等待耗时长，数据抓取效率难以提升。这是因为：Python语言具有全局解释器锁的特性，它的作用在于让同一时刻只能有一个线程对于Python对象进行操作，这使得逻辑上并发的线程物理上仍然是顺序执行的；网页的渲染缺少异步处理的能力，单个线程会被挂起等待，直到渲染结果返回才会唤醒继续执行，这也导致数据抓取效率的降低。\n\n在我们使用ghost.py的过程，我们遇到一个非常棘手的内存段错误的bug，爬虫每次在爬取到几百条URL之后会出现core dump异常并自动结束程序。我们使用dmesg命令检测内核环缓冲，了解系统的启动信息，得到如下的错误信息提示：\n![内存段错误提示](/uploads/img/20170512/bug.png)\n\n我们计算得到libQtWebkit.so.4.8.1中出错函数的相对地址： A1E7F9。然后利用gdb调试反汇编代码，定位到运行出错的机器码如下图：\n![定位至出错的机器码](/uploads/img/20170512/error.png)\n经过初步分析，我们认为出错发生在程序访问eax地址时，内存读操作越界。该错误出现在QtWebkit中CSS色彩梯度渐变渲染函数内，这应该是PyQt 4.8.1版本中的一个小bug，而修改源代码涉及的工作较为复杂且工程浩大，故也因此舍弃了该方案。\n\n\n# 基于[splash](https://github.com/scrapinghub/splash)渲染服务\n\n通过对前面几种技术的分析和测试，我们发现这些方案都存在服务器资源利用效率不高、运行效率低和不稳定性等问题，于是我们考虑引入一种基于splash服务的动态解析方案。splash是基于Twisted异步处理框架和Qt开发框架实现的、对外提供HTTP API的一种网页渲染服务，其中Twisted使渲染服务具有异步处理能力，可以发挥Webkit的并发能力。结合了splash的scrapy系统架构和工作方式如下图：\n![scrapy+splash示意图](/uploads/img/20170512/splash.png)\n\nsplash的动态渲染服务是我们最终采用的方案，但它在应用时有如下问题：splash的启停和参数设置无法由scrapy控制；splash运行时有极小概率崩溃；splash的endpoint参数设置对抓取效率和内存占用影响较大。\n\n虽然splash内部采用了Twisted异步处理框架，但这种方法只能节省线程等待网络IO和CPU渲染的时间，splash仍然会受困于Python全局解释器锁的限制，无法有效地利用多个CPU内核进行处理。单个splash即使满负荷运行也只能占用一个内核。而且原有方案是一个scrapy进程关联一个splash服务，splash的启停由scrapy单独控制，不与别的scrapy进程共享。这种即时开关splash服务的方法本省会消耗系统宝贵的计算资源，也会给整个爬虫系统的运行带来不稳定性。同时，不同splash服务之间的负载不均衡，也会给系统整体的抓取性能带来损害。\n\n所以，我们利用nginx反向代理服务来实现splash服务的负载均衡。nginx是一款轻量级的、高性能的Web 服务器/反向代理服务器，这里我们将它用作反向代理服务器来实现负载均衡。反向代理方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。\n\nnginx可以很容易通过模块扩展，所以nginx所拥有的功能集合也是很大的。nginx中的upstream负载均衡支持下面几种方式：轮询（默认）：按照时间顺序对所有服务器一个一个的访问，如果有服务器宕机，会自动剔除；权重：服务器的访问概率和权重成正比，这个可以在服务器配置不均的时候进行配置；IP哈希：对每个请求的IP进行哈希计算，并按照一定的规则分配对应的服务器；公平：按照每台服务器的响应时间来分配请求，响应时间小的优先分配；URL哈希：按照访问URL的哈希值来分配请求。\n\n下面的测试中，我们使用单个scrapy进程进行抓取，一共抓取991条URL，通过nginx反向代理请求不同数量的splash渲染服务，分别在scrapy请求并发数为50、100和150三种情况下进行对比。其中nginx在实现负载均衡时采用了默认的轮训方式。测试结果如下图：\n![不同splash数量的用时对比](/uploads/img/20170512/splashs.png)\n从图中可以发现，在相同的scrapy请求并发数情况下，利用更多的splash渲染服务能够明显地获得更高的抓取效率；但如果在拥有同样数量的splash服务时，单纯地去提高scrapy请求并发数，得到的抓取效率提升就显得相对有限，这很可能是因为整体的性能瓶颈此时已经不再是渲染需要的CPU利用率了，而是变成scrapy自身处理效率及整体网络通信的瓶颈。\n\n## splash的启动\n\nsplash既可以在程序里使用依赖docker.py进行启动，或者在命令行中人工启动。通过程序启动的示例如下：\n```sh\ncon = client.create_container(image=\"scrapinghub/splash\", volumes=[\"/etc/splash/filters\"], ports=[8050], host_config=client.create_host_config(port_bindings={8050:port_use}, binds=[\"/home/spider/Downloads:/etc/splash/filters\"]))\n```\n其中参数image是该容器启动的映像名字；volume代表容器内部使用的挂载，其中的地址与外部文件无关，是定义在容器内一块文件；ports代表该容器会监听自己内部的8050端口，这是splash内部固定设置，不建议更改；host_config是建立主机和容器的一种绑定关系的设置，绑定包括两部分内容，一个是内部的8050端口和外部port_use端口的绑定，如果以后确定scrapy请求的端口号设置为8060，则将port_use设置为8060，另外一个是外部文件和容器内部挂载的绑定，冒号后面的目录是之前volumes的值，冒号之前的目录则填写你自定义过滤文件的目录，默认取名为default.txt，例如在后面会提到的请求过滤规则文件default.txt我们放在\"/home/spider/Downloads\"目录下，则像上述代码一样填写即可。\n\nsplash还有另外一种命令行人工启动的方法，需要敲入的命令是：\n```sh\n[root@slave1 ~]# docker run -p 8053:8050 –v /home/spider/Downloads:/etc/splash/filters scrapinghub/splash\n```\n这里的-p后跟的8053代表主机提供scrapy的请求端口，8050仍然是docker容器内部定义的固定端口；-v仍然是对volumes的配置，冒号前为本机实际的文件目录，冒号后是容器内部的文件系统，不用更改；最后添加的scrapinghub/splash是容器要启动的映像名称。\n\n## splash的使用\n* 在scrapy的setting.py文件中添加：\n```python\nSPLASH_URL = 'http://192.168.0.1:8053'\n```\n* 在scrapy的setting.py文件中修改下载中间件的设置：\n```python\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\n```\n* 在scrapy的setting.py文件中添加去重中间件：\n```python\nSPIDER_MIDDLEWARES = {\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n}\n```\n* 设置定制的去重类：\n```python\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter' \n```\n* 设置URL使用splash渲染服务：\n```python\nyield scrapy.Request(url, self.parse_result, meta={\n    \t\t'splash': {\n        \t\t'args': {\n            \t\t# set rendering arguments here\n            \t\t'html': 1,\n        \t\t},\n        \t'endpoint': 'render.json',  # optional; default is render.json\n        \t'splash_url': '<url>',      # optional; overrides SPLASH_URL\n    \t\t}\n})\n```\n\n## splash请求过滤文件\n\n```sh\n! customize\n.css|\n.jpg|\n.png|\n.gif|\n\t\n! Fork of: ChinaList https://github.com/chinalist/chinalist\n/adflash/*\n/attachments/ad/*\n/tan1.js\n@@||poster.weather.com.cn/p_files/base/$image\n@@||union.bokecc.com/crossdomain.xml\nhk.search.auctions.yahoo.com###yauadysmh\nhk.yahoo.com###mntl1\n||114lm.com^$third-party\n||116b.com^$third-party\n……\n```\n上述代码列举了CSS文件，图片文件和部分广告链接等。将此文件改名为default.txt即可。\n\n\n## nginx负载均衡的配置和使用\n\n* 安装nginx，只需要命令行使用yum install即可：\n```sh\n[root@slave1 ~]# yum install nginx\n```\n* 运行前修改配置文件/etc/nginx/nginx.conf，在http处添加：\n```sh\nhttp { \n  \t  upstream splash{\n        \t server 223.3.95.199:8051;\n        \t server 223.3.95.199:8052;\n        \t server 223.3.95.199:8053;\n       \t  }\n}\n```\n* 再修改配置文件/etc/nginx/conf.d/default.conf：\n```sh\nserver {\n \t   listen         8050;\n \t   server_name    mySplash;\n\t\n    \tlocation / {\n        \tproxy_pass http://splash;\n   \t }\n}\n```\n* 开启nginx进程：\n```sh\n[root@slave1 spider]# /usr/sbin/nginx\n```\n* 重启或关闭进程：\n```sh\n[root@slave1 spider]# /usr/sbin/nginx –s reload\n[root@slave1 spider]# /usr/sbin/nginx –s stop\n```\n","source":"_posts/dynamic-crawling.md","raw":"title: 基于scrapy的动态网页采集方案总结\ndate: 2017-05-12 12:25:31\ncomments: true\ntags: \n - scrapy\n - python\ncategories: Crawler\n---\n基于scrapy的动态网页采集一直是个难点，而且如果想达到工程级别的抓取，需要有个高效率的解决方案。我列出了几个曾经尝试过的方案和它们的特点。\n\n# 基于PyV8等脚本解析引擎\n\n这类方案的原理是利用开源浏览器项目中的脚本解释引擎，实现相关脚本片段的解析，从而获得动态页面主体内容与超连接网络地址。自行构建脚本环境的方法以JavaScript脚本解析引擎为基础，构造解析动态页面脚本程序片段的环境。由于JavaScript脚本解析引擎只对JavaScript语言的基本对象做了支持，无法解析动态页面嵌入的JavaScript片段所使用的浏览器DOM对象，因而在利用这些脚本解析引擎解析动态页面脚本片段时，需要自己实现DOM对象，使脚本解析引擎支持浏览器DOM对象。\n![网页中Javascript片段示例](/uploads/img/20170512/js.png)\n这种方法能够在非web环境下执行独立的Javascript语句，可以细粒度地控制网页中Javascript的执行。但由于实际抓取时缺少web环境，无法对如上图所示的DOM树等对象进行操作，因此这种方案很难适用。\n\n\n# 自动化测试工具 + 浏览器\n\n这类方案使用开源浏览器渲染整个动态页面，从浏览器输出结果中提取页面主体内容与超链接网络地址。Selenium是由thought Works公司开发的web自动化测试工具。它直接在浏览器中运行，就像真正的用户在操作一样。Selenium框架底层使用Javascript模拟真实用户对浏览器进行操作。当执测试脚本时，浏览器自动按照做出相应的操作，就仿佛真实用户一样，因而可以从终端用户的角度测试Web应用。下图是在并发数为4，抓取20条URL情况下各浏览器性能对比：\n![并发数4，抓取20条URL各浏览器性能对比](/uploads/img/20170512/browsers.png)\n这种方法能够完整得到浏览器渲染过的动态网页源码，但它需要调用外部程序，CPU和内存资源占用较高。由上图可以发现各个浏览器的用时、CPU占用和内存占用对实际应用来说是无法接受的；同时，该方案执行了很多和页面的文本内容和链接无关的脚本片段，针对性不够强，会使得整个系统的性能消耗过大，导致其效率低下，严重影响数据的采集速度。\n\n\n# 基于QtWebkit的下载中间件\n\nWebKit 是一个开源的浏览器引擎，相比于其他浏览器引擎来说，Webkit的优势在于高效稳定，兼容性好，且源码结构清晰，易于维护。而QtWebkit是集成了Qt接口的Webkit实现，底层由C++开发，本方案中使用的是该内核的Python封装PyQt。在具体实现中，我们单独构建一个独立的下载中间件，在process_response函数中对网页源码进行动态渲染，并将渲染后得到的HTML代码替换源码。\n\n它能够利用其中的脚本解析引擎执行页面中嵌入的JavaScript脚本程序片段；无需调用外部程序，系统资源占用较小。但这种方案很难适用，因为QtWebkit的调用方法与下载中间件的结构设计无法统一，导致爬虫退出时QtWebkit异常崩溃；下载中间件渲染网页时，其他请求会被阻塞，渲染效率较低。\n\n\n# 基于ghost.py的下载中间件\n\nghost.py是对QtWebkit进一步封装，封装后的调用方法与下载中间件的结构能够较好地结合，解决了爬虫退出时的崩溃问题；同时，ghost.py能够创建DOM对象，构造解析动态页面脚本程序片段的环境，支持单条Javascript语句的执行；在具体实现中，我们仍然构建一个独立的下载中间件，处理返回结果时进行动态渲染，并将渲染后得到的HTML代码替换源码。\n\n这种方法可以细粒度地控制渲染细节，比如禁用图片设置、资源加载超时设置等。但是，下载中间件是处于核心引擎和下载器之间，爬虫运行时只有一个示例对象，这样的系统定位导致网页渲染仍然是阻塞式的，执行效率较低。\n\n\n# 基于ghost.py的下载器\n\n改造scrapy的下载器，在下载器中使用ghost.py完成网页源码解压和动态网页渲染的步骤，这种方案的目的是尝试在下载器多线程并发下载的同时能够并发渲染，以期能够提高下载器的抓取效率。关于下载中间件渲染和下载器渲染的系统结构区别如下图\n![下载中间件与下载器示意图](/uploads/img/20170512/middle&downloader.png)\n我们的测试结果表明这种方案下爬虫的CPU使用率较低，下载器的线程无法利用多核执行；网络I/O和动态解析等待耗时长，数据抓取效率难以提升。这是因为：Python语言具有全局解释器锁的特性，它的作用在于让同一时刻只能有一个线程对于Python对象进行操作，这使得逻辑上并发的线程物理上仍然是顺序执行的；网页的渲染缺少异步处理的能力，单个线程会被挂起等待，直到渲染结果返回才会唤醒继续执行，这也导致数据抓取效率的降低。\n\n在我们使用ghost.py的过程，我们遇到一个非常棘手的内存段错误的bug，爬虫每次在爬取到几百条URL之后会出现core dump异常并自动结束程序。我们使用dmesg命令检测内核环缓冲，了解系统的启动信息，得到如下的错误信息提示：\n![内存段错误提示](/uploads/img/20170512/bug.png)\n\n我们计算得到libQtWebkit.so.4.8.1中出错函数的相对地址： A1E7F9。然后利用gdb调试反汇编代码，定位到运行出错的机器码如下图：\n![定位至出错的机器码](/uploads/img/20170512/error.png)\n经过初步分析，我们认为出错发生在程序访问eax地址时，内存读操作越界。该错误出现在QtWebkit中CSS色彩梯度渐变渲染函数内，这应该是PyQt 4.8.1版本中的一个小bug，而修改源代码涉及的工作较为复杂且工程浩大，故也因此舍弃了该方案。\n\n\n# 基于[splash](https://github.com/scrapinghub/splash)渲染服务\n\n通过对前面几种技术的分析和测试，我们发现这些方案都存在服务器资源利用效率不高、运行效率低和不稳定性等问题，于是我们考虑引入一种基于splash服务的动态解析方案。splash是基于Twisted异步处理框架和Qt开发框架实现的、对外提供HTTP API的一种网页渲染服务，其中Twisted使渲染服务具有异步处理能力，可以发挥Webkit的并发能力。结合了splash的scrapy系统架构和工作方式如下图：\n![scrapy+splash示意图](/uploads/img/20170512/splash.png)\n\nsplash的动态渲染服务是我们最终采用的方案，但它在应用时有如下问题：splash的启停和参数设置无法由scrapy控制；splash运行时有极小概率崩溃；splash的endpoint参数设置对抓取效率和内存占用影响较大。\n\n虽然splash内部采用了Twisted异步处理框架，但这种方法只能节省线程等待网络IO和CPU渲染的时间，splash仍然会受困于Python全局解释器锁的限制，无法有效地利用多个CPU内核进行处理。单个splash即使满负荷运行也只能占用一个内核。而且原有方案是一个scrapy进程关联一个splash服务，splash的启停由scrapy单独控制，不与别的scrapy进程共享。这种即时开关splash服务的方法本省会消耗系统宝贵的计算资源，也会给整个爬虫系统的运行带来不稳定性。同时，不同splash服务之间的负载不均衡，也会给系统整体的抓取性能带来损害。\n\n所以，我们利用nginx反向代理服务来实现splash服务的负载均衡。nginx是一款轻量级的、高性能的Web 服务器/反向代理服务器，这里我们将它用作反向代理服务器来实现负载均衡。反向代理方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。\n\nnginx可以很容易通过模块扩展，所以nginx所拥有的功能集合也是很大的。nginx中的upstream负载均衡支持下面几种方式：轮询（默认）：按照时间顺序对所有服务器一个一个的访问，如果有服务器宕机，会自动剔除；权重：服务器的访问概率和权重成正比，这个可以在服务器配置不均的时候进行配置；IP哈希：对每个请求的IP进行哈希计算，并按照一定的规则分配对应的服务器；公平：按照每台服务器的响应时间来分配请求，响应时间小的优先分配；URL哈希：按照访问URL的哈希值来分配请求。\n\n下面的测试中，我们使用单个scrapy进程进行抓取，一共抓取991条URL，通过nginx反向代理请求不同数量的splash渲染服务，分别在scrapy请求并发数为50、100和150三种情况下进行对比。其中nginx在实现负载均衡时采用了默认的轮训方式。测试结果如下图：\n![不同splash数量的用时对比](/uploads/img/20170512/splashs.png)\n从图中可以发现，在相同的scrapy请求并发数情况下，利用更多的splash渲染服务能够明显地获得更高的抓取效率；但如果在拥有同样数量的splash服务时，单纯地去提高scrapy请求并发数，得到的抓取效率提升就显得相对有限，这很可能是因为整体的性能瓶颈此时已经不再是渲染需要的CPU利用率了，而是变成scrapy自身处理效率及整体网络通信的瓶颈。\n\n## splash的启动\n\nsplash既可以在程序里使用依赖docker.py进行启动，或者在命令行中人工启动。通过程序启动的示例如下：\n```sh\ncon = client.create_container(image=\"scrapinghub/splash\", volumes=[\"/etc/splash/filters\"], ports=[8050], host_config=client.create_host_config(port_bindings={8050:port_use}, binds=[\"/home/spider/Downloads:/etc/splash/filters\"]))\n```\n其中参数image是该容器启动的映像名字；volume代表容器内部使用的挂载，其中的地址与外部文件无关，是定义在容器内一块文件；ports代表该容器会监听自己内部的8050端口，这是splash内部固定设置，不建议更改；host_config是建立主机和容器的一种绑定关系的设置，绑定包括两部分内容，一个是内部的8050端口和外部port_use端口的绑定，如果以后确定scrapy请求的端口号设置为8060，则将port_use设置为8060，另外一个是外部文件和容器内部挂载的绑定，冒号后面的目录是之前volumes的值，冒号之前的目录则填写你自定义过滤文件的目录，默认取名为default.txt，例如在后面会提到的请求过滤规则文件default.txt我们放在\"/home/spider/Downloads\"目录下，则像上述代码一样填写即可。\n\nsplash还有另外一种命令行人工启动的方法，需要敲入的命令是：\n```sh\n[root@slave1 ~]# docker run -p 8053:8050 –v /home/spider/Downloads:/etc/splash/filters scrapinghub/splash\n```\n这里的-p后跟的8053代表主机提供scrapy的请求端口，8050仍然是docker容器内部定义的固定端口；-v仍然是对volumes的配置，冒号前为本机实际的文件目录，冒号后是容器内部的文件系统，不用更改；最后添加的scrapinghub/splash是容器要启动的映像名称。\n\n## splash的使用\n* 在scrapy的setting.py文件中添加：\n```python\nSPLASH_URL = 'http://192.168.0.1:8053'\n```\n* 在scrapy的setting.py文件中修改下载中间件的设置：\n```python\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\n```\n* 在scrapy的setting.py文件中添加去重中间件：\n```python\nSPIDER_MIDDLEWARES = {\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n}\n```\n* 设置定制的去重类：\n```python\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter' \n```\n* 设置URL使用splash渲染服务：\n```python\nyield scrapy.Request(url, self.parse_result, meta={\n    \t\t'splash': {\n        \t\t'args': {\n            \t\t# set rendering arguments here\n            \t\t'html': 1,\n        \t\t},\n        \t'endpoint': 'render.json',  # optional; default is render.json\n        \t'splash_url': '<url>',      # optional; overrides SPLASH_URL\n    \t\t}\n})\n```\n\n## splash请求过滤文件\n\n```sh\n! customize\n.css|\n.jpg|\n.png|\n.gif|\n\t\n! Fork of: ChinaList https://github.com/chinalist/chinalist\n/adflash/*\n/attachments/ad/*\n/tan1.js\n@@||poster.weather.com.cn/p_files/base/$image\n@@||union.bokecc.com/crossdomain.xml\nhk.search.auctions.yahoo.com###yauadysmh\nhk.yahoo.com###mntl1\n||114lm.com^$third-party\n||116b.com^$third-party\n……\n```\n上述代码列举了CSS文件，图片文件和部分广告链接等。将此文件改名为default.txt即可。\n\n\n## nginx负载均衡的配置和使用\n\n* 安装nginx，只需要命令行使用yum install即可：\n```sh\n[root@slave1 ~]# yum install nginx\n```\n* 运行前修改配置文件/etc/nginx/nginx.conf，在http处添加：\n```sh\nhttp { \n  \t  upstream splash{\n        \t server 223.3.95.199:8051;\n        \t server 223.3.95.199:8052;\n        \t server 223.3.95.199:8053;\n       \t  }\n}\n```\n* 再修改配置文件/etc/nginx/conf.d/default.conf：\n```sh\nserver {\n \t   listen         8050;\n \t   server_name    mySplash;\n\t\n    \tlocation / {\n        \tproxy_pass http://splash;\n   \t }\n}\n```\n* 开启nginx进程：\n```sh\n[root@slave1 spider]# /usr/sbin/nginx\n```\n* 重启或关闭进程：\n```sh\n[root@slave1 spider]# /usr/sbin/nginx –s reload\n[root@slave1 spider]# /usr/sbin/nginx –s stop\n```\n","slug":"dynamic-crawling","published":1,"updated":"2018-05-16T14:13:09.228Z","layout":"post","photos":[],"link":"","_id":"cjh96tx2h0002hoc5ob6ur4g8","content":"<p>基于scrapy的动态网页采集一直是个难点，而且如果想达到工程级别的抓取，需要有个高效率的解决方案。我列出了几个曾经尝试过的方案和它们的特点。</p>\n<h1 id=\"基于PyV8等脚本解析引擎\"><a href=\"#基于PyV8等脚本解析引擎\" class=\"headerlink\" title=\"基于PyV8等脚本解析引擎\"></a>基于PyV8等脚本解析引擎</h1><p>这类方案的原理是利用开源浏览器项目中的脚本解释引擎，实现相关脚本片段的解析，从而获得动态页面主体内容与超连接网络地址。自行构建脚本环境的方法以JavaScript脚本解析引擎为基础，构造解析动态页面脚本程序片段的环境。由于JavaScript脚本解析引擎只对JavaScript语言的基本对象做了支持，无法解析动态页面嵌入的JavaScript片段所使用的浏览器DOM对象，因而在利用这些脚本解析引擎解析动态页面脚本片段时，需要自己实现DOM对象，使脚本解析引擎支持浏览器DOM对象。<br><img src=\"/uploads/img/20170512/js.png\" alt=\"网页中Javascript片段示例\"><br>这种方法能够在非web环境下执行独立的Javascript语句，可以细粒度地控制网页中Javascript的执行。但由于实际抓取时缺少web环境，无法对如上图所示的DOM树等对象进行操作，因此这种方案很难适用。</p>\n<h1 id=\"自动化测试工具-浏览器\"><a href=\"#自动化测试工具-浏览器\" class=\"headerlink\" title=\"自动化测试工具 + 浏览器\"></a>自动化测试工具 + 浏览器</h1><p>这类方案使用开源浏览器渲染整个动态页面，从浏览器输出结果中提取页面主体内容与超链接网络地址。Selenium是由thought Works公司开发的web自动化测试工具。它直接在浏览器中运行，就像真正的用户在操作一样。Selenium框架底层使用Javascript模拟真实用户对浏览器进行操作。当执测试脚本时，浏览器自动按照做出相应的操作，就仿佛真实用户一样，因而可以从终端用户的角度测试Web应用。下图是在并发数为4，抓取20条URL情况下各浏览器性能对比：<br><img src=\"/uploads/img/20170512/browsers.png\" alt=\"并发数4，抓取20条URL各浏览器性能对比\"><br>这种方法能够完整得到浏览器渲染过的动态网页源码，但它需要调用外部程序，CPU和内存资源占用较高。由上图可以发现各个浏览器的用时、CPU占用和内存占用对实际应用来说是无法接受的；同时，该方案执行了很多和页面的文本内容和链接无关的脚本片段，针对性不够强，会使得整个系统的性能消耗过大，导致其效率低下，严重影响数据的采集速度。</p>\n<h1 id=\"基于QtWebkit的下载中间件\"><a href=\"#基于QtWebkit的下载中间件\" class=\"headerlink\" title=\"基于QtWebkit的下载中间件\"></a>基于QtWebkit的下载中间件</h1><p>WebKit 是一个开源的浏览器引擎，相比于其他浏览器引擎来说，Webkit的优势在于高效稳定，兼容性好，且源码结构清晰，易于维护。而QtWebkit是集成了Qt接口的Webkit实现，底层由C++开发，本方案中使用的是该内核的Python封装PyQt。在具体实现中，我们单独构建一个独立的下载中间件，在process_response函数中对网页源码进行动态渲染，并将渲染后得到的HTML代码替换源码。</p>\n<p>它能够利用其中的脚本解析引擎执行页面中嵌入的JavaScript脚本程序片段；无需调用外部程序，系统资源占用较小。但这种方案很难适用，因为QtWebkit的调用方法与下载中间件的结构设计无法统一，导致爬虫退出时QtWebkit异常崩溃；下载中间件渲染网页时，其他请求会被阻塞，渲染效率较低。</p>\n<h1 id=\"基于ghost-py的下载中间件\"><a href=\"#基于ghost-py的下载中间件\" class=\"headerlink\" title=\"基于ghost.py的下载中间件\"></a>基于ghost.py的下载中间件</h1><p>ghost.py是对QtWebkit进一步封装，封装后的调用方法与下载中间件的结构能够较好地结合，解决了爬虫退出时的崩溃问题；同时，ghost.py能够创建DOM对象，构造解析动态页面脚本程序片段的环境，支持单条Javascript语句的执行；在具体实现中，我们仍然构建一个独立的下载中间件，处理返回结果时进行动态渲染，并将渲染后得到的HTML代码替换源码。</p>\n<p>这种方法可以细粒度地控制渲染细节，比如禁用图片设置、资源加载超时设置等。但是，下载中间件是处于核心引擎和下载器之间，爬虫运行时只有一个示例对象，这样的系统定位导致网页渲染仍然是阻塞式的，执行效率较低。</p>\n<h1 id=\"基于ghost-py的下载器\"><a href=\"#基于ghost-py的下载器\" class=\"headerlink\" title=\"基于ghost.py的下载器\"></a>基于ghost.py的下载器</h1><p>改造scrapy的下载器，在下载器中使用ghost.py完成网页源码解压和动态网页渲染的步骤，这种方案的目的是尝试在下载器多线程并发下载的同时能够并发渲染，以期能够提高下载器的抓取效率。关于下载中间件渲染和下载器渲染的系统结构区别如下图<br><img src=\"/uploads/img/20170512/middle&amp;downloader.png\" alt=\"下载中间件与下载器示意图\"><br>我们的测试结果表明这种方案下爬虫的CPU使用率较低，下载器的线程无法利用多核执行；网络I/O和动态解析等待耗时长，数据抓取效率难以提升。这是因为：Python语言具有全局解释器锁的特性，它的作用在于让同一时刻只能有一个线程对于Python对象进行操作，这使得逻辑上并发的线程物理上仍然是顺序执行的；网页的渲染缺少异步处理的能力，单个线程会被挂起等待，直到渲染结果返回才会唤醒继续执行，这也导致数据抓取效率的降低。</p>\n<p>在我们使用ghost.py的过程，我们遇到一个非常棘手的内存段错误的bug，爬虫每次在爬取到几百条URL之后会出现core dump异常并自动结束程序。我们使用dmesg命令检测内核环缓冲，了解系统的启动信息，得到如下的错误信息提示：<br><img src=\"/uploads/img/20170512/bug.png\" alt=\"内存段错误提示\"></p>\n<p>我们计算得到libQtWebkit.so.4.8.1中出错函数的相对地址： A1E7F9。然后利用gdb调试反汇编代码，定位到运行出错的机器码如下图：<br><img src=\"/uploads/img/20170512/error.png\" alt=\"定位至出错的机器码\"><br>经过初步分析，我们认为出错发生在程序访问eax地址时，内存读操作越界。该错误出现在QtWebkit中CSS色彩梯度渐变渲染函数内，这应该是PyQt 4.8.1版本中的一个小bug，而修改源代码涉及的工作较为复杂且工程浩大，故也因此舍弃了该方案。</p>\n<h1 id=\"基于splash渲染服务\"><a href=\"#基于splash渲染服务\" class=\"headerlink\" title=\"基于splash渲染服务\"></a>基于<a href=\"https://github.com/scrapinghub/splash\" target=\"_blank\" rel=\"external\">splash</a>渲染服务</h1><p>通过对前面几种技术的分析和测试，我们发现这些方案都存在服务器资源利用效率不高、运行效率低和不稳定性等问题，于是我们考虑引入一种基于splash服务的动态解析方案。splash是基于Twisted异步处理框架和Qt开发框架实现的、对外提供HTTP API的一种网页渲染服务，其中Twisted使渲染服务具有异步处理能力，可以发挥Webkit的并发能力。结合了splash的scrapy系统架构和工作方式如下图：<br><img src=\"/uploads/img/20170512/splash.png\" alt=\"scrapy+splash示意图\"></p>\n<p>splash的动态渲染服务是我们最终采用的方案，但它在应用时有如下问题：splash的启停和参数设置无法由scrapy控制；splash运行时有极小概率崩溃；splash的endpoint参数设置对抓取效率和内存占用影响较大。</p>\n<p>虽然splash内部采用了Twisted异步处理框架，但这种方法只能节省线程等待网络IO和CPU渲染的时间，splash仍然会受困于Python全局解释器锁的限制，无法有效地利用多个CPU内核进行处理。单个splash即使满负荷运行也只能占用一个内核。而且原有方案是一个scrapy进程关联一个splash服务，splash的启停由scrapy单独控制，不与别的scrapy进程共享。这种即时开关splash服务的方法本省会消耗系统宝贵的计算资源，也会给整个爬虫系统的运行带来不稳定性。同时，不同splash服务之间的负载不均衡，也会给系统整体的抓取性能带来损害。</p>\n<p>所以，我们利用nginx反向代理服务来实现splash服务的负载均衡。nginx是一款轻量级的、高性能的Web 服务器/反向代理服务器，这里我们将它用作反向代理服务器来实现负载均衡。反向代理方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。</p>\n<p>nginx可以很容易通过模块扩展，所以nginx所拥有的功能集合也是很大的。nginx中的upstream负载均衡支持下面几种方式：轮询（默认）：按照时间顺序对所有服务器一个一个的访问，如果有服务器宕机，会自动剔除；权重：服务器的访问概率和权重成正比，这个可以在服务器配置不均的时候进行配置；IP哈希：对每个请求的IP进行哈希计算，并按照一定的规则分配对应的服务器；公平：按照每台服务器的响应时间来分配请求，响应时间小的优先分配；URL哈希：按照访问URL的哈希值来分配请求。</p>\n<p>下面的测试中，我们使用单个scrapy进程进行抓取，一共抓取991条URL，通过nginx反向代理请求不同数量的splash渲染服务，分别在scrapy请求并发数为50、100和150三种情况下进行对比。其中nginx在实现负载均衡时采用了默认的轮训方式。测试结果如下图：<br><img src=\"/uploads/img/20170512/splashs.png\" alt=\"不同splash数量的用时对比\"><br>从图中可以发现，在相同的scrapy请求并发数情况下，利用更多的splash渲染服务能够明显地获得更高的抓取效率；但如果在拥有同样数量的splash服务时，单纯地去提高scrapy请求并发数，得到的抓取效率提升就显得相对有限，这很可能是因为整体的性能瓶颈此时已经不再是渲染需要的CPU利用率了，而是变成scrapy自身处理效率及整体网络通信的瓶颈。</p>\n<h2 id=\"splash的启动\"><a href=\"#splash的启动\" class=\"headerlink\" title=\"splash的启动\"></a>splash的启动</h2><p>splash既可以在程序里使用依赖docker.py进行启动，或者在命令行中人工启动。通过程序启动的示例如下：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">con = client.create_container(image=<span class=\"string\">\"scrapinghub/splash\"</span>, volumes=[<span class=\"string\">\"/etc/splash/filters\"</span>], ports=[8050], host_config=client.create_host_config(port_bindings=&#123;8050:port_use&#125;, binds=[<span class=\"string\">\"/home/spider/Downloads:/etc/splash/filters\"</span>]))</span><br></pre></td></tr></table></figure></p>\n<p>其中参数image是该容器启动的映像名字；volume代表容器内部使用的挂载，其中的地址与外部文件无关，是定义在容器内一块文件；ports代表该容器会监听自己内部的8050端口，这是splash内部固定设置，不建议更改；host_config是建立主机和容器的一种绑定关系的设置，绑定包括两部分内容，一个是内部的8050端口和外部port_use端口的绑定，如果以后确定scrapy请求的端口号设置为8060，则将port_use设置为8060，另外一个是外部文件和容器内部挂载的绑定，冒号后面的目录是之前volumes的值，冒号之前的目录则填写你自定义过滤文件的目录，默认取名为default.txt，例如在后面会提到的请求过滤规则文件default.txt我们放在”/home/spider/Downloads”目录下，则像上述代码一样填写即可。</p>\n<p>splash还有另外一种命令行人工启动的方法，需要敲入的命令是：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 ~]<span class=\"comment\"># docker run -p 8053:8050 –v /home/spider/Downloads:/etc/splash/filters scrapinghub/splash</span></span><br></pre></td></tr></table></figure></p>\n<p>这里的-p后跟的8053代表主机提供scrapy的请求端口，8050仍然是docker容器内部定义的固定端口；-v仍然是对volumes的配置，冒号前为本机实际的文件目录，冒号后是容器内部的文件系统，不用更改；最后添加的scrapinghub/splash是容器要启动的映像名称。</p>\n<h2 id=\"splash的使用\"><a href=\"#splash的使用\" class=\"headerlink\" title=\"splash的使用\"></a>splash的使用</h2><ul>\n<li><p>在scrapy的setting.py文件中添加：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPLASH_URL = <span class=\"string\">'http://192.168.0.1:8053'</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在scrapy的setting.py文件中修改下载中间件的设置：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class=\"line\">    <span class=\"string\">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class=\"number\">723</span>,</span><br><span class=\"line\">    <span class=\"string\">'scrapy_splash.SplashMiddleware'</span>: <span class=\"number\">725</span>,</span><br><span class=\"line\"><span class=\"string\">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class=\"number\">810</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在scrapy的setting.py文件中添加去重中间件：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPIDER_MIDDLEWARES = &#123;</span><br><span class=\"line\">    <span class=\"string\">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class=\"number\">100</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>设置定制的去重类：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DUPEFILTER_CLASS = <span class=\"string\">'scrapy_splash.SplashAwareDupeFilter'</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>设置URL使用splash渲染服务：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> scrapy.Request(url, self.parse_result, meta=&#123;</span><br><span class=\"line\">    \t\t<span class=\"string\">'splash'</span>: &#123;</span><br><span class=\"line\">        \t\t<span class=\"string\">'args'</span>: &#123;</span><br><span class=\"line\">            \t\t<span class=\"comment\"># set rendering arguments here</span></span><br><span class=\"line\">            \t\t<span class=\"string\">'html'</span>: <span class=\"number\">1</span>,</span><br><span class=\"line\">        \t\t&#125;,</span><br><span class=\"line\">        \t<span class=\"string\">'endpoint'</span>: <span class=\"string\">'render.json'</span>,  <span class=\"comment\"># optional; default is render.json</span></span><br><span class=\"line\">        \t<span class=\"string\">'splash_url'</span>: <span class=\"string\">'&lt;url&gt;'</span>,      <span class=\"comment\"># optional; overrides SPLASH_URL</span></span><br><span class=\"line\">    \t\t&#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"splash请求过滤文件\"><a href=\"#splash请求过滤文件\" class=\"headerlink\" title=\"splash请求过滤文件\"></a>splash请求过滤文件</h2><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">! customize</span><br><span class=\"line\">.css|</span><br><span class=\"line\">.jpg|</span><br><span class=\"line\">.png|</span><br><span class=\"line\">.gif|</span><br><span class=\"line\">\t</span><br><span class=\"line\">! Fork of: ChinaList https://github.com/chinalist/chinalist</span><br><span class=\"line\">/adflash/*</span><br><span class=\"line\">/attachments/ad/*</span><br><span class=\"line\">/tan1.js</span><br><span class=\"line\">@@||poster.weather.com.cn/p_files/base/<span class=\"variable\">$image</span></span><br><span class=\"line\">@@||union.bokecc.com/crossdomain.xml</span><br><span class=\"line\">hk.search.auctions.yahoo.com<span class=\"comment\">###yauadysmh</span></span><br><span class=\"line\">hk.yahoo.com<span class=\"comment\">###mntl1</span></span><br><span class=\"line\">||114lm.com^<span class=\"variable\">$third</span>-party</span><br><span class=\"line\">||116b.com^<span class=\"variable\">$third</span>-party</span><br><span class=\"line\">……</span><br></pre></td></tr></table></figure>\n<p>上述代码列举了CSS文件，图片文件和部分广告链接等。将此文件改名为default.txt即可。</p>\n<h2 id=\"nginx负载均衡的配置和使用\"><a href=\"#nginx负载均衡的配置和使用\" class=\"headerlink\" title=\"nginx负载均衡的配置和使用\"></a>nginx负载均衡的配置和使用</h2><ul>\n<li><p>安装nginx，只需要命令行使用yum install即可：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 ~]<span class=\"comment\"># yum install nginx</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>运行前修改配置文件/etc/nginx/nginx.conf，在http处添加：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http &#123; </span><br><span class=\"line\">  \t  upstream splash&#123;</span><br><span class=\"line\">        \t server 223.3.95.199:8051;</span><br><span class=\"line\">        \t server 223.3.95.199:8052;</span><br><span class=\"line\">        \t server 223.3.95.199:8053;</span><br><span class=\"line\">       \t  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>再修改配置文件/etc/nginx/conf.d/default.conf：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server &#123;</span><br><span class=\"line\"> \t   listen         8050;</span><br><span class=\"line\"> \t   server_name    mySplash;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    \tlocation / &#123;</span><br><span class=\"line\">        \tproxy_pass http://splash;</span><br><span class=\"line\">   \t &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>开启nginx进程：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 spider]<span class=\"comment\"># /usr/sbin/nginx</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>重启或关闭进程：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 spider]<span class=\"comment\"># /usr/sbin/nginx –s reload</span></span><br><span class=\"line\">[root@slave1 spider]<span class=\"comment\"># /usr/sbin/nginx –s stop</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n","excerpt":"","more":"<p>基于scrapy的动态网页采集一直是个难点，而且如果想达到工程级别的抓取，需要有个高效率的解决方案。我列出了几个曾经尝试过的方案和它们的特点。</p>\n<h1 id=\"基于PyV8等脚本解析引擎\"><a href=\"#基于PyV8等脚本解析引擎\" class=\"headerlink\" title=\"基于PyV8等脚本解析引擎\"></a>基于PyV8等脚本解析引擎</h1><p>这类方案的原理是利用开源浏览器项目中的脚本解释引擎，实现相关脚本片段的解析，从而获得动态页面主体内容与超连接网络地址。自行构建脚本环境的方法以JavaScript脚本解析引擎为基础，构造解析动态页面脚本程序片段的环境。由于JavaScript脚本解析引擎只对JavaScript语言的基本对象做了支持，无法解析动态页面嵌入的JavaScript片段所使用的浏览器DOM对象，因而在利用这些脚本解析引擎解析动态页面脚本片段时，需要自己实现DOM对象，使脚本解析引擎支持浏览器DOM对象。<br><img src=\"/uploads/img/20170512/js.png\" alt=\"网页中Javascript片段示例\"><br>这种方法能够在非web环境下执行独立的Javascript语句，可以细粒度地控制网页中Javascript的执行。但由于实际抓取时缺少web环境，无法对如上图所示的DOM树等对象进行操作，因此这种方案很难适用。</p>\n<h1 id=\"自动化测试工具-浏览器\"><a href=\"#自动化测试工具-浏览器\" class=\"headerlink\" title=\"自动化测试工具 + 浏览器\"></a>自动化测试工具 + 浏览器</h1><p>这类方案使用开源浏览器渲染整个动态页面，从浏览器输出结果中提取页面主体内容与超链接网络地址。Selenium是由thought Works公司开发的web自动化测试工具。它直接在浏览器中运行，就像真正的用户在操作一样。Selenium框架底层使用Javascript模拟真实用户对浏览器进行操作。当执测试脚本时，浏览器自动按照做出相应的操作，就仿佛真实用户一样，因而可以从终端用户的角度测试Web应用。下图是在并发数为4，抓取20条URL情况下各浏览器性能对比：<br><img src=\"/uploads/img/20170512/browsers.png\" alt=\"并发数4，抓取20条URL各浏览器性能对比\"><br>这种方法能够完整得到浏览器渲染过的动态网页源码，但它需要调用外部程序，CPU和内存资源占用较高。由上图可以发现各个浏览器的用时、CPU占用和内存占用对实际应用来说是无法接受的；同时，该方案执行了很多和页面的文本内容和链接无关的脚本片段，针对性不够强，会使得整个系统的性能消耗过大，导致其效率低下，严重影响数据的采集速度。</p>\n<h1 id=\"基于QtWebkit的下载中间件\"><a href=\"#基于QtWebkit的下载中间件\" class=\"headerlink\" title=\"基于QtWebkit的下载中间件\"></a>基于QtWebkit的下载中间件</h1><p>WebKit 是一个开源的浏览器引擎，相比于其他浏览器引擎来说，Webkit的优势在于高效稳定，兼容性好，且源码结构清晰，易于维护。而QtWebkit是集成了Qt接口的Webkit实现，底层由C++开发，本方案中使用的是该内核的Python封装PyQt。在具体实现中，我们单独构建一个独立的下载中间件，在process_response函数中对网页源码进行动态渲染，并将渲染后得到的HTML代码替换源码。</p>\n<p>它能够利用其中的脚本解析引擎执行页面中嵌入的JavaScript脚本程序片段；无需调用外部程序，系统资源占用较小。但这种方案很难适用，因为QtWebkit的调用方法与下载中间件的结构设计无法统一，导致爬虫退出时QtWebkit异常崩溃；下载中间件渲染网页时，其他请求会被阻塞，渲染效率较低。</p>\n<h1 id=\"基于ghost-py的下载中间件\"><a href=\"#基于ghost-py的下载中间件\" class=\"headerlink\" title=\"基于ghost.py的下载中间件\"></a>基于ghost.py的下载中间件</h1><p>ghost.py是对QtWebkit进一步封装，封装后的调用方法与下载中间件的结构能够较好地结合，解决了爬虫退出时的崩溃问题；同时，ghost.py能够创建DOM对象，构造解析动态页面脚本程序片段的环境，支持单条Javascript语句的执行；在具体实现中，我们仍然构建一个独立的下载中间件，处理返回结果时进行动态渲染，并将渲染后得到的HTML代码替换源码。</p>\n<p>这种方法可以细粒度地控制渲染细节，比如禁用图片设置、资源加载超时设置等。但是，下载中间件是处于核心引擎和下载器之间，爬虫运行时只有一个示例对象，这样的系统定位导致网页渲染仍然是阻塞式的，执行效率较低。</p>\n<h1 id=\"基于ghost-py的下载器\"><a href=\"#基于ghost-py的下载器\" class=\"headerlink\" title=\"基于ghost.py的下载器\"></a>基于ghost.py的下载器</h1><p>改造scrapy的下载器，在下载器中使用ghost.py完成网页源码解压和动态网页渲染的步骤，这种方案的目的是尝试在下载器多线程并发下载的同时能够并发渲染，以期能够提高下载器的抓取效率。关于下载中间件渲染和下载器渲染的系统结构区别如下图<br><img src=\"/uploads/img/20170512/middle&amp;downloader.png\" alt=\"下载中间件与下载器示意图\"><br>我们的测试结果表明这种方案下爬虫的CPU使用率较低，下载器的线程无法利用多核执行；网络I/O和动态解析等待耗时长，数据抓取效率难以提升。这是因为：Python语言具有全局解释器锁的特性，它的作用在于让同一时刻只能有一个线程对于Python对象进行操作，这使得逻辑上并发的线程物理上仍然是顺序执行的；网页的渲染缺少异步处理的能力，单个线程会被挂起等待，直到渲染结果返回才会唤醒继续执行，这也导致数据抓取效率的降低。</p>\n<p>在我们使用ghost.py的过程，我们遇到一个非常棘手的内存段错误的bug，爬虫每次在爬取到几百条URL之后会出现core dump异常并自动结束程序。我们使用dmesg命令检测内核环缓冲，了解系统的启动信息，得到如下的错误信息提示：<br><img src=\"/uploads/img/20170512/bug.png\" alt=\"内存段错误提示\"></p>\n<p>我们计算得到libQtWebkit.so.4.8.1中出错函数的相对地址： A1E7F9。然后利用gdb调试反汇编代码，定位到运行出错的机器码如下图：<br><img src=\"/uploads/img/20170512/error.png\" alt=\"定位至出错的机器码\"><br>经过初步分析，我们认为出错发生在程序访问eax地址时，内存读操作越界。该错误出现在QtWebkit中CSS色彩梯度渐变渲染函数内，这应该是PyQt 4.8.1版本中的一个小bug，而修改源代码涉及的工作较为复杂且工程浩大，故也因此舍弃了该方案。</p>\n<h1 id=\"基于splash渲染服务\"><a href=\"#基于splash渲染服务\" class=\"headerlink\" title=\"基于splash渲染服务\"></a>基于<a href=\"https://github.com/scrapinghub/splash\">splash</a>渲染服务</h1><p>通过对前面几种技术的分析和测试，我们发现这些方案都存在服务器资源利用效率不高、运行效率低和不稳定性等问题，于是我们考虑引入一种基于splash服务的动态解析方案。splash是基于Twisted异步处理框架和Qt开发框架实现的、对外提供HTTP API的一种网页渲染服务，其中Twisted使渲染服务具有异步处理能力，可以发挥Webkit的并发能力。结合了splash的scrapy系统架构和工作方式如下图：<br><img src=\"/uploads/img/20170512/splash.png\" alt=\"scrapy+splash示意图\"></p>\n<p>splash的动态渲染服务是我们最终采用的方案，但它在应用时有如下问题：splash的启停和参数设置无法由scrapy控制；splash运行时有极小概率崩溃；splash的endpoint参数设置对抓取效率和内存占用影响较大。</p>\n<p>虽然splash内部采用了Twisted异步处理框架，但这种方法只能节省线程等待网络IO和CPU渲染的时间，splash仍然会受困于Python全局解释器锁的限制，无法有效地利用多个CPU内核进行处理。单个splash即使满负荷运行也只能占用一个内核。而且原有方案是一个scrapy进程关联一个splash服务，splash的启停由scrapy单独控制，不与别的scrapy进程共享。这种即时开关splash服务的方法本省会消耗系统宝贵的计算资源，也会给整个爬虫系统的运行带来不稳定性。同时，不同splash服务之间的负载不均衡，也会给系统整体的抓取性能带来损害。</p>\n<p>所以，我们利用nginx反向代理服务来实现splash服务的负载均衡。nginx是一款轻量级的、高性能的Web 服务器/反向代理服务器，这里我们将它用作反向代理服务器来实现负载均衡。反向代理方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。</p>\n<p>nginx可以很容易通过模块扩展，所以nginx所拥有的功能集合也是很大的。nginx中的upstream负载均衡支持下面几种方式：轮询（默认）：按照时间顺序对所有服务器一个一个的访问，如果有服务器宕机，会自动剔除；权重：服务器的访问概率和权重成正比，这个可以在服务器配置不均的时候进行配置；IP哈希：对每个请求的IP进行哈希计算，并按照一定的规则分配对应的服务器；公平：按照每台服务器的响应时间来分配请求，响应时间小的优先分配；URL哈希：按照访问URL的哈希值来分配请求。</p>\n<p>下面的测试中，我们使用单个scrapy进程进行抓取，一共抓取991条URL，通过nginx反向代理请求不同数量的splash渲染服务，分别在scrapy请求并发数为50、100和150三种情况下进行对比。其中nginx在实现负载均衡时采用了默认的轮训方式。测试结果如下图：<br><img src=\"/uploads/img/20170512/splashs.png\" alt=\"不同splash数量的用时对比\"><br>从图中可以发现，在相同的scrapy请求并发数情况下，利用更多的splash渲染服务能够明显地获得更高的抓取效率；但如果在拥有同样数量的splash服务时，单纯地去提高scrapy请求并发数，得到的抓取效率提升就显得相对有限，这很可能是因为整体的性能瓶颈此时已经不再是渲染需要的CPU利用率了，而是变成scrapy自身处理效率及整体网络通信的瓶颈。</p>\n<h2 id=\"splash的启动\"><a href=\"#splash的启动\" class=\"headerlink\" title=\"splash的启动\"></a>splash的启动</h2><p>splash既可以在程序里使用依赖docker.py进行启动，或者在命令行中人工启动。通过程序启动的示例如下：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">con = client.create_container(image=<span class=\"string\">\"scrapinghub/splash\"</span>, volumes=[<span class=\"string\">\"/etc/splash/filters\"</span>], ports=[8050], host_config=client.create_host_config(port_bindings=&#123;8050:port_use&#125;, binds=[<span class=\"string\">\"/home/spider/Downloads:/etc/splash/filters\"</span>]))</span><br></pre></td></tr></table></figure></p>\n<p>其中参数image是该容器启动的映像名字；volume代表容器内部使用的挂载，其中的地址与外部文件无关，是定义在容器内一块文件；ports代表该容器会监听自己内部的8050端口，这是splash内部固定设置，不建议更改；host_config是建立主机和容器的一种绑定关系的设置，绑定包括两部分内容，一个是内部的8050端口和外部port_use端口的绑定，如果以后确定scrapy请求的端口号设置为8060，则将port_use设置为8060，另外一个是外部文件和容器内部挂载的绑定，冒号后面的目录是之前volumes的值，冒号之前的目录则填写你自定义过滤文件的目录，默认取名为default.txt，例如在后面会提到的请求过滤规则文件default.txt我们放在”/home/spider/Downloads”目录下，则像上述代码一样填写即可。</p>\n<p>splash还有另外一种命令行人工启动的方法，需要敲入的命令是：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 ~]<span class=\"comment\"># docker run -p 8053:8050 –v /home/spider/Downloads:/etc/splash/filters scrapinghub/splash</span></span><br></pre></td></tr></table></figure></p>\n<p>这里的-p后跟的8053代表主机提供scrapy的请求端口，8050仍然是docker容器内部定义的固定端口；-v仍然是对volumes的配置，冒号前为本机实际的文件目录，冒号后是容器内部的文件系统，不用更改；最后添加的scrapinghub/splash是容器要启动的映像名称。</p>\n<h2 id=\"splash的使用\"><a href=\"#splash的使用\" class=\"headerlink\" title=\"splash的使用\"></a>splash的使用</h2><ul>\n<li><p>在scrapy的setting.py文件中添加：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPLASH_URL = <span class=\"string\">'http://192.168.0.1:8053'</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在scrapy的setting.py文件中修改下载中间件的设置：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class=\"line\">    <span class=\"string\">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class=\"number\">723</span>,</span><br><span class=\"line\">    <span class=\"string\">'scrapy_splash.SplashMiddleware'</span>: <span class=\"number\">725</span>,</span><br><span class=\"line\"><span class=\"string\">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class=\"number\">810</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在scrapy的setting.py文件中添加去重中间件：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPIDER_MIDDLEWARES = &#123;</span><br><span class=\"line\">    <span class=\"string\">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class=\"number\">100</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>设置定制的去重类：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DUPEFILTER_CLASS = <span class=\"string\">'scrapy_splash.SplashAwareDupeFilter'</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>设置URL使用splash渲染服务：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> scrapy.Request(url, self.parse_result, meta=&#123;</span><br><span class=\"line\">    \t\t<span class=\"string\">'splash'</span>: &#123;</span><br><span class=\"line\">        \t\t<span class=\"string\">'args'</span>: &#123;</span><br><span class=\"line\">            \t\t<span class=\"comment\"># set rendering arguments here</span></span><br><span class=\"line\">            \t\t<span class=\"string\">'html'</span>: <span class=\"number\">1</span>,</span><br><span class=\"line\">        \t\t&#125;,</span><br><span class=\"line\">        \t<span class=\"string\">'endpoint'</span>: <span class=\"string\">'render.json'</span>,  <span class=\"comment\"># optional; default is render.json</span></span><br><span class=\"line\">        \t<span class=\"string\">'splash_url'</span>: <span class=\"string\">'&lt;url&gt;'</span>,      <span class=\"comment\"># optional; overrides SPLASH_URL</span></span><br><span class=\"line\">    \t\t&#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"splash请求过滤文件\"><a href=\"#splash请求过滤文件\" class=\"headerlink\" title=\"splash请求过滤文件\"></a>splash请求过滤文件</h2><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">! customize</span><br><span class=\"line\">.css|</span><br><span class=\"line\">.jpg|</span><br><span class=\"line\">.png|</span><br><span class=\"line\">.gif|</span><br><span class=\"line\">\t</span><br><span class=\"line\">! Fork of: ChinaList https://github.com/chinalist/chinalist</span><br><span class=\"line\">/adflash/*</span><br><span class=\"line\">/attachments/ad/*</span><br><span class=\"line\">/tan1.js</span><br><span class=\"line\">@@||poster.weather.com.cn/p_files/base/<span class=\"variable\">$image</span></span><br><span class=\"line\">@@||union.bokecc.com/crossdomain.xml</span><br><span class=\"line\">hk.search.auctions.yahoo.com<span class=\"comment\">###yauadysmh</span></span><br><span class=\"line\">hk.yahoo.com<span class=\"comment\">###mntl1</span></span><br><span class=\"line\">||114lm.com^<span class=\"variable\">$third</span>-party</span><br><span class=\"line\">||116b.com^<span class=\"variable\">$third</span>-party</span><br><span class=\"line\">……</span><br></pre></td></tr></table></figure>\n<p>上述代码列举了CSS文件，图片文件和部分广告链接等。将此文件改名为default.txt即可。</p>\n<h2 id=\"nginx负载均衡的配置和使用\"><a href=\"#nginx负载均衡的配置和使用\" class=\"headerlink\" title=\"nginx负载均衡的配置和使用\"></a>nginx负载均衡的配置和使用</h2><ul>\n<li><p>安装nginx，只需要命令行使用yum install即可：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 ~]<span class=\"comment\"># yum install nginx</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>运行前修改配置文件/etc/nginx/nginx.conf，在http处添加：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http &#123; </span><br><span class=\"line\">  \t  upstream splash&#123;</span><br><span class=\"line\">        \t server 223.3.95.199:8051;</span><br><span class=\"line\">        \t server 223.3.95.199:8052;</span><br><span class=\"line\">        \t server 223.3.95.199:8053;</span><br><span class=\"line\">       \t  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>再修改配置文件/etc/nginx/conf.d/default.conf：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server &#123;</span><br><span class=\"line\"> \t   listen         8050;</span><br><span class=\"line\"> \t   server_name    mySplash;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    \tlocation / &#123;</span><br><span class=\"line\">        \tproxy_pass http://splash;</span><br><span class=\"line\">   \t &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>开启nginx进程：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 spider]<span class=\"comment\"># /usr/sbin/nginx</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>重启或关闭进程：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@slave1 spider]<span class=\"comment\"># /usr/sbin/nginx –s reload</span></span><br><span class=\"line\">[root@slave1 spider]<span class=\"comment\"># /usr/sbin/nginx –s stop</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n"},{"title":"Hello World","date":"2016-06-12T13:28:52.000Z","comments":1,"photos":["/uploads/img/20160612/cover.gif"],"_content":"\nThis is my **first blog** after I established this site using [Hexo](https://hexo.io/) and [random](https://github.com/stiekel/hexo-theme-random). \nHere are some tests for Hexo:\n\n# Code Test:\n```javascript\n$ hexo clean\n$ hexo generate\n$ hexo server\n$ hexo deploy\n```\n# Mathjax Test:\n$$\\sum_{i=1}^n a_i=0$$\n\n$$\n\\begin{eqnarray}\nf(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2\n\\end{eqnarray}\n$$ \n\n$$\n\\begin{eqnarray}\n\\nabla\\cdot\\vec{E} &=& \\frac{\\rho}{\\epsilon_0} \\\\\n\\nabla\\cdot\\vec{B} &=& 0 \\\\\n\\nabla\\times\\vec{E} &=& -\\frac{\\partial B}{\\partial t} \\\\\n\\nabla\\times\\vec{B} &=& \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)\n\\end{eqnarray}\n$$\n\n# PDF Test:\n{% pdf http://tripleday.cn/uploads/pdf/Clean%20Code.pdf %}\n\n# iFrame Test:\n{% iframe http://www.seu.edu.cn/english/main.htm 100% 500 %}\n\n# Picture Test:\n![Facebook](/uploads/img/20160612/facebook.jpg)\n\n# Youtube Test:\n{% youtube https://youtu.be/QBJxGklvHRg %}\n\n# Youku Test:\n{% youku 480 %}\nXMTU3NjExOTUwMA==\n{% endyouku %}\n\n","source":"_posts/hello-world.md","raw":"title: Hello World\ndate: 2016-06-12 21:28:52\ncomments: true\ntags: \n - English\n - Hexo\ncategories: Site Est\nphotos: \n - /uploads/img/20160612/cover.gif\n---\n\nThis is my **first blog** after I established this site using [Hexo](https://hexo.io/) and [random](https://github.com/stiekel/hexo-theme-random). \nHere are some tests for Hexo:\n\n# Code Test:\n```javascript\n$ hexo clean\n$ hexo generate\n$ hexo server\n$ hexo deploy\n```\n# Mathjax Test:\n$$\\sum_{i=1}^n a_i=0$$\n\n$$\n\\begin{eqnarray}\nf(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2\n\\end{eqnarray}\n$$ \n\n$$\n\\begin{eqnarray}\n\\nabla\\cdot\\vec{E} &=& \\frac{\\rho}{\\epsilon_0} \\\\\n\\nabla\\cdot\\vec{B} &=& 0 \\\\\n\\nabla\\times\\vec{E} &=& -\\frac{\\partial B}{\\partial t} \\\\\n\\nabla\\times\\vec{B} &=& \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)\n\\end{eqnarray}\n$$\n\n# PDF Test:\n{% pdf http://tripleday.cn/uploads/pdf/Clean%20Code.pdf %}\n\n# iFrame Test:\n{% iframe http://www.seu.edu.cn/english/main.htm 100% 500 %}\n\n# Picture Test:\n![Facebook](/uploads/img/20160612/facebook.jpg)\n\n# Youtube Test:\n{% youtube https://youtu.be/QBJxGklvHRg %}\n\n# Youku Test:\n{% youku 480 %}\nXMTU3NjExOTUwMA==\n{% endyouku %}\n\n","slug":"hello-world","published":1,"updated":"2017-08-11T11:30:15.502Z","layout":"post","link":"","_id":"cjh96tx2r0006hoc5t4f2sjmr","content":"<p>This is my <strong>first blog</strong> after I established this site using <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a> and <a href=\"https://github.com/stiekel/hexo-theme-random\" target=\"_blank\" rel=\"external\">random</a>.<br>Here are some tests for Hexo:</p>\n<h1 id=\"Code-Test\"><a href=\"#Code-Test\" class=\"headerlink\" title=\"Code Test:\"></a>Code Test:</h1><figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo server</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<h1 id=\"Mathjax-Test\"><a href=\"#Mathjax-Test\" class=\"headerlink\" title=\"Mathjax Test:\"></a>Mathjax Test:</h1><p>$$\\sum_{i=1}^n a_i=0$$</p>\n<p>$$<br>\\begin{eqnarray}<br>f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2<br>\\end{eqnarray}<br>$$ </p>\n<p>$$<br>\\begin{eqnarray}<br>\\nabla\\cdot\\vec{E} &amp;=&amp; \\frac{\\rho}{\\epsilon_0} \\\\<br>\\nabla\\cdot\\vec{B} &amp;=&amp; 0 \\\\<br>\\nabla\\times\\vec{E} &amp;=&amp; -\\frac{\\partial B}{\\partial t} \\\\<br>\\nabla\\times\\vec{B} &amp;=&amp; \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)<br>\\end{eqnarray}<br>$$</p>\n<h1 id=\"PDF-Test\"><a href=\"#PDF-Test\" class=\"headerlink\" title=\"PDF Test:\"></a>PDF Test:</h1>\n\n\t<div class=\"row\">\n    <embed src=\"http://tripleday.cn/uploads/pdf/Clean%20Code.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n<h1 id=\"iFrame-Test\"><a href=\"#iFrame-Test\" class=\"headerlink\" title=\"iFrame Test:\"></a>iFrame Test:</h1><iframe src=\"http://www.seu.edu.cn/english/main.htm\" width=\"100%\" height=\"500\" frameborder=\"0\" allowfullscreen></iframe>\n<h1 id=\"Picture-Test\"><a href=\"#Picture-Test\" class=\"headerlink\" title=\"Picture Test:\"></a>Picture Test:</h1><p><img src=\"/uploads/img/20160612/facebook.jpg\" alt=\"Facebook\"></p>\n<h1 id=\"Youtube-Test\"><a href=\"#Youtube-Test\" class=\"headerlink\" title=\"Youtube Test:\"></a>Youtube Test:</h1><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/https://youtu.be/QBJxGklvHRg\" frameborder=\"0\" allowfullscreen></iframe></div>\n<h1 id=\"Youku-Test\"><a href=\"#Youku-Test\" class=\"headerlink\" title=\"Youku Test:\"></a>Youku Test:</h1><div class=\"video-container\"><iframe height=\"480\" width=\"100%\" src=\"http://player.youku.com/embed/XMTU3NjExOTUwMA==\" frameborder=\"0\" allowfullscreen></iframe></div>\n","excerpt":"","more":"<p>This is my <strong>first blog</strong> after I established this site using <a href=\"https://hexo.io/\">Hexo</a> and <a href=\"https://github.com/stiekel/hexo-theme-random\">random</a>.<br>Here are some tests for Hexo:</p>\n<h1 id=\"Code-Test\"><a href=\"#Code-Test\" class=\"headerlink\" title=\"Code Test:\"></a>Code Test:</h1><figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo server</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<h1 id=\"Mathjax-Test\"><a href=\"#Mathjax-Test\" class=\"headerlink\" title=\"Mathjax Test:\"></a>Mathjax Test:</h1><p>$$\\sum_{i=1}^n a_i=0$$</p>\n<p>$$<br>\\begin{eqnarray}<br>f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2<br>\\end{eqnarray}<br>$$ </p>\n<p>$$<br>\\begin{eqnarray}<br>\\nabla\\cdot\\vec{E} &amp;=&amp; \\frac{\\rho}{\\epsilon_0} \\\\<br>\\nabla\\cdot\\vec{B} &amp;=&amp; 0 \\\\<br>\\nabla\\times\\vec{E} &amp;=&amp; -\\frac{\\partial B}{\\partial t} \\\\<br>\\nabla\\times\\vec{B} &amp;=&amp; \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)<br>\\end{eqnarray}<br>$$</p>\n<h1 id=\"PDF-Test\"><a href=\"#PDF-Test\" class=\"headerlink\" title=\"PDF Test:\"></a>PDF Test:</h1>\n\n\t<div class=\"row\">\n    <embed src=\"http://tripleday.cn/uploads/pdf/Clean%20Code.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n<h1 id=\"iFrame-Test\"><a href=\"#iFrame-Test\" class=\"headerlink\" title=\"iFrame Test:\"></a>iFrame Test:</h1><iframe src=\"http://www.seu.edu.cn/english/main.htm\" width=\"100%\" height=\"500\" frameborder=\"0\" allowfullscreen></iframe>\n<h1 id=\"Picture-Test\"><a href=\"#Picture-Test\" class=\"headerlink\" title=\"Picture Test:\"></a>Picture Test:</h1><p><img src=\"/uploads/img/20160612/facebook.jpg\" alt=\"Facebook\"></p>\n<h1 id=\"Youtube-Test\"><a href=\"#Youtube-Test\" class=\"headerlink\" title=\"Youtube Test:\"></a>Youtube Test:</h1><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/https://youtu.be/QBJxGklvHRg\" frameborder=\"0\" allowfullscreen></iframe></div>\n<h1 id=\"Youku-Test\"><a href=\"#Youku-Test\" class=\"headerlink\" title=\"Youku Test:\"></a>Youku Test:</h1><div class=\"video-container\"><iframe height=480 width=100% src=\"http://player.youku.com/embed/XMTU3NjExOTUwMA==\" frameborder=0 allowfullscreen></iframe></div>\n"},{"title":"NLP中使用HMM进行tag、seg和ner","date":"2016-07-28T13:54:17.000Z","comments":1,"photos":["/uploads/img/20160728/cover.png"],"_content":"看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称**tag**）、分词（Segmentation，以下简称**seg**）和命名实体识别（Named Entity Recognition，以下简称**ner**）。\n\n具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文[HMM、MEMM和CRF的学习总结](http://tripleday.github.io/2016/07/14/hmm-memm-crf/)和里面提供的一些链接。\n\n# 代码相关\n\n整个HMM的代码和相关测试数据已上传至Github上，附上[链接](https://github.com/tripleday/simple_HMM)。整个代码的实现有部分学习参考博客[python词法分析(分词+词性标注）](http://blog.csdn.net/soundfuture/article/details/4135216)，感谢博主的分享。\n\n* 文件图如下：\n![](/uploads/img/20160728/file.png)\n这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见[Chunking](http://www.cnts.ua.ac.be/conll2000/chunking/)，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。\n\n* 其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。\n\n\n# tag\n\n* 数据说明\ntag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。\n  * conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，**第二列**是由Brill tagger标注的词性，**第三列**是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。\n  * pku的数据是中文语料，且已经进行过分词操作，它的词性标注与[近代汉语词类标注简表](http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a)相似但不完全相同。\n\n* 测试结果\n关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的**Brill tagger**的准确率约为94.09%，对第三列的**WSJ corpus**的准确率约为87.93%。pku的准确率约为93.0%。\n\n* 结果分析\n  * 本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。\n  * 我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。\n\n# seg\n\n* 处理思路\nseg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇[中文分词入门之字标注法3](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953)。\n\n* 测试结果\n对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。\n\n# ner\n\n* 数据说明\nner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。\n\n* 测试结果\n对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。\n\n* 结果分析\n从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。\n\n\n","source":"_posts/hmm4nlp.md","raw":"title: NLP中使用HMM进行tag、seg和ner\ndate: 2016-07-28 21:54:17\ncomments: true\ntags: \n - HMM\n - python\ncategories: NLP\nphotos: \n - /uploads/img/20160728/cover.png\n---\n看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称**tag**）、分词（Segmentation，以下简称**seg**）和命名实体识别（Named Entity Recognition，以下简称**ner**）。\n\n具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文[HMM、MEMM和CRF的学习总结](http://tripleday.github.io/2016/07/14/hmm-memm-crf/)和里面提供的一些链接。\n\n# 代码相关\n\n整个HMM的代码和相关测试数据已上传至Github上，附上[链接](https://github.com/tripleday/simple_HMM)。整个代码的实现有部分学习参考博客[python词法分析(分词+词性标注）](http://blog.csdn.net/soundfuture/article/details/4135216)，感谢博主的分享。\n\n* 文件图如下：\n![](/uploads/img/20160728/file.png)\n这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见[Chunking](http://www.cnts.ua.ac.be/conll2000/chunking/)，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。\n\n* 其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。\n\n\n# tag\n\n* 数据说明\ntag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。\n  * conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，**第二列**是由Brill tagger标注的词性，**第三列**是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。\n  * pku的数据是中文语料，且已经进行过分词操作，它的词性标注与[近代汉语词类标注简表](http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a)相似但不完全相同。\n\n* 测试结果\n关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的**Brill tagger**的准确率约为94.09%，对第三列的**WSJ corpus**的准确率约为87.93%。pku的准确率约为93.0%。\n\n* 结果分析\n  * 本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。\n  * 我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。\n\n# seg\n\n* 处理思路\nseg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇[中文分词入门之字标注法3](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953)。\n\n* 测试结果\n对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。\n\n# ner\n\n* 数据说明\nner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。\n\n* 测试结果\n对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。\n\n* 结果分析\n从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。\n\n\n","slug":"hmm4nlp","published":1,"updated":"2016-09-02T08:44:17.690Z","layout":"post","link":"","_id":"cjh96tx2v0007hoc5mgewom5k","content":"<p>看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称<strong>tag</strong>）、分词（Segmentation，以下简称<strong>seg</strong>）和命名实体识别（Named Entity Recognition，以下简称<strong>ner</strong>）。</p>\n<p>具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文<a href=\"http://tripleday.github.io/2016/07/14/hmm-memm-crf/\">HMM、MEMM和CRF的学习总结</a>和里面提供的一些链接。</p>\n<h1 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h1><p>整个HMM的代码和相关测试数据已上传至Github上，附上<a href=\"https://github.com/tripleday/simple_HMM\" target=\"_blank\" rel=\"external\">链接</a>。整个代码的实现有部分学习参考博客<a href=\"http://blog.csdn.net/soundfuture/article/details/4135216\" target=\"_blank\" rel=\"external\">python词法分析(分词+词性标注）</a>，感谢博主的分享。</p>\n<ul>\n<li><p>文件图如下：<br><img src=\"/uploads/img/20160728/file.png\" alt=\"\"><br>这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见<a href=\"http://www.cnts.ua.ac.be/conll2000/chunking/\" target=\"_blank\" rel=\"external\">Chunking</a>，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。</p>\n</li>\n<li><p>其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。</p>\n</li>\n</ul>\n<h1 id=\"tag\"><a href=\"#tag\" class=\"headerlink\" title=\"tag\"></a>tag</h1><ul>\n<li><p>数据说明<br>tag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。</p>\n<ul>\n<li>conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，<strong>第二列</strong>是由Brill tagger标注的词性，<strong>第三列</strong>是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。</li>\n<li>pku的数据是中文语料，且已经进行过分词操作，它的词性标注与<a href=\"http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a\" target=\"_blank\" rel=\"external\">近代汉语词类标注简表</a>相似但不完全相同。</li>\n</ul>\n</li>\n<li><p>测试结果<br>关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的<strong>Brill tagger</strong>的准确率约为94.09%，对第三列的<strong>WSJ corpus</strong>的准确率约为87.93%。pku的准确率约为93.0%。</p>\n</li>\n<li><p>结果分析</p>\n<ul>\n<li>本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。</li>\n<li>我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"seg\"><a href=\"#seg\" class=\"headerlink\" title=\"seg\"></a>seg</h1><ul>\n<li><p>处理思路<br>seg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953\" target=\"_blank\" rel=\"external\">中文分词入门之字标注法3</a>。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。</p>\n</li>\n</ul>\n<h1 id=\"ner\"><a href=\"#ner\" class=\"headerlink\" title=\"ner\"></a>ner</h1><ul>\n<li><p>数据说明<br>ner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。</p>\n</li>\n<li><p>结果分析<br>从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。</p>\n</li>\n</ul>\n","excerpt":"","more":"<p>看到网上有很多分词、词性标注的工具，但大多是已经训练过的模型，有些可以添加一些自定义的词典来定制模型。趁同学给了我一些nlp的中文语料数据，我就尝试自己实现一个简单的HMM（隐马尔科夫模型）来进行中文的词性标注（Part-of-Speech tagging或POS tagging，以下简称<strong>tag</strong>）、分词（Segmentation，以下简称<strong>seg</strong>）和命名实体识别（Named Entity Recognition，以下简称<strong>ner</strong>）。</p>\n<p>具体关于HMM的内容，这篇博文里面不做赘述，读者可以自行学习了解，也可以参考本人之前的一篇博文<a href=\"http://tripleday.github.io/2016/07/14/hmm-memm-crf/\">HMM、MEMM和CRF的学习总结</a>和里面提供的一些链接。</p>\n<h1 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h1><p>整个HMM的代码和相关测试数据已上传至Github上，附上<a href=\"https://github.com/tripleday/simple_HMM\">链接</a>。整个代码的实现有部分学习参考博客<a href=\"http://blog.csdn.net/soundfuture/article/details/4135216\">python词法分析(分词+词性标注）</a>，感谢博主的分享。</p>\n<ul>\n<li><p>文件图如下：<br><img src=\"/uploads/img/20160728/file.png\" alt=\"\"><br>这四个文件夹中都是用的是同一个HMM模型，只是测试数据和目标任务不同而已。其中tag相关的有两个：conll_tag和pku_tag，conll_tag使用的CoNLL-2000的英文数据，具体数据下载见<a href=\"http://www.cnts.ua.ac.be/conll2000/chunking/\">Chunking</a>，pku_tag使用的一个北大同学给的课程作业的中文语料数据。pku_seg和pku_ner同理分别是在相应的数据上进行的分词和命名实体识别。</p>\n</li>\n<li><p>其实，seg和ner的实现依赖于tag的词性标注，只是seg、ner要学习的的标签不同。整个HMM的实现也中规中矩，从语料数据中学习转移概率、发射概率等等，然后利用viterbi算法求解最大路径。其中在计算路径概率的时候，为了防止概率相乘过小约等于零的情况，程序取概率对数再取反，将概率相乘转化为对数相加。</p>\n</li>\n</ul>\n<h1 id=\"tag\"><a href=\"#tag\" class=\"headerlink\" title=\"tag\"></a>tag</h1><ul>\n<li><p>数据说明<br>tag这里指语句的词性标注，当然不同的语料采用的词性体系不太相同。</p>\n<ul>\n<li>conll2000为英文语料，给出的数据一共有三列，每一列代表的含义在上面给出的链接里有提及。第一列是英文单词本身，<strong>第二列</strong>是由Brill tagger标注的词性，<strong>第三列</strong>是华尔街日报语料库产生的标注，其实第三列的标注和分词的标注方法类似。</li>\n<li>pku的数据是中文语料，且已经进行过分词操作，它的词性标注与<a href=\"http://wenku.baidu.com/view/3c5488b75ef7ba0d4b733b1a\">近代汉语词类标注简表</a>相似但不完全相同。</li>\n</ul>\n</li>\n<li><p>测试结果<br>关于准确率，程序将数据七三分做交叉验证来计算。conll2000对第二列的<strong>Brill tagger</strong>的准确率约为94.09%，对第三列的<strong>WSJ corpus</strong>的准确率约为87.93%。pku的准确率约为93.0%。</p>\n</li>\n<li><p>结果分析</p>\n<ul>\n<li>本文实现的HMM对实际详细的词性标注如conll的Brill tagger和pku的标注有较好的效果，而conll的WSJ corpus的标注偏向语句的分词，效果会差一些。</li>\n<li>我实现的HMM在训练完进行标注的时候，没有去检测一些数字，日期等等的存在，而是单纯地看它在不在训练集里，比如之前训练时统计到过6.11是数字，但没见过6.12，那之后就标注不出来了。这是很大一块需要改进的地方。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"seg\"><a href=\"#seg\" class=\"headerlink\" title=\"seg\"></a>seg</h1><ul>\n<li><p>处理思路<br>seg的处理思路就是将训练数据转化为上一项tag（POS Tagging）所需的训练数据格式，程序里我采用的是4-tag（B（Begin，词首）, E（End，词尾）, M（Middle，词中）, S（Single,单字词））标记集。这里转化来转化去的脚本需要自己写。关于这种标记方法和转化过程的详细介绍可以参考这篇<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%953\">中文分词入门之字标注法3</a>。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为92.13%，召回率为91.88%，F1值为92.01%。</p>\n</li>\n</ul>\n<h1 id=\"ner\"><a href=\"#ner\" class=\"headerlink\" title=\"ner\"></a>ner</h1><ul>\n<li><p>数据说明<br>ner的训练数据里，非命名实体的都标注为N，“北京市”中“北”标为“B-LOC”，代表为地名的开始，“京”和“市”跟在后面则标为“I-LOC”。在最后计算准确率和召回率的时候，标为“N”的字都不需要考虑，仅看那些命名实体是否被标注出来。</p>\n</li>\n<li><p>测试结果<br>对pku的数据测试得到的准确率为67.6%，召回率为63.9%，F1值为65.7%。</p>\n</li>\n<li><p>结果分析<br>从结果可以看出，在不使用规则或字典等其他方法干预的情况下，单纯使用HMM对命名实体识别效果较差。</p>\n</li>\n</ul>\n"},{"title":"使用python将图片转化为字符画","date":"2016-07-20T13:35:43.000Z","comments":1,"photos":["/uploads/img/20160720/cover.png"],"_content":"忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：\n![拿衣服](/uploads/img/20160720/mo.jpg)\n当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：[Emoji Mosaic](http://ericandrewlewis.github.io/emoji-mosaic/)。\n\n做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：[ericandrewlewis/emoji-mosaic](https://github.com/ericandrewlewis/emoji-mosaic)。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。\n\n代码很简单：\n```python\n#-*- coding: utf-8 -*-\nfrom PIL import Image\n \ngrey2char = ['@','#','$','%','&','?','*','o','/','{','[','(','|','!','^','~','-','_',':',';',',','.','`',' ']\ncount = len(grey2char)\n \ndef toText(image_file):\n   image_file = image_file.convert('L')# 转灰度\n   result = ''# 储存字符串\n   for h in range(0,  image_file.size[1]):# height\n      for w in range(0, image_file.size[0]):# width\n         gray = image_file.getpixel((w,h))\n         result += grey2char[int(gray/(255/(count-1)))]\n      result += '\\r\\n'\n   return result\n \nimage_file = Image.open(\"input.jpg\")# 打开图片\nimage_file = image_file.resize((int(image_file.size[0]), int(image_file.size[1]*0.55)))# 调整图片大小\n \noutput = open('output.txt','w')\noutput.write(toText(image_file))\noutput.close()\n```\n需要注意的是要安装依赖PIL。\n\n原图：\n![](/uploads/img/20160720/input.jpg)\n\n字符图：\n![](/uploads/img/20160720/cover.png)\n","source":"_posts/img2txt.md","raw":"title: 使用python将图片转化为字符画\ndate: 2016-07-20 21:35:43\ncomments: true\ntags: \n - PIL\n - python\ncategories: Fun\nphotos: \n - /uploads/img/20160720/cover.png\n---\n忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：\n![拿衣服](/uploads/img/20160720/mo.jpg)\n当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：[Emoji Mosaic](http://ericandrewlewis.github.io/emoji-mosaic/)。\n\n做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：[ericandrewlewis/emoji-mosaic](https://github.com/ericandrewlewis/emoji-mosaic)。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。\n\n代码很简单：\n```python\n#-*- coding: utf-8 -*-\nfrom PIL import Image\n \ngrey2char = ['@','#','$','%','&','?','*','o','/','{','[','(','|','!','^','~','-','_',':',';',',','.','`',' ']\ncount = len(grey2char)\n \ndef toText(image_file):\n   image_file = image_file.convert('L')# 转灰度\n   result = ''# 储存字符串\n   for h in range(0,  image_file.size[1]):# height\n      for w in range(0, image_file.size[0]):# width\n         gray = image_file.getpixel((w,h))\n         result += grey2char[int(gray/(255/(count-1)))]\n      result += '\\r\\n'\n   return result\n \nimage_file = Image.open(\"input.jpg\")# 打开图片\nimage_file = image_file.resize((int(image_file.size[0]), int(image_file.size[1]*0.55)))# 调整图片大小\n \noutput = open('output.txt','w')\noutput.write(toText(image_file))\noutput.close()\n```\n需要注意的是要安装依赖PIL。\n\n原图：\n![](/uploads/img/20160720/input.jpg)\n\n字符图：\n![](/uploads/img/20160720/cover.png)\n","slug":"img2txt","published":1,"updated":"2016-07-20T14:20:30.941Z","layout":"post","link":"","_id":"cjh96tx2x0009hoc5egzfxu3r","content":"<p>忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：<br><img src=\"/uploads/img/20160720/mo.jpg\" alt=\"拿衣服\"><br>当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：<a href=\"http://ericandrewlewis.github.io/emoji-mosaic/\" target=\"_blank\" rel=\"external\">Emoji Mosaic</a>。</p>\n<p>做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：<a href=\"https://github.com/ericandrewlewis/emoji-mosaic\" target=\"_blank\" rel=\"external\">ericandrewlewis/emoji-mosaic</a>。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。</p>\n<p>代码很简单：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"> </span><br><span class=\"line\">grey2char = [<span class=\"string\">'@'</span>,<span class=\"string\">'#'</span>,<span class=\"string\">'$'</span>,<span class=\"string\">'%'</span>,<span class=\"string\">'&amp;'</span>,<span class=\"string\">'?'</span>,<span class=\"string\">'*'</span>,<span class=\"string\">'o'</span>,<span class=\"string\">'/'</span>,<span class=\"string\">'&#123;'</span>,<span class=\"string\">'['</span>,<span class=\"string\">'('</span>,<span class=\"string\">'|'</span>,<span class=\"string\">'!'</span>,<span class=\"string\">'^'</span>,<span class=\"string\">'~'</span>,<span class=\"string\">'-'</span>,<span class=\"string\">'_'</span>,<span class=\"string\">':'</span>,<span class=\"string\">';'</span>,<span class=\"string\">','</span>,<span class=\"string\">'.'</span>,<span class=\"string\">'`'</span>,<span class=\"string\">' '</span>]</span><br><span class=\"line\">count = len(grey2char)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toText</span><span class=\"params\">(image_file)</span>:</span></span><br><span class=\"line\">   image_file = image_file.convert(<span class=\"string\">'L'</span>)<span class=\"comment\"># 转灰度</span></span><br><span class=\"line\">   result = <span class=\"string\">''</span><span class=\"comment\"># 储存字符串</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> h <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,  image_file.size[<span class=\"number\">1</span>]):<span class=\"comment\"># height</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, image_file.size[<span class=\"number\">0</span>]):<span class=\"comment\"># width</span></span><br><span class=\"line\">         gray = image_file.getpixel((w,h))</span><br><span class=\"line\">         result += grey2char[int(gray/(<span class=\"number\">255</span>/(count<span class=\"number\">-1</span>)))]</span><br><span class=\"line\">      result += <span class=\"string\">'\\r\\n'</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> result</span><br><span class=\"line\"> </span><br><span class=\"line\">image_file = Image.open(<span class=\"string\">\"input.jpg\"</span>)<span class=\"comment\"># 打开图片</span></span><br><span class=\"line\">image_file = image_file.resize((int(image_file.size[<span class=\"number\">0</span>]), int(image_file.size[<span class=\"number\">1</span>]*<span class=\"number\">0.55</span>)))<span class=\"comment\"># 调整图片大小</span></span><br><span class=\"line\"> </span><br><span class=\"line\">output = open(<span class=\"string\">'output.txt'</span>,<span class=\"string\">'w'</span>)</span><br><span class=\"line\">output.write(toText(image_file))</span><br><span class=\"line\">output.close()</span><br></pre></td></tr></table></figure></p>\n<p>需要注意的是要安装依赖PIL。</p>\n<p>原图：<br><img src=\"/uploads/img/20160720/input.jpg\" alt=\"\"></p>\n<p>字符图：<br><img src=\"/uploads/img/20160720/cover.png\" alt=\"\"></p>\n","excerpt":"","more":"<p>忽然想玩这个图片转化的把戏，是因为之前在知乎上看到一个专栏里用到了下面这个图：<br><img src=\"/uploads/img/20160720/mo.jpg\" alt=\"拿衣服\"><br>当然，那篇专栏没过几小时就被和谐了。我在网上貌似搜到了这个图片转emoji mosaic的网址，供大家戏耍：<a href=\"http://ericandrewlewis.github.io/emoji-mosaic/\">Emoji Mosaic</a>。</p>\n<p>做这个转emoji马赛克应该蛮复杂的，当然它的源码也很容易找到：<a href=\"https://github.com/ericandrewlewis/emoji-mosaic\">ericandrewlewis/emoji-mosaic</a>。但我纯属娱乐又不想花功夫，于是就在晚上学了学把图片转为字符图的代码玩玩。</p>\n<p>代码很简单：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"> </span><br><span class=\"line\">grey2char = [<span class=\"string\">'@'</span>,<span class=\"string\">'#'</span>,<span class=\"string\">'$'</span>,<span class=\"string\">'%'</span>,<span class=\"string\">'&amp;'</span>,<span class=\"string\">'?'</span>,<span class=\"string\">'*'</span>,<span class=\"string\">'o'</span>,<span class=\"string\">'/'</span>,<span class=\"string\">'&#123;'</span>,<span class=\"string\">'['</span>,<span class=\"string\">'('</span>,<span class=\"string\">'|'</span>,<span class=\"string\">'!'</span>,<span class=\"string\">'^'</span>,<span class=\"string\">'~'</span>,<span class=\"string\">'-'</span>,<span class=\"string\">'_'</span>,<span class=\"string\">':'</span>,<span class=\"string\">';'</span>,<span class=\"string\">','</span>,<span class=\"string\">'.'</span>,<span class=\"string\">'`'</span>,<span class=\"string\">' '</span>]</span><br><span class=\"line\">count = len(grey2char)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toText</span><span class=\"params\">(image_file)</span>:</span></span><br><span class=\"line\">   image_file = image_file.convert(<span class=\"string\">'L'</span>)<span class=\"comment\"># 转灰度</span></span><br><span class=\"line\">   result = <span class=\"string\">''</span><span class=\"comment\"># 储存字符串</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> h <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,  image_file.size[<span class=\"number\">1</span>]):<span class=\"comment\"># height</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, image_file.size[<span class=\"number\">0</span>]):<span class=\"comment\"># width</span></span><br><span class=\"line\">         gray = image_file.getpixel((w,h))</span><br><span class=\"line\">         result += grey2char[int(gray/(<span class=\"number\">255</span>/(count<span class=\"number\">-1</span>)))]</span><br><span class=\"line\">      result += <span class=\"string\">'\\r\\n'</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> result</span><br><span class=\"line\"> </span><br><span class=\"line\">image_file = Image.open(<span class=\"string\">\"input.jpg\"</span>)<span class=\"comment\"># 打开图片</span></span><br><span class=\"line\">image_file = image_file.resize((int(image_file.size[<span class=\"number\">0</span>]), int(image_file.size[<span class=\"number\">1</span>]*<span class=\"number\">0.55</span>)))<span class=\"comment\"># 调整图片大小</span></span><br><span class=\"line\"> </span><br><span class=\"line\">output = open(<span class=\"string\">'output.txt'</span>,<span class=\"string\">'w'</span>)</span><br><span class=\"line\">output.write(toText(image_file))</span><br><span class=\"line\">output.close()</span><br></pre></td></tr></table></figure></p>\n<p>需要注意的是要安装依赖PIL。</p>\n<p>原图：<br><img src=\"/uploads/img/20160720/input.jpg\" alt=\"\"></p>\n<p>字符图：<br><img src=\"/uploads/img/20160720/cover.png\" alt=\"\"></p>\n"},{"title":"推荐系统中NMF的一点看法","date":"2017-01-12T04:23:41.000Z","comments":1,"_content":"NMF（Non-negative Matrix Factorization），即**非负**矩阵分解，它可以简单定义为：给定矩阵$X∈R_+^{(n×m)}$，寻找非负矩阵$W∈R_+^{(n×k)}$和非负矩阵$H∈R_+^{(k×m)}$，使得$X≈WH$。其简单的直观表示如下图所示：\n![](/uploads/img/20170112/mf.png)\n在计算中等式左右很难完全相等。在计算中往往是根据某更新法则迭代更新出两个乘子，当上式左右两端的距离（如欧式距离）满足我们设定的大小，停止迭代。它的目标很明确，就是将大矩阵分解成两个小矩阵，使得这两个小矩阵相乘后能够还原到大矩阵，而非负表示分解的矩阵都不包含负值。矩阵分解能够用于发现两种实体间的潜在特征，一个最常见的应用就是协同过滤中的预测打分值，即采用矩阵分解来进行用户推荐，而从协同过滤的这个角度来说，矩阵的非负性也很容易理解：用户打分都是正的，不会出现负值。\n\n在例如MovieLens这样的推荐系统中，有用户和电影两个集合。给出每个用户对部分电影的打分，我们希望预测该用户对其他没看过电影的打分值，这样可以根据打分值为其做出推荐。用户和电影的关系，可以用一个矩阵来表示，每一行表示用户，每一列表示电影，每个元素的值表示用户（User）对已经看过的电影（Item）的打分（1-5分），矩阵形如下表，表中的 -- 代表该用户未对此电影打分：\n![](/uploads/img/20170112/rank.png)\n\n在对上述矩阵进行矩阵分解时，我们希望得到如下图的矩阵乘积，为方便理解和之后的可视化操作，我们假设特征（Feature）的数量K为2：\n![](/uploads/img/20170112/f.png)\n\n图中$X$为原始的评分矩阵，$W$为用户对特征的偏好程度矩阵，$H$为电影对特征的拥有程度矩阵。$W$的每一行表示用户，每一列表示一个特征，它们的值表示用户与某一特征的相关性，值越大，表明特征越明显。同理，$H$的每一行表示电影，每一列表示电影与特征的关联。最后为了预测用户算$U_i$对特定电影$I_j$的评分，我们可以直接计算$U_i$和$I_j$对应的特征向量的点积。\n\n使用矩阵分解来预测评分的思想来源于，我们可以通过矩阵分解来发现一些用户打分的潜在特征。比如两个人都喜欢某一演员，那他们就倾向于给TA演的电影打高分；或者两个人都喜欢动作片，则他们会偏向给动作片好评。假如我们能够发现这些特征，我们就能够预测特定用户对特定电影的打分。通常情况下，我们假设特征的数量少于用户和电影的数量，但同时也必须保证不能过少或过多，否则会陷入欠拟合和过拟合的情况。\n\n接下来是我有些不解的地方：在实际生活中，我们有时很难拿到用户所有的打分值，所以得到的用户评分是一个非常**稀疏**的矩阵。由于NMF分解是对原矩阵的近似还原分解，所以该算法会尽力去拟合原矩阵中的大量的**零值点**，这对于其他项目的评分预测都是相当不准确的。所以我想尝试利用梯度下降的方法，只去拟合那些非零值，以期可以忽略零值对分解结果的影响。\n\n# 目标函数\n\n- 误差函数\n$$error=\\sum\\limits_{i,j}^{X_{ij}>0} [X_{ij}-(WH)_{ij}]^2$$\n计算预测评分与原始评分之差的平方和，并将之作为误差函数进行最优化，其中的计算项只包括原始评分不为零的评分项。\n\n- 正则化项\n$$reg=β(‖W‖^2+‖H‖^2)$$\n上式中，$β$为权重，$‖W‖^2$和$‖H‖^2$分别为矩阵$W$和$H$的L2范数，即矩阵中所有元素的平方和。\n\n- 目标函数\n$$T(W,H)=\\sum\\limits_{i,j}^{X_{ij}>0} [X_{ij}-(WH)_{ij}]^2+β(‖W‖^2+‖H‖^2)$$\n\n# 梯度下降\n\n- 计算偏导\n$$\\frac{∂T(W,H)}{∂W_{ik}}=-\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})H_{kj}+2βW_{ik}$$\n$$\\frac{∂T(W,H)}{∂H_{kj}}=-\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})W_{ik}+2βH_{kj}$$\n上式中，我只考虑原始矩阵中的非零元素，因为其中的零值只代表用户未对电影做出评分，并不代表该用户对此电影的评分就是0，所以我们在计算时不对零值进行拟合。\n\n- 迭代更新\n$$W_{ik}=W_{ik}+α(\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k (X_{ij}-(WH)_{ij})H_{kj}-βW_{ik})$$\n$$H_{kj}=H_{kj}+α(\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k (X_{ij}-(WH)_{ij})W_{ik}-βH_{kj})$$\n其中，$α$是学习速率，即每次前进的步长，$α$越大，迭代下降的越快。上式中，我们将因子2提取出来计入$α$，简化表达式。\n\n# 非负性保证\n\n传统NMF中，如[非负矩阵分解（NMF）](http://blog.csdn.net/acdreamers/article/details/44663421)中所示的高斯分布噪声下，可以转化为矩阵乘法的迭代，从而保证非负性。而这里情况就不同，我没有依据地自创了两个条件保证非负性。\n- $W$与$H$的初始化\n随机初始化时保证所有元素均为非负数，则能够在后续迭代中提供一个很好的非负性保证。\n事实上，$W$与$H$的结果对初始状态是非常敏感的。我在实现时参考了sklearn里nmf采用的一种基于svd分解的初始化方法。\n\n- 学习速率递减\n在$W_{ik}$或者$H_{kj}$在梯度下降时，如果某个元素值即将发生由非负到负的转变时，我们将学习速率自动减半，重新进行迭代计算，如果仍然产生负值，则重复上述步骤，当学习速率减半操作次数达到一定阈值时，我们则判定该元素无法继续进行迭代，保持原来的数值不变。\n这点没有什么理论支撑，纯属自己臆想。\n\n# 推荐应用\n假设有如下所示的用户对电影的评分矩阵：\n![](/uploads/img/20170112/x.png)\n对其做非负矩阵分解，可得$W$矩阵和$H$矩阵：\n![](/uploads/img/20170112/w.png)\n![](/uploads/img/20170112/h.png)\n我们将$W$和$H$矩阵相乘，可以重构还原得到如下矩阵：\n![](/uploads/img/20170112/wh.png)\n\n可以看到，重构还原后的矩阵$V$跟原矩阵$X$很接近，并且对原来空缺的值作出了预测。我们在这些结果的基础上,通过计算用户间或电影间的相似度，进行一些推荐的应用，可以参考博客[矩阵分解在推荐系统中的应用：NMF和经典SVD实战](http://www.letiantian.me/2015-05-25-nmf-svd-recommend/?utm_source=tuicool&utm_medium=referral)。\n\n# 与SVD和NMF的对比\n\n相关代码上传至Github，附上[链接](https://github.com/tripleday/sparse_NMF)，很杂乱。\n\n我使用了MovieLens推荐系统的公开100K实验数据集，为了比较本文算法在评分较少的稀疏矩阵中的效果，实验中选取了已看电影不超过100部的观众，根据其对一些电影的评价来预测对其他部分电影的评分。训练集中选取了655个人对1342部电影的评分，分数仅限1,2,3,4,5这五种打分，测试数据集是对应观众对其他部分电影的评分。\n\n实验采用的算法除了本文的稀疏矩阵分解（Sparse NMF），还有现有的奇异值矩阵分解法（SVD）和非负矩阵分解法（NMF），以及评分全为3分的平均值矩阵（Average）来做比较。算法的预测评分中，小于1分的将被划为1，大于5分的将被划为5。实验中采用的误差评价为测试集中有效评分与算法预测评分差的平方和，公式表示如下：\n$$error=\\sum\\limits_{i,j}^{X_{ij}>0} [X_{ij}-V_{ij}]^2$$\n\n上式中$X_{ij}$表示用户$i$对电影$j$的实际评分，$V_{ij}$表示用户$i$对电影$j$的预测评分。实验得到的结果对比如下图：\n![](/uploads/img/20170112/figure.png)\n上图中的横坐标components数目是上面提到的特征数量K。\n\n由于SVD矩阵分解和NMF分解都是对原矩阵的近似还原分解，所以这两种算法会尽力去拟合原矩阵中的大量的零值点，这对于其他电影的评分预测都是相当不准确的。他们的性能甚至比平均值猜测都差很多，所以在稀疏的评分矩阵中，SVD矩阵分解和NMF分解都是不太合适的。而本文的Sparse NMF误差大约为平均值的70%，有一定的效果，但仍有很大的改善空间。\n\n吐槽：\n- 我这种一个元素一个元素计算的方法效率很低，没考虑优化的问题。\n- 我还试过另外一种方法，将所有空缺值用所有其他非零项平均值填补，之后再使用SVD或者NMF，效果比不填补使用Sparse NMF要好，填补再用Sparse NMF我也没试过。\n\n\n以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。\n本文有参考以下博文：\n- [NMF算法简介及python实现](http://blog.sina.com.cn/s/blog_9ce5a1b501018vb2.html)\n- [使用LFM（Latent factor model）隐语义模型进行Top-N推荐](http://blog.csdn.net/harryhuang1990/article/details/9924377)\n- [浅谈矩阵分解在推荐系统中的应用](http://blog.csdn.net/sun_168/article/details/20637833)\n- [矩阵分解在推荐系统中的应用](http://www.dataguru.cn/thread-484000-1-1.html)\n\n\n\n\n","source":"_posts/sparse-nmf.md","raw":"title: 推荐系统中NMF的一点看法\ndate: 2017-01-12 12:23:41\ncomments: true\ntags: \n - NMF\n - python\ncategories: ML\n---\nNMF（Non-negative Matrix Factorization），即**非负**矩阵分解，它可以简单定义为：给定矩阵$X∈R_+^{(n×m)}$，寻找非负矩阵$W∈R_+^{(n×k)}$和非负矩阵$H∈R_+^{(k×m)}$，使得$X≈WH$。其简单的直观表示如下图所示：\n![](/uploads/img/20170112/mf.png)\n在计算中等式左右很难完全相等。在计算中往往是根据某更新法则迭代更新出两个乘子，当上式左右两端的距离（如欧式距离）满足我们设定的大小，停止迭代。它的目标很明确，就是将大矩阵分解成两个小矩阵，使得这两个小矩阵相乘后能够还原到大矩阵，而非负表示分解的矩阵都不包含负值。矩阵分解能够用于发现两种实体间的潜在特征，一个最常见的应用就是协同过滤中的预测打分值，即采用矩阵分解来进行用户推荐，而从协同过滤的这个角度来说，矩阵的非负性也很容易理解：用户打分都是正的，不会出现负值。\n\n在例如MovieLens这样的推荐系统中，有用户和电影两个集合。给出每个用户对部分电影的打分，我们希望预测该用户对其他没看过电影的打分值，这样可以根据打分值为其做出推荐。用户和电影的关系，可以用一个矩阵来表示，每一行表示用户，每一列表示电影，每个元素的值表示用户（User）对已经看过的电影（Item）的打分（1-5分），矩阵形如下表，表中的 -- 代表该用户未对此电影打分：\n![](/uploads/img/20170112/rank.png)\n\n在对上述矩阵进行矩阵分解时，我们希望得到如下图的矩阵乘积，为方便理解和之后的可视化操作，我们假设特征（Feature）的数量K为2：\n![](/uploads/img/20170112/f.png)\n\n图中$X$为原始的评分矩阵，$W$为用户对特征的偏好程度矩阵，$H$为电影对特征的拥有程度矩阵。$W$的每一行表示用户，每一列表示一个特征，它们的值表示用户与某一特征的相关性，值越大，表明特征越明显。同理，$H$的每一行表示电影，每一列表示电影与特征的关联。最后为了预测用户算$U_i$对特定电影$I_j$的评分，我们可以直接计算$U_i$和$I_j$对应的特征向量的点积。\n\n使用矩阵分解来预测评分的思想来源于，我们可以通过矩阵分解来发现一些用户打分的潜在特征。比如两个人都喜欢某一演员，那他们就倾向于给TA演的电影打高分；或者两个人都喜欢动作片，则他们会偏向给动作片好评。假如我们能够发现这些特征，我们就能够预测特定用户对特定电影的打分。通常情况下，我们假设特征的数量少于用户和电影的数量，但同时也必须保证不能过少或过多，否则会陷入欠拟合和过拟合的情况。\n\n接下来是我有些不解的地方：在实际生活中，我们有时很难拿到用户所有的打分值，所以得到的用户评分是一个非常**稀疏**的矩阵。由于NMF分解是对原矩阵的近似还原分解，所以该算法会尽力去拟合原矩阵中的大量的**零值点**，这对于其他项目的评分预测都是相当不准确的。所以我想尝试利用梯度下降的方法，只去拟合那些非零值，以期可以忽略零值对分解结果的影响。\n\n# 目标函数\n\n- 误差函数\n$$error=\\sum\\limits_{i,j}^{X_{ij}>0} [X_{ij}-(WH)_{ij}]^2$$\n计算预测评分与原始评分之差的平方和，并将之作为误差函数进行最优化，其中的计算项只包括原始评分不为零的评分项。\n\n- 正则化项\n$$reg=β(‖W‖^2+‖H‖^2)$$\n上式中，$β$为权重，$‖W‖^2$和$‖H‖^2$分别为矩阵$W$和$H$的L2范数，即矩阵中所有元素的平方和。\n\n- 目标函数\n$$T(W,H)=\\sum\\limits_{i,j}^{X_{ij}>0} [X_{ij}-(WH)_{ij}]^2+β(‖W‖^2+‖H‖^2)$$\n\n# 梯度下降\n\n- 计算偏导\n$$\\frac{∂T(W,H)}{∂W_{ik}}=-\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})H_{kj}+2βW_{ik}$$\n$$\\frac{∂T(W,H)}{∂H_{kj}}=-\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})W_{ik}+2βH_{kj}$$\n上式中，我只考虑原始矩阵中的非零元素，因为其中的零值只代表用户未对电影做出评分，并不代表该用户对此电影的评分就是0，所以我们在计算时不对零值进行拟合。\n\n- 迭代更新\n$$W_{ik}=W_{ik}+α(\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k (X_{ij}-(WH)_{ij})H_{kj}-βW_{ik})$$\n$$H_{kj}=H_{kj}+α(\\sum\\limits_{i,j}^{X_{ij}>0} \\sum\\limits_k (X_{ij}-(WH)_{ij})W_{ik}-βH_{kj})$$\n其中，$α$是学习速率，即每次前进的步长，$α$越大，迭代下降的越快。上式中，我们将因子2提取出来计入$α$，简化表达式。\n\n# 非负性保证\n\n传统NMF中，如[非负矩阵分解（NMF）](http://blog.csdn.net/acdreamers/article/details/44663421)中所示的高斯分布噪声下，可以转化为矩阵乘法的迭代，从而保证非负性。而这里情况就不同，我没有依据地自创了两个条件保证非负性。\n- $W$与$H$的初始化\n随机初始化时保证所有元素均为非负数，则能够在后续迭代中提供一个很好的非负性保证。\n事实上，$W$与$H$的结果对初始状态是非常敏感的。我在实现时参考了sklearn里nmf采用的一种基于svd分解的初始化方法。\n\n- 学习速率递减\n在$W_{ik}$或者$H_{kj}$在梯度下降时，如果某个元素值即将发生由非负到负的转变时，我们将学习速率自动减半，重新进行迭代计算，如果仍然产生负值，则重复上述步骤，当学习速率减半操作次数达到一定阈值时，我们则判定该元素无法继续进行迭代，保持原来的数值不变。\n这点没有什么理论支撑，纯属自己臆想。\n\n# 推荐应用\n假设有如下所示的用户对电影的评分矩阵：\n![](/uploads/img/20170112/x.png)\n对其做非负矩阵分解，可得$W$矩阵和$H$矩阵：\n![](/uploads/img/20170112/w.png)\n![](/uploads/img/20170112/h.png)\n我们将$W$和$H$矩阵相乘，可以重构还原得到如下矩阵：\n![](/uploads/img/20170112/wh.png)\n\n可以看到，重构还原后的矩阵$V$跟原矩阵$X$很接近，并且对原来空缺的值作出了预测。我们在这些结果的基础上,通过计算用户间或电影间的相似度，进行一些推荐的应用，可以参考博客[矩阵分解在推荐系统中的应用：NMF和经典SVD实战](http://www.letiantian.me/2015-05-25-nmf-svd-recommend/?utm_source=tuicool&utm_medium=referral)。\n\n# 与SVD和NMF的对比\n\n相关代码上传至Github，附上[链接](https://github.com/tripleday/sparse_NMF)，很杂乱。\n\n我使用了MovieLens推荐系统的公开100K实验数据集，为了比较本文算法在评分较少的稀疏矩阵中的效果，实验中选取了已看电影不超过100部的观众，根据其对一些电影的评价来预测对其他部分电影的评分。训练集中选取了655个人对1342部电影的评分，分数仅限1,2,3,4,5这五种打分，测试数据集是对应观众对其他部分电影的评分。\n\n实验采用的算法除了本文的稀疏矩阵分解（Sparse NMF），还有现有的奇异值矩阵分解法（SVD）和非负矩阵分解法（NMF），以及评分全为3分的平均值矩阵（Average）来做比较。算法的预测评分中，小于1分的将被划为1，大于5分的将被划为5。实验中采用的误差评价为测试集中有效评分与算法预测评分差的平方和，公式表示如下：\n$$error=\\sum\\limits_{i,j}^{X_{ij}>0} [X_{ij}-V_{ij}]^2$$\n\n上式中$X_{ij}$表示用户$i$对电影$j$的实际评分，$V_{ij}$表示用户$i$对电影$j$的预测评分。实验得到的结果对比如下图：\n![](/uploads/img/20170112/figure.png)\n上图中的横坐标components数目是上面提到的特征数量K。\n\n由于SVD矩阵分解和NMF分解都是对原矩阵的近似还原分解，所以这两种算法会尽力去拟合原矩阵中的大量的零值点，这对于其他电影的评分预测都是相当不准确的。他们的性能甚至比平均值猜测都差很多，所以在稀疏的评分矩阵中，SVD矩阵分解和NMF分解都是不太合适的。而本文的Sparse NMF误差大约为平均值的70%，有一定的效果，但仍有很大的改善空间。\n\n吐槽：\n- 我这种一个元素一个元素计算的方法效率很低，没考虑优化的问题。\n- 我还试过另外一种方法，将所有空缺值用所有其他非零项平均值填补，之后再使用SVD或者NMF，效果比不填补使用Sparse NMF要好，填补再用Sparse NMF我也没试过。\n\n\n以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。\n本文有参考以下博文：\n- [NMF算法简介及python实现](http://blog.sina.com.cn/s/blog_9ce5a1b501018vb2.html)\n- [使用LFM（Latent factor model）隐语义模型进行Top-N推荐](http://blog.csdn.net/harryhuang1990/article/details/9924377)\n- [浅谈矩阵分解在推荐系统中的应用](http://blog.csdn.net/sun_168/article/details/20637833)\n- [矩阵分解在推荐系统中的应用](http://www.dataguru.cn/thread-484000-1-1.html)\n\n\n\n\n","slug":"sparse-nmf","published":1,"updated":"2017-08-18T12:03:08.890Z","layout":"post","photos":[],"link":"","_id":"cjh96tx32000bhoc5c8i8ae0z","content":"<p>NMF（Non-negative Matrix Factorization），即<strong>非负</strong>矩阵分解，它可以简单定义为：给定矩阵$X∈R_+^{(n×m)}$，寻找非负矩阵$W∈R_+^{(n×k)}$和非负矩阵$H∈R_+^{(k×m)}$，使得$X≈WH$。其简单的直观表示如下图所示：<br><img src=\"/uploads/img/20170112/mf.png\" alt=\"\"><br>在计算中等式左右很难完全相等。在计算中往往是根据某更新法则迭代更新出两个乘子，当上式左右两端的距离（如欧式距离）满足我们设定的大小，停止迭代。它的目标很明确，就是将大矩阵分解成两个小矩阵，使得这两个小矩阵相乘后能够还原到大矩阵，而非负表示分解的矩阵都不包含负值。矩阵分解能够用于发现两种实体间的潜在特征，一个最常见的应用就是协同过滤中的预测打分值，即采用矩阵分解来进行用户推荐，而从协同过滤的这个角度来说，矩阵的非负性也很容易理解：用户打分都是正的，不会出现负值。</p>\n<p>在例如MovieLens这样的推荐系统中，有用户和电影两个集合。给出每个用户对部分电影的打分，我们希望预测该用户对其他没看过电影的打分值，这样可以根据打分值为其做出推荐。用户和电影的关系，可以用一个矩阵来表示，每一行表示用户，每一列表示电影，每个元素的值表示用户（User）对已经看过的电影（Item）的打分（1-5分），矩阵形如下表，表中的 – 代表该用户未对此电影打分：<br><img src=\"/uploads/img/20170112/rank.png\" alt=\"\"></p>\n<p>在对上述矩阵进行矩阵分解时，我们希望得到如下图的矩阵乘积，为方便理解和之后的可视化操作，我们假设特征（Feature）的数量K为2：<br><img src=\"/uploads/img/20170112/f.png\" alt=\"\"></p>\n<p>图中$X$为原始的评分矩阵，$W$为用户对特征的偏好程度矩阵，$H$为电影对特征的拥有程度矩阵。$W$的每一行表示用户，每一列表示一个特征，它们的值表示用户与某一特征的相关性，值越大，表明特征越明显。同理，$H$的每一行表示电影，每一列表示电影与特征的关联。最后为了预测用户算$U_i$对特定电影$I_j$的评分，我们可以直接计算$U_i$和$I_j$对应的特征向量的点积。</p>\n<p>使用矩阵分解来预测评分的思想来源于，我们可以通过矩阵分解来发现一些用户打分的潜在特征。比如两个人都喜欢某一演员，那他们就倾向于给TA演的电影打高分；或者两个人都喜欢动作片，则他们会偏向给动作片好评。假如我们能够发现这些特征，我们就能够预测特定用户对特定电影的打分。通常情况下，我们假设特征的数量少于用户和电影的数量，但同时也必须保证不能过少或过多，否则会陷入欠拟合和过拟合的情况。</p>\n<p>接下来是我有些不解的地方：在实际生活中，我们有时很难拿到用户所有的打分值，所以得到的用户评分是一个非常<strong>稀疏</strong>的矩阵。由于NMF分解是对原矩阵的近似还原分解，所以该算法会尽力去拟合原矩阵中的大量的<strong>零值点</strong>，这对于其他项目的评分预测都是相当不准确的。所以我想尝试利用梯度下降的方法，只去拟合那些非零值，以期可以忽略零值对分解结果的影响。</p>\n<h1 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h1><ul>\n<li><p>误差函数<br>$$error=\\sum\\limits_{i,j}^{X_{ij}&gt;0} [X_{ij}-(WH)_{ij}]^2$$<br>计算预测评分与原始评分之差的平方和，并将之作为误差函数进行最优化，其中的计算项只包括原始评分不为零的评分项。</p>\n</li>\n<li><p>正则化项<br>$$reg=β(‖W‖^2+‖H‖^2)$$<br>上式中，$β$为权重，$‖W‖^2$和$‖H‖^2$分别为矩阵$W$和$H$的L2范数，即矩阵中所有元素的平方和。</p>\n</li>\n<li><p>目标函数<br>$$T(W,H)=\\sum\\limits_{i,j}^{X_{ij}&gt;0} [X_{ij}-(WH)_{ij}]^2+β(‖W‖^2+‖H‖^2)$$</p>\n</li>\n</ul>\n<h1 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h1><ul>\n<li><p>计算偏导<br>$$\\frac{∂T(W,H)}{∂W_{ik}}=-\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})H_{kj}+2βW_{ik}$$<br>$$\\frac{∂T(W,H)}{∂H_{kj}}=-\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})W_{ik}+2βH_{kj}$$<br>上式中，我只考虑原始矩阵中的非零元素，因为其中的零值只代表用户未对电影做出评分，并不代表该用户对此电影的评分就是0，所以我们在计算时不对零值进行拟合。</p>\n</li>\n<li><p>迭代更新<br>$$W_{ik}=W_{ik}+α(\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k (X_{ij}-(WH)_{ij})H_{kj}-βW_{ik})$$<br>$$H_{kj}=H_{kj}+α(\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k (X_{ij}-(WH)_{ij})W_{ik}-βH_{kj})$$<br>其中，$α$是学习速率，即每次前进的步长，$α$越大，迭代下降的越快。上式中，我们将因子2提取出来计入$α$，简化表达式。</p>\n</li>\n</ul>\n<h1 id=\"非负性保证\"><a href=\"#非负性保证\" class=\"headerlink\" title=\"非负性保证\"></a>非负性保证</h1><p>传统NMF中，如<a href=\"http://blog.csdn.net/acdreamers/article/details/44663421\" target=\"_blank\" rel=\"external\">非负矩阵分解（NMF）</a>中所示的高斯分布噪声下，可以转化为矩阵乘法的迭代，从而保证非负性。而这里情况就不同，我没有依据地自创了两个条件保证非负性。</p>\n<ul>\n<li><p>$W$与$H$的初始化<br>随机初始化时保证所有元素均为非负数，则能够在后续迭代中提供一个很好的非负性保证。<br>事实上，$W$与$H$的结果对初始状态是非常敏感的。我在实现时参考了sklearn里nmf采用的一种基于svd分解的初始化方法。</p>\n</li>\n<li><p>学习速率递减<br>在$W_{ik}$或者$H_{kj}$在梯度下降时，如果某个元素值即将发生由非负到负的转变时，我们将学习速率自动减半，重新进行迭代计算，如果仍然产生负值，则重复上述步骤，当学习速率减半操作次数达到一定阈值时，我们则判定该元素无法继续进行迭代，保持原来的数值不变。<br>这点没有什么理论支撑，纯属自己臆想。</p>\n</li>\n</ul>\n<h1 id=\"推荐应用\"><a href=\"#推荐应用\" class=\"headerlink\" title=\"推荐应用\"></a>推荐应用</h1><p>假设有如下所示的用户对电影的评分矩阵：<br><img src=\"/uploads/img/20170112/x.png\" alt=\"\"><br>对其做非负矩阵分解，可得$W$矩阵和$H$矩阵：<br><img src=\"/uploads/img/20170112/w.png\" alt=\"\"><br><img src=\"/uploads/img/20170112/h.png\" alt=\"\"><br>我们将$W$和$H$矩阵相乘，可以重构还原得到如下矩阵：<br><img src=\"/uploads/img/20170112/wh.png\" alt=\"\"></p>\n<p>可以看到，重构还原后的矩阵$V$跟原矩阵$X$很接近，并且对原来空缺的值作出了预测。我们在这些结果的基础上,通过计算用户间或电影间的相似度，进行一些推荐的应用，可以参考博客<a href=\"http://www.letiantian.me/2015-05-25-nmf-svd-recommend/?utm_source=tuicool&amp;utm_medium=referral\" target=\"_blank\" rel=\"external\">矩阵分解在推荐系统中的应用：NMF和经典SVD实战</a>。</p>\n<h1 id=\"与SVD和NMF的对比\"><a href=\"#与SVD和NMF的对比\" class=\"headerlink\" title=\"与SVD和NMF的对比\"></a>与SVD和NMF的对比</h1><p>相关代码上传至Github，附上<a href=\"https://github.com/tripleday/sparse_NMF\" target=\"_blank\" rel=\"external\">链接</a>，很杂乱。</p>\n<p>我使用了MovieLens推荐系统的公开100K实验数据集，为了比较本文算法在评分较少的稀疏矩阵中的效果，实验中选取了已看电影不超过100部的观众，根据其对一些电影的评价来预测对其他部分电影的评分。训练集中选取了655个人对1342部电影的评分，分数仅限1,2,3,4,5这五种打分，测试数据集是对应观众对其他部分电影的评分。</p>\n<p>实验采用的算法除了本文的稀疏矩阵分解（Sparse NMF），还有现有的奇异值矩阵分解法（SVD）和非负矩阵分解法（NMF），以及评分全为3分的平均值矩阵（Average）来做比较。算法的预测评分中，小于1分的将被划为1，大于5分的将被划为5。实验中采用的误差评价为测试集中有效评分与算法预测评分差的平方和，公式表示如下：<br>$$error=\\sum\\limits_{i,j}^{X_{ij}&gt;0} [X_{ij}-V_{ij}]^2$$</p>\n<p>上式中$X_{ij}$表示用户$i$对电影$j$的实际评分，$V_{ij}$表示用户$i$对电影$j$的预测评分。实验得到的结果对比如下图：<br><img src=\"/uploads/img/20170112/figure.png\" alt=\"\"><br>上图中的横坐标components数目是上面提到的特征数量K。</p>\n<p>由于SVD矩阵分解和NMF分解都是对原矩阵的近似还原分解，所以这两种算法会尽力去拟合原矩阵中的大量的零值点，这对于其他电影的评分预测都是相当不准确的。他们的性能甚至比平均值猜测都差很多，所以在稀疏的评分矩阵中，SVD矩阵分解和NMF分解都是不太合适的。而本文的Sparse NMF误差大约为平均值的70%，有一定的效果，但仍有很大的改善空间。</p>\n<p>吐槽：</p>\n<ul>\n<li>我这种一个元素一个元素计算的方法效率很低，没考虑优化的问题。</li>\n<li>我还试过另外一种方法，将所有空缺值用所有其他非零项平均值填补，之后再使用SVD或者NMF，效果比不填补使用Sparse NMF要好，填补再用Sparse NMF我也没试过。</li>\n</ul>\n<p>以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。<br>本文有参考以下博文：</p>\n<ul>\n<li><a href=\"http://blog.sina.com.cn/s/blog_9ce5a1b501018vb2.html\" target=\"_blank\" rel=\"external\">NMF算法简介及python实现</a></li>\n<li><a href=\"http://blog.csdn.net/harryhuang1990/article/details/9924377\" target=\"_blank\" rel=\"external\">使用LFM（Latent factor model）隐语义模型进行Top-N推荐</a></li>\n<li><a href=\"http://blog.csdn.net/sun_168/article/details/20637833\" target=\"_blank\" rel=\"external\">浅谈矩阵分解在推荐系统中的应用</a></li>\n<li><a href=\"http://www.dataguru.cn/thread-484000-1-1.html\" target=\"_blank\" rel=\"external\">矩阵分解在推荐系统中的应用</a></li>\n</ul>\n","excerpt":"","more":"<p>NMF（Non-negative Matrix Factorization），即<strong>非负</strong>矩阵分解，它可以简单定义为：给定矩阵$X∈R_+^{(n×m)}$，寻找非负矩阵$W∈R_+^{(n×k)}$和非负矩阵$H∈R_+^{(k×m)}$，使得$X≈WH$。其简单的直观表示如下图所示：<br><img src=\"/uploads/img/20170112/mf.png\" alt=\"\"><br>在计算中等式左右很难完全相等。在计算中往往是根据某更新法则迭代更新出两个乘子，当上式左右两端的距离（如欧式距离）满足我们设定的大小，停止迭代。它的目标很明确，就是将大矩阵分解成两个小矩阵，使得这两个小矩阵相乘后能够还原到大矩阵，而非负表示分解的矩阵都不包含负值。矩阵分解能够用于发现两种实体间的潜在特征，一个最常见的应用就是协同过滤中的预测打分值，即采用矩阵分解来进行用户推荐，而从协同过滤的这个角度来说，矩阵的非负性也很容易理解：用户打分都是正的，不会出现负值。</p>\n<p>在例如MovieLens这样的推荐系统中，有用户和电影两个集合。给出每个用户对部分电影的打分，我们希望预测该用户对其他没看过电影的打分值，这样可以根据打分值为其做出推荐。用户和电影的关系，可以用一个矩阵来表示，每一行表示用户，每一列表示电影，每个元素的值表示用户（User）对已经看过的电影（Item）的打分（1-5分），矩阵形如下表，表中的 – 代表该用户未对此电影打分：<br><img src=\"/uploads/img/20170112/rank.png\" alt=\"\"></p>\n<p>在对上述矩阵进行矩阵分解时，我们希望得到如下图的矩阵乘积，为方便理解和之后的可视化操作，我们假设特征（Feature）的数量K为2：<br><img src=\"/uploads/img/20170112/f.png\" alt=\"\"></p>\n<p>图中$X$为原始的评分矩阵，$W$为用户对特征的偏好程度矩阵，$H$为电影对特征的拥有程度矩阵。$W$的每一行表示用户，每一列表示一个特征，它们的值表示用户与某一特征的相关性，值越大，表明特征越明显。同理，$H$的每一行表示电影，每一列表示电影与特征的关联。最后为了预测用户算$U_i$对特定电影$I_j$的评分，我们可以直接计算$U_i$和$I_j$对应的特征向量的点积。</p>\n<p>使用矩阵分解来预测评分的思想来源于，我们可以通过矩阵分解来发现一些用户打分的潜在特征。比如两个人都喜欢某一演员，那他们就倾向于给TA演的电影打高分；或者两个人都喜欢动作片，则他们会偏向给动作片好评。假如我们能够发现这些特征，我们就能够预测特定用户对特定电影的打分。通常情况下，我们假设特征的数量少于用户和电影的数量，但同时也必须保证不能过少或过多，否则会陷入欠拟合和过拟合的情况。</p>\n<p>接下来是我有些不解的地方：在实际生活中，我们有时很难拿到用户所有的打分值，所以得到的用户评分是一个非常<strong>稀疏</strong>的矩阵。由于NMF分解是对原矩阵的近似还原分解，所以该算法会尽力去拟合原矩阵中的大量的<strong>零值点</strong>，这对于其他项目的评分预测都是相当不准确的。所以我想尝试利用梯度下降的方法，只去拟合那些非零值，以期可以忽略零值对分解结果的影响。</p>\n<h1 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h1><ul>\n<li><p>误差函数<br>$$error=\\sum\\limits_{i,j}^{X_{ij}&gt;0} [X_{ij}-(WH)_{ij}]^2$$<br>计算预测评分与原始评分之差的平方和，并将之作为误差函数进行最优化，其中的计算项只包括原始评分不为零的评分项。</p>\n</li>\n<li><p>正则化项<br>$$reg=β(‖W‖^2+‖H‖^2)$$<br>上式中，$β$为权重，$‖W‖^2$和$‖H‖^2$分别为矩阵$W$和$H$的L2范数，即矩阵中所有元素的平方和。</p>\n</li>\n<li><p>目标函数<br>$$T(W,H)=\\sum\\limits_{i,j}^{X_{ij}&gt;0} [X_{ij}-(WH)_{ij}]^2+β(‖W‖^2+‖H‖^2)$$</p>\n</li>\n</ul>\n<h1 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h1><ul>\n<li><p>计算偏导<br>$$\\frac{∂T(W,H)}{∂W_{ik}}=-\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})H_{kj}+2βW_{ik}$$<br>$$\\frac{∂T(W,H)}{∂H_{kj}}=-\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k 2(X_{ij}-(WH)_{ij})W_{ik}+2βH_{kj}$$<br>上式中，我只考虑原始矩阵中的非零元素，因为其中的零值只代表用户未对电影做出评分，并不代表该用户对此电影的评分就是0，所以我们在计算时不对零值进行拟合。</p>\n</li>\n<li><p>迭代更新<br>$$W_{ik}=W_{ik}+α(\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k (X_{ij}-(WH)_{ij})H_{kj}-βW_{ik})$$<br>$$H_{kj}=H_{kj}+α(\\sum\\limits_{i,j}^{X_{ij}&gt;0} \\sum\\limits_k (X_{ij}-(WH)_{ij})W_{ik}-βH_{kj})$$<br>其中，$α$是学习速率，即每次前进的步长，$α$越大，迭代下降的越快。上式中，我们将因子2提取出来计入$α$，简化表达式。</p>\n</li>\n</ul>\n<h1 id=\"非负性保证\"><a href=\"#非负性保证\" class=\"headerlink\" title=\"非负性保证\"></a>非负性保证</h1><p>传统NMF中，如<a href=\"http://blog.csdn.net/acdreamers/article/details/44663421\">非负矩阵分解（NMF）</a>中所示的高斯分布噪声下，可以转化为矩阵乘法的迭代，从而保证非负性。而这里情况就不同，我没有依据地自创了两个条件保证非负性。</p>\n<ul>\n<li><p>$W$与$H$的初始化<br>随机初始化时保证所有元素均为非负数，则能够在后续迭代中提供一个很好的非负性保证。<br>事实上，$W$与$H$的结果对初始状态是非常敏感的。我在实现时参考了sklearn里nmf采用的一种基于svd分解的初始化方法。</p>\n</li>\n<li><p>学习速率递减<br>在$W_{ik}$或者$H_{kj}$在梯度下降时，如果某个元素值即将发生由非负到负的转变时，我们将学习速率自动减半，重新进行迭代计算，如果仍然产生负值，则重复上述步骤，当学习速率减半操作次数达到一定阈值时，我们则判定该元素无法继续进行迭代，保持原来的数值不变。<br>这点没有什么理论支撑，纯属自己臆想。</p>\n</li>\n</ul>\n<h1 id=\"推荐应用\"><a href=\"#推荐应用\" class=\"headerlink\" title=\"推荐应用\"></a>推荐应用</h1><p>假设有如下所示的用户对电影的评分矩阵：<br><img src=\"/uploads/img/20170112/x.png\" alt=\"\"><br>对其做非负矩阵分解，可得$W$矩阵和$H$矩阵：<br><img src=\"/uploads/img/20170112/w.png\" alt=\"\"><br><img src=\"/uploads/img/20170112/h.png\" alt=\"\"><br>我们将$W$和$H$矩阵相乘，可以重构还原得到如下矩阵：<br><img src=\"/uploads/img/20170112/wh.png\" alt=\"\"></p>\n<p>可以看到，重构还原后的矩阵$V$跟原矩阵$X$很接近，并且对原来空缺的值作出了预测。我们在这些结果的基础上,通过计算用户间或电影间的相似度，进行一些推荐的应用，可以参考博客<a href=\"http://www.letiantian.me/2015-05-25-nmf-svd-recommend/?utm_source=tuicool&amp;utm_medium=referral\">矩阵分解在推荐系统中的应用：NMF和经典SVD实战</a>。</p>\n<h1 id=\"与SVD和NMF的对比\"><a href=\"#与SVD和NMF的对比\" class=\"headerlink\" title=\"与SVD和NMF的对比\"></a>与SVD和NMF的对比</h1><p>相关代码上传至Github，附上<a href=\"https://github.com/tripleday/sparse_NMF\">链接</a>，很杂乱。</p>\n<p>我使用了MovieLens推荐系统的公开100K实验数据集，为了比较本文算法在评分较少的稀疏矩阵中的效果，实验中选取了已看电影不超过100部的观众，根据其对一些电影的评价来预测对其他部分电影的评分。训练集中选取了655个人对1342部电影的评分，分数仅限1,2,3,4,5这五种打分，测试数据集是对应观众对其他部分电影的评分。</p>\n<p>实验采用的算法除了本文的稀疏矩阵分解（Sparse NMF），还有现有的奇异值矩阵分解法（SVD）和非负矩阵分解法（NMF），以及评分全为3分的平均值矩阵（Average）来做比较。算法的预测评分中，小于1分的将被划为1，大于5分的将被划为5。实验中采用的误差评价为测试集中有效评分与算法预测评分差的平方和，公式表示如下：<br>$$error=\\sum\\limits_{i,j}^{X_{ij}&gt;0} [X_{ij}-V_{ij}]^2$$</p>\n<p>上式中$X_{ij}$表示用户$i$对电影$j$的实际评分，$V_{ij}$表示用户$i$对电影$j$的预测评分。实验得到的结果对比如下图：<br><img src=\"/uploads/img/20170112/figure.png\" alt=\"\"><br>上图中的横坐标components数目是上面提到的特征数量K。</p>\n<p>由于SVD矩阵分解和NMF分解都是对原矩阵的近似还原分解，所以这两种算法会尽力去拟合原矩阵中的大量的零值点，这对于其他电影的评分预测都是相当不准确的。他们的性能甚至比平均值猜测都差很多，所以在稀疏的评分矩阵中，SVD矩阵分解和NMF分解都是不太合适的。而本文的Sparse NMF误差大约为平均值的70%，有一定的效果，但仍有很大的改善空间。</p>\n<p>吐槽：</p>\n<ul>\n<li>我这种一个元素一个元素计算的方法效率很低，没考虑优化的问题。</li>\n<li>我还试过另外一种方法，将所有空缺值用所有其他非零项平均值填补，之后再使用SVD或者NMF，效果比不填补使用Sparse NMF要好，填补再用Sparse NMF我也没试过。</li>\n</ul>\n<p>以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。<br>本文有参考以下博文：</p>\n<ul>\n<li><a href=\"http://blog.sina.com.cn/s/blog_9ce5a1b501018vb2.html\">NMF算法简介及python实现</a></li>\n<li><a href=\"http://blog.csdn.net/harryhuang1990/article/details/9924377\">使用LFM（Latent factor model）隐语义模型进行Top-N推荐</a></li>\n<li><a href=\"http://blog.csdn.net/sun_168/article/details/20637833\">浅谈矩阵分解在推荐系统中的应用</a></li>\n<li><a href=\"http://www.dataguru.cn/thread-484000-1-1.html\">矩阵分解在推荐系统中的应用</a></li>\n</ul>\n"},{"title":"HMM、MEMM和CRF的学习总结","date":"2016-07-14T08:03:45.000Z","comments":1,"photos":["/uploads/img/20160714/cover.png"],"_content":"最近一直在学习NLP里最基础的几个语言模型：**隐马尔科夫模型**（Hidden Markov Model，HMM）、**最大熵马尔科夫模型**（Maximum Entropy Markov Model，MEMM）和**条件随机场**（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的**《统计学习方法》**和吴军老师的**《数学之美》**。如需这两本书的电子版可以给我留言。\n\n我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。\n\n# HMM\n\n下图是《统计学习方法》中的描述：\n![隐马尔科夫模型](/uploads/img/20160714/hmm.png)\nHMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种**生成模型**（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。\n\n如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：\n- [如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)\n- [隐马尔可夫模型（HMM）攻略](http://blog.csdn.net/likelet/article/details/7056068)\n\n# Maximum Entropy Model\n\n首先贴一下关于最大熵模型的定义：\n![最大熵模型](/uploads/img/20160714/me.png)\n最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是**特征函数**，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出**最不偏倚**的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。\n\n最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。\n\n最大熵模型的**优点**：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。\n\n最大熵模型的**不足**：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。\n\n# MEMM\n\n最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种**生成模型**（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。\n![最大熵马尔科夫模型](/uploads/img/20160714/memm.png)\n可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。\n\nHMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在**标注偏置问题**（Label Bias Problem）。\n\n关于标注偏置问题，网上最多的是下面这个例子解释：\n![](/uploads/img/20160714/label-bias-1.png)\n路径1-1-1-1的概率：0.4\\*0.45\\*0.5=0.09\n路径2-2-2-2的概率：0.018\n路径1-2-1-2的概率：0.06\n路径1-1-2-2的概率：0.066\n由此可得最优路径为：1-1-1-1\n![](/uploads/img/20160714/label-bias-2.png)\n而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。\n例子的出处参见[标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较](http://blog.csdn.net/lskyne/article/details/8669301)\n\n# CRF\n\n![线性链条件随机场模型](/uploads/img/20160714/crf-1.png)\n这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于**判别模型**（Discrimitive Model）。\n![线性链条件随机场模型图示](/uploads/img/20160714/crf-2.png)\n上式中也同样有$f_i$**特征函数**。之前我对模型中的特征函数一直不太理解。大家可以参考[中文分词入门之字标注法4](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954)这篇文章。文章主要介绍借用条件随机场工具“[CRF++: Yet Another CRF toolkit](http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip)”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客[52nlp](http://www.52nlp.cn/)。\n《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。\n\nCRF模型的**优点**：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。\n\nCRF模型的**不足**：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。\n\n更多一些详细的CRF解释可以参考知乎的相关问题[如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？](https://www.zhihu.com/question/35866596)\n\n# MEMM与CRF区别\n\n上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。\n\nMEMM的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &=& \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\\np(y_i | x_1, \\ldots, x_T, y_{i-1}) &=&\n\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}\n{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}\n\\end{eqnarray\\*}\n$$\n线性链CRF的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y|x) &=& \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}\n\\\\\n&=& \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}\n{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }\n\\end{eqnarray\\*}\n$$\n不同点：\n- 首先，CRF是**判别模型**，而MEMM我个人理解是**生成模型**。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个**不大于1**的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。\n- MEMM和CRF的**归一化位置**不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。\n- MEMM在用**viterbi算法**求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。\n\n关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：\n- [MEMM和CRF有什么不同？](https://www.zhihu.com/question/30869789)\n- [统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 ](http://blog.sina.com.cn/s/blog_8af106960102v0v1.html)\n\n# 写在最后\n\n关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为[An introduction to conditional random fields](http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)。\n\n以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。\n\n","source":"_posts/hmm-memm-crf.md","raw":"title: HMM、MEMM和CRF的学习总结\ndate: 2016-07-14 16:03:45\ncomments: true\ntags: \n - HMM\n - MEMM\n - CRF\ncategories: NLP\nphotos: \n - /uploads/img/20160714/cover.png\n---\n最近一直在学习NLP里最基础的几个语言模型：**隐马尔科夫模型**（Hidden Markov Model，HMM）、**最大熵马尔科夫模型**（Maximum Entropy Markov Model，MEMM）和**条件随机场**（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的**《统计学习方法》**和吴军老师的**《数学之美》**。如需这两本书的电子版可以给我留言。\n\n我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。\n\n# HMM\n\n下图是《统计学习方法》中的描述：\n![隐马尔科夫模型](/uploads/img/20160714/hmm.png)\nHMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种**生成模型**（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。\n\n如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：\n- [如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)\n- [隐马尔可夫模型（HMM）攻略](http://blog.csdn.net/likelet/article/details/7056068)\n\n# Maximum Entropy Model\n\n首先贴一下关于最大熵模型的定义：\n![最大熵模型](/uploads/img/20160714/me.png)\n最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是**特征函数**，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出**最不偏倚**的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。\n\n最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。\n\n最大熵模型的**优点**：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。\n\n最大熵模型的**不足**：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。\n\n# MEMM\n\n最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种**生成模型**（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。\n![最大熵马尔科夫模型](/uploads/img/20160714/memm.png)\n可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。\n\nHMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在**标注偏置问题**（Label Bias Problem）。\n\n关于标注偏置问题，网上最多的是下面这个例子解释：\n![](/uploads/img/20160714/label-bias-1.png)\n路径1-1-1-1的概率：0.4\\*0.45\\*0.5=0.09\n路径2-2-2-2的概率：0.018\n路径1-2-1-2的概率：0.06\n路径1-1-2-2的概率：0.066\n由此可得最优路径为：1-1-1-1\n![](/uploads/img/20160714/label-bias-2.png)\n而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。\n例子的出处参见[标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较](http://blog.csdn.net/lskyne/article/details/8669301)\n\n# CRF\n\n![线性链条件随机场模型](/uploads/img/20160714/crf-1.png)\n这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于**判别模型**（Discrimitive Model）。\n![线性链条件随机场模型图示](/uploads/img/20160714/crf-2.png)\n上式中也同样有$f_i$**特征函数**。之前我对模型中的特征函数一直不太理解。大家可以参考[中文分词入门之字标注法4](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954)这篇文章。文章主要介绍借用条件随机场工具“[CRF++: Yet Another CRF toolkit](http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip)”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客[52nlp](http://www.52nlp.cn/)。\n《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。\n\nCRF模型的**优点**：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。\n\nCRF模型的**不足**：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。\n\n更多一些详细的CRF解释可以参考知乎的相关问题[如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？](https://www.zhihu.com/question/35866596)\n\n# MEMM与CRF区别\n\n上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。\n\nMEMM的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &=& \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\\np(y_i | x_1, \\ldots, x_T, y_{i-1}) &=&\n\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}\n{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}\n\\end{eqnarray\\*}\n$$\n线性链CRF的公式表示如下：\n$$\n\\begin{eqnarray\\*}\np(y|x) &=& \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}\n\\\\\n&=& \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}\n{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }\n\\end{eqnarray\\*}\n$$\n不同点：\n- 首先，CRF是**判别模型**，而MEMM我个人理解是**生成模型**。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个**不大于1**的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。\n- MEMM和CRF的**归一化位置**不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。\n- MEMM在用**viterbi算法**求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。\n\n关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：\n- [MEMM和CRF有什么不同？](https://www.zhihu.com/question/30869789)\n- [统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 ](http://blog.sina.com.cn/s/blog_8af106960102v0v1.html)\n\n# 写在最后\n\n关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为[An introduction to conditional random fields](http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)。\n\n以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。\n\n","slug":"hmm-memm-crf","published":1,"updated":"2016-07-21T01:42:35.744Z","layout":"post","link":"","_id":"cjh96tx38000dhoc5torxnwqk","content":"<p>最近一直在学习NLP里最基础的几个语言模型：<strong>隐马尔科夫模型</strong>（Hidden Markov Model，HMM）、<strong>最大熵马尔科夫模型</strong>（Maximum Entropy Markov Model，MEMM）和<strong>条件随机场</strong>（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的<strong>《统计学习方法》</strong>和吴军老师的<strong>《数学之美》</strong>。如需这两本书的电子版可以给我留言。</p>\n<p>我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。</p>\n<h1 id=\"HMM\"><a href=\"#HMM\" class=\"headerlink\" title=\"HMM\"></a>HMM</h1><p>下图是《统计学习方法》中的描述：<br><img src=\"/uploads/img/20160714/hmm.png\" alt=\"隐马尔科夫模型\"><br>HMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种<strong>生成模型</strong>（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。</p>\n<p>如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/20962240\" target=\"_blank\" rel=\"external\">如何用简单易懂的例子解释隐马尔可夫模型？</a></li>\n<li><a href=\"http://blog.csdn.net/likelet/article/details/7056068\" target=\"_blank\" rel=\"external\">隐马尔可夫模型（HMM）攻略</a></li>\n</ul>\n<h1 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h1><p>首先贴一下关于最大熵模型的定义：<br><img src=\"/uploads/img/20160714/me.png\" alt=\"最大熵模型\"><br>最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是<strong>特征函数</strong>，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出<strong>最不偏倚</strong>的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。</p>\n<p>最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。</p>\n<p>最大熵模型的<strong>优点</strong>：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。</p>\n<p>最大熵模型的<strong>不足</strong>：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。</p>\n<h1 id=\"MEMM\"><a href=\"#MEMM\" class=\"headerlink\" title=\"MEMM\"></a>MEMM</h1><p>最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种<strong>生成模型</strong>（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。<br><img src=\"/uploads/img/20160714/memm.png\" alt=\"最大熵马尔科夫模型\"><br>可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。</p>\n<p>HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在<strong>标注偏置问题</strong>（Label Bias Problem）。</p>\n<p>关于标注偏置问题，网上最多的是下面这个例子解释：<br><img src=\"/uploads/img/20160714/label-bias-1.png\" alt=\"\"><br>路径1-1-1-1的概率：0.4*0.45*0.5=0.09<br>路径2-2-2-2的概率：0.018<br>路径1-2-1-2的概率：0.06<br>路径1-1-2-2的概率：0.066<br>由此可得最优路径为：1-1-1-1<br><img src=\"/uploads/img/20160714/label-bias-2.png\" alt=\"\"><br>而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。<br>例子的出处参见<a href=\"http://blog.csdn.net/lskyne/article/details/8669301\" target=\"_blank\" rel=\"external\">标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较</a></p>\n<h1 id=\"CRF\"><a href=\"#CRF\" class=\"headerlink\" title=\"CRF\"></a>CRF</h1><p><img src=\"/uploads/img/20160714/crf-1.png\" alt=\"线性链条件随机场模型\"><br>这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于<strong>判别模型</strong>（Discrimitive Model）。<br><img src=\"/uploads/img/20160714/crf-2.png\" alt=\"线性链条件随机场模型图示\"><br>上式中也同样有$f_i$<strong>特征函数</strong>。之前我对模型中的特征函数一直不太理解。大家可以参考<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954\" target=\"_blank\" rel=\"external\">中文分词入门之字标注法4</a>这篇文章。文章主要介绍借用条件随机场工具“<a href=\"http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip\" target=\"_blank\" rel=\"external\">CRF++: Yet Another CRF toolkit</a>”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客<a href=\"http://www.52nlp.cn/\" target=\"_blank\" rel=\"external\">52nlp</a>。<br>《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。</p>\n<p>CRF模型的<strong>优点</strong>：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。</p>\n<p>CRF模型的<strong>不足</strong>：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。</p>\n<p>更多一些详细的CRF解释可以参考知乎的相关问题<a href=\"https://www.zhihu.com/question/35866596\" target=\"_blank\" rel=\"external\">如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？</a></p>\n<h1 id=\"MEMM与CRF区别\"><a href=\"#MEMM与CRF区别\" class=\"headerlink\" title=\"MEMM与CRF区别\"></a>MEMM与CRF区别</h1><p>上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。</p>\n<p>MEMM的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &amp;=&amp; \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\<br>p(y_i | x_1, \\ldots, x_T, y_{i-1}) &amp;=&amp;<br>\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}<br>{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}<br>\\end{eqnarray*}<br>$$<br>线性链CRF的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y|x) &amp;=&amp; \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}<br>\\\\<br>&amp;=&amp; \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}<br>{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }<br>\\end{eqnarray*}<br>$$<br>不同点：</p>\n<ul>\n<li>首先，CRF是<strong>判别模型</strong>，而MEMM我个人理解是<strong>生成模型</strong>。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个<strong>不大于1</strong>的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。</li>\n<li>MEMM和CRF的<strong>归一化位置</strong>不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。</li>\n<li>MEMM在用<strong>viterbi算法</strong>求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。</li>\n</ul>\n<p>关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/30869789\" target=\"_blank\" rel=\"external\">MEMM和CRF有什么不同？</a></li>\n<li><a href=\"http://blog.sina.com.cn/s/blog_8af106960102v0v1.html\" target=\"_blank\" rel=\"external\">统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 </a></li>\n</ul>\n<h1 id=\"写在最后\"><a href=\"#写在最后\" class=\"headerlink\" title=\"写在最后\"></a>写在最后</h1><p>关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为<a href=\"http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf\" target=\"_blank\" rel=\"external\">An introduction to conditional random fields</a>。</p>\n<p>以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。</p>\n","excerpt":"","more":"<p>最近一直在学习NLP里最基础的几个语言模型：<strong>隐马尔科夫模型</strong>（Hidden Markov Model，HMM）、<strong>最大熵马尔科夫模型</strong>（Maximum Entropy Markov Model，MEMM）和<strong>条件随机场</strong>（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的<strong>《统计学习方法》</strong>和吴军老师的<strong>《数学之美》</strong>。如需这两本书的电子版可以给我留言。</p>\n<p>我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。</p>\n<h1 id=\"HMM\"><a href=\"#HMM\" class=\"headerlink\" title=\"HMM\"></a>HMM</h1><p>下图是《统计学习方法》中的描述：<br><img src=\"/uploads/img/20160714/hmm.png\" alt=\"隐马尔科夫模型\"><br>HMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种<strong>生成模型</strong>（Generative Model），定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。</p>\n<p>如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/20962240\">如何用简单易懂的例子解释隐马尔可夫模型？</a></li>\n<li><a href=\"http://blog.csdn.net/likelet/article/details/7056068\">隐马尔可夫模型（HMM）攻略</a></li>\n</ul>\n<h1 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h1><p>首先贴一下关于最大熵模型的定义：<br><img src=\"/uploads/img/20160714/me.png\" alt=\"最大熵模型\"><br>最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是<strong>特征函数</strong>，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出<strong>最不偏倚</strong>的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。</p>\n<p>最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。</p>\n<p>最大熵模型的<strong>优点</strong>：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。</p>\n<p>最大熵模型的<strong>不足</strong>：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。</p>\n<h1 id=\"MEMM\"><a href=\"#MEMM\" class=\"headerlink\" title=\"MEMM\"></a>MEMM</h1><p>最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种<strong>生成模型</strong>（Generative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。<br><img src=\"/uploads/img/20160714/memm.png\" alt=\"最大熵马尔科夫模型\"><br>可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。</p>\n<p>HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在<strong>标注偏置问题</strong>（Label Bias Problem）。</p>\n<p>关于标注偏置问题，网上最多的是下面这个例子解释：<br><img src=\"/uploads/img/20160714/label-bias-1.png\" alt=\"\"><br>路径1-1-1-1的概率：0.4*0.45*0.5=0.09<br>路径2-2-2-2的概率：0.018<br>路径1-2-1-2的概率：0.06<br>路径1-1-2-2的概率：0.066<br>由此可得最优路径为：1-1-1-1<br><img src=\"/uploads/img/20160714/label-bias-2.png\" alt=\"\"><br>而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。<br>例子的出处参见<a href=\"http://blog.csdn.net/lskyne/article/details/8669301\">标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较</a></p>\n<h1 id=\"CRF\"><a href=\"#CRF\" class=\"headerlink\" title=\"CRF\"></a>CRF</h1><p><img src=\"/uploads/img/20160714/crf-1.png\" alt=\"线性链条件随机场模型\"><br>这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的条件随机场模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。CRF属于<strong>判别模型</strong>（Discrimitive Model）。<br><img src=\"/uploads/img/20160714/crf-2.png\" alt=\"线性链条件随机场模型图示\"><br>上式中也同样有$f_i$<strong>特征函数</strong>。之前我对模型中的特征函数一直不太理解。大家可以参考<a href=\"http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954\">中文分词入门之字标注法4</a>这篇文章。文章主要介绍借用条件随机场工具“<a href=\"http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip\">CRF++: Yet Another CRF toolkit</a>”来完成字标注中文分词的全过程。其中提及了特征模板文件，它的特征提取可能包含了前后多个节点的观测序列。顺便推荐一下这个非常厉害的群体博客<a href=\"http://www.52nlp.cn/\">52nlp</a>。<br>《数学之美》里“徐志摩喜欢林徽因”的例子也可供参考。</p>\n<p>CRF模型的<strong>优点</strong>：首先，CRF具有很强的推理能力，并且能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征，使得模型能够获取的信息非常丰富。其次，CRF的性能更好，CRF对特征的融合能力比较强，识别效果好于MEMM。</p>\n<p>CRF模型的<strong>不足</strong>：使用CRF方法的过程中，特征的选择和优化是影响结果的关键因素，特征选择问题的好与坏，直接决定了系统性能的高低。而且，CRF训练模型的时间较长，且获得的模型很大，在一般的PC机上无法运行。</p>\n<p>更多一些详细的CRF解释可以参考知乎的相关问题<a href=\"https://www.zhihu.com/question/35866596\">如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？</a></p>\n<h1 id=\"MEMM与CRF区别\"><a href=\"#MEMM与CRF区别\" class=\"headerlink\" title=\"MEMM与CRF区别\"></a>MEMM与CRF区别</h1><p>上面的公式都是别人贴图里的，下面的公式是我走心地敲出来的，方便看出两者的差异。</p>\n<p>MEMM的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y_1, \\ldots, y_T | x_1, \\ldots, x_T) &amp;=&amp; \\prod_{i=1}^T p(y_i | x_1, \\ldots, x_T, y_{i-1}) \\\\<br>p(y_i | x_1, \\ldots, x_T, y_{i-1}) &amp;=&amp;<br>\\frac{exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y_i)}<br>{\\sum\\limits_y exp(\\sum\\limits_{k=1}^K w_{k}f_k(x_1, \\ldots, x_T, y_{i-1}, y)}<br>\\end{eqnarray*}<br>$$<br>线性链CRF的公式表示如下：<br>$$<br>\\begin{eqnarray*}<br>p(y|x) &amp;=&amp; \\frac{p(y, x)}{\\sum\\limits_Y p(y, x)}<br>\\\\<br>&amp;=&amp; \\frac{\\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x))}<br>{\\sum\\limits_Y \\prod\\limits_{t=1}^T exp(\\sum\\limits_{k=1}^K w_k f_k(y_t, y_{t-1}, x)) }<br>\\end{eqnarray*}<br>$$<br>不同点：</p>\n<ul>\n<li>首先，CRF是<strong>判别模型</strong>，而MEMM我个人理解是<strong>生成模型</strong>。MEMM是在HMM基础上的优化，它保留了“状态的转移过程中当前状态只与前一状态有关”这一个独立性假设，状态与状态之间的转移仍是遵循一个<strong>不大于1</strong>的、只在同一结点作归一化的局部归一化概率，与HMM的思想相近。</li>\n<li>MEMM和CRF的<strong>归一化位置</strong>不同。从上面的公式可以看出，MEMM是在given前一状态$y_{i-1}$的情况下，对下一个节点所有可能的$y_i$作局部的归一化，利用最大熵模型，从观测序列$x$和前一状态$y_{i-1}$中的特征学习到$y_i$的分布。而CRF是对$Y$中所有可能的状态序列作全局的归一化，假设每个节点有$L$中状态，序列中有$T$个节点，那么所有可能的状态序列数为$L^T$，这导致在模型学习时会较为复杂。</li>\n<li>MEMM在用<strong>viterbi算法</strong>求解最优路径时，每次乘上的是个归一化概率，而CRF乘上的是一个自然指数，没有经过归一化。当遇到某些不公平的情况：某条路径自然指数本身很小，但归一化后变为一个很大的概率比如0.9，而同时即使别的路径自然指数很大，但它们竞争也激烈，归一化后的概率反而不大，这样前一条路径就会被选中，导致了之前提过的标记偏置问题，而CRF可以避免这一问题。</li>\n</ul>\n<p>关于MEMM和CRF两者的区别，推荐可以参考下面的一个知乎问题和一篇博客：</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/30869789\">MEMM和CRF有什么不同？</a></li>\n<li><a href=\"http://blog.sina.com.cn/s/blog_8af106960102v0v1.html\">统计模型之间的比较，HMM，最大熵模型，CRF条件随机场 </a></li>\n</ul>\n<h1 id=\"写在最后\"><a href=\"#写在最后\" class=\"headerlink\" title=\"写在最后\"></a>写在最后</h1><p>关于用做封面的那张图，是对相关模型一个非常抽象、宏观的转换图，感觉非常精髓，出处为<a href=\"http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf\">An introduction to conditional random fields</a>。</p>\n<p>以上均为本小白个人理解，如有任何不当或者错误，欢迎指正。</p>\n"},{"title":"基于Neo4j的知乎关系爬虫","date":"2016-06-29T03:15:56.000Z","comments":1,"photos":["/uploads/img/20160629/cover.jpg"],"_content":"前两天做了一个爬取知乎用户**follow**关系的爬虫。做这个爬虫是受一个知乎专栏的启发[Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系](https://zhuanlan.zhihu.com/p/20546546)，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个**六度分隔理论**的直观展现，也是我在做爬虫时的意外验证。\n# 环境安装\n\n首先交代一下爬虫所用到的数据库和环境：\n## MongoDB\nMongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。\n在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，`pip install pymongo`就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具[Robomongo](https://robomongo.org/)。\n\n## Neo4j\nNeo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。\n关于Neo4j的安装，可以参考这篇博客[Neo4j介绍与使用](http://blog.csdn.net/dyllove98/article/details/8635965)。Win7环境下，官网[下载](https://neo4j.com/download/)可以一键安装Neo4j。\n\nNeo4j使用类似SQL的查询语言**Cypher**，关于Cypher的使用和简单demo，可以参考[Cypher查询语言--Neo4j中的SQL](http://www.uml.org.cn/sjjm/201203063.asp)。当然，为了减少学习Cypher的时间成本，我在python环境中安装了**py2neo**，`pip install py2neo`。\n\npy2neo的handbook见[The Py2neo v3 Handbook](http://py2neo.org/v3/)。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。\n\n## python依赖\n* requests\nrequests是一个非常好用的网络依赖包，API文档见[Requests: HTTP for Humans](http://www.python-requests.org/en/master/)。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。\n* BeautifulSoup\nBeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)。\n\n# 爬虫概要\n## 目的\n我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V---轮子哥（**vczh**）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“**三天三夜**”。\n## 开发思路\n爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是**BFS**的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num**不超过100**的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。\n## 代码相关\n整个爬虫的代码我push到Github上，附上[链接](https://github.com/tripleday/zhihu_link)。\n贴上几个想到的小细节：\n* 这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装**EditThisCookie**插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。\n* 知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的**data-id**，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。\n* BeautifulSoup的官方文档里的一张解析器对比表格\n\n![BeautifulSoup解析器对比](/uploads/img/20160629/bs.png)\n实际使用中，在解析`https://www.zhihu.com/people/hong-ming-da`这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。\n* 代码中有关知乎爬虫的代码，我是在[egrcc/zhihu-python](https://github.com/egrcc/zhihu-python)的基础上改动的，非常感谢原作者的分享。\n* 其他的细节想到后再补充。\n\n# 爬取结果\n\n爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：\n![部分用户关系图](/uploads/img/20160629/whole.png)\n\n在命令行执行Cypher语句：`MATCH (a {_id : '0970f947b898ecc0ec035f9126dd4e08'}), (b {_id : 'bd648b6ef0f14880a522e09ce2752465'}), p = allShortestPaths( (a)-[*..200]->(b) ) RETURN p`可以得到轮子哥到我的最短路径：\n![最短路径图](/uploads/img/20160629/shortestpath.png)\n\n可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 **6** 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。\n","source":"_posts/zhihu-link.md","raw":"title: 基于Neo4j的知乎关系爬虫\ndate: 2016-06-29 11:15:56\ncomments: true\ntags: \n - zhihu\n - Neo4j\n - python\ncategories: Crawler\nphotos: \n - /uploads/img/20160629/cover.jpg\n---\n前两天做了一个爬取知乎用户**follow**关系的爬虫。做这个爬虫是受一个知乎专栏的启发[Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系](https://zhuanlan.zhihu.com/p/20546546)，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个**六度分隔理论**的直观展现，也是我在做爬虫时的意外验证。\n# 环境安装\n\n首先交代一下爬虫所用到的数据库和环境：\n## MongoDB\nMongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。\n在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，`pip install pymongo`就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具[Robomongo](https://robomongo.org/)。\n\n## Neo4j\nNeo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。\n关于Neo4j的安装，可以参考这篇博客[Neo4j介绍与使用](http://blog.csdn.net/dyllove98/article/details/8635965)。Win7环境下，官网[下载](https://neo4j.com/download/)可以一键安装Neo4j。\n\nNeo4j使用类似SQL的查询语言**Cypher**，关于Cypher的使用和简单demo，可以参考[Cypher查询语言--Neo4j中的SQL](http://www.uml.org.cn/sjjm/201203063.asp)。当然，为了减少学习Cypher的时间成本，我在python环境中安装了**py2neo**，`pip install py2neo`。\n\npy2neo的handbook见[The Py2neo v3 Handbook](http://py2neo.org/v3/)。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。\n\n## python依赖\n* requests\nrequests是一个非常好用的网络依赖包，API文档见[Requests: HTTP for Humans](http://www.python-requests.org/en/master/)。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。\n* BeautifulSoup\nBeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)。\n\n# 爬虫概要\n## 目的\n我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V---轮子哥（**vczh**）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“**三天三夜**”。\n## 开发思路\n爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是**BFS**的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num**不超过100**的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。\n## 代码相关\n整个爬虫的代码我push到Github上，附上[链接](https://github.com/tripleday/zhihu_link)。\n贴上几个想到的小细节：\n* 这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装**EditThisCookie**插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。\n* 知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的**data-id**，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。\n* BeautifulSoup的官方文档里的一张解析器对比表格\n\n![BeautifulSoup解析器对比](/uploads/img/20160629/bs.png)\n实际使用中，在解析`https://www.zhihu.com/people/hong-ming-da`这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。\n* 代码中有关知乎爬虫的代码，我是在[egrcc/zhihu-python](https://github.com/egrcc/zhihu-python)的基础上改动的，非常感谢原作者的分享。\n* 其他的细节想到后再补充。\n\n# 爬取结果\n\n爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：\n![部分用户关系图](/uploads/img/20160629/whole.png)\n\n在命令行执行Cypher语句：`MATCH (a {_id : '0970f947b898ecc0ec035f9126dd4e08'}), (b {_id : 'bd648b6ef0f14880a522e09ce2752465'}), p = allShortestPaths( (a)-[*..200]->(b) ) RETURN p`可以得到轮子哥到我的最短路径：\n![最短路径图](/uploads/img/20160629/shortestpath.png)\n\n可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 **6** 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。\n","slug":"zhihu-link","published":1,"updated":"2016-07-20T13:49:47.806Z","layout":"post","link":"","_id":"cjh96tx5j001mhoc5d2ehdak8","content":"<p>前两天做了一个爬取知乎用户<strong>follow</strong>关系的爬虫。做这个爬虫是受一个知乎专栏的启发<a href=\"https://zhuanlan.zhihu.com/p/20546546\" target=\"_blank\" rel=\"external\">Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系</a>，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个<strong>六度分隔理论</strong>的直观展现，也是我在做爬虫时的意外验证。</p>\n<h1 id=\"环境安装\"><a href=\"#环境安装\" class=\"headerlink\" title=\"环境安装\"></a>环境安装</h1><p>首先交代一下爬虫所用到的数据库和环境：</p>\n<h2 id=\"MongoDB\"><a href=\"#MongoDB\" class=\"headerlink\" title=\"MongoDB\"></a>MongoDB</h2><p>MongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。<br>在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，<code>pip install pymongo</code>就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具<a href=\"https://robomongo.org/\" target=\"_blank\" rel=\"external\">Robomongo</a>。</p>\n<h2 id=\"Neo4j\"><a href=\"#Neo4j\" class=\"headerlink\" title=\"Neo4j\"></a>Neo4j</h2><p>Neo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。<br>关于Neo4j的安装，可以参考这篇博客<a href=\"http://blog.csdn.net/dyllove98/article/details/8635965\" target=\"_blank\" rel=\"external\">Neo4j介绍与使用</a>。Win7环境下，官网<a href=\"https://neo4j.com/download/\" target=\"_blank\" rel=\"external\">下载</a>可以一键安装Neo4j。</p>\n<p>Neo4j使用类似SQL的查询语言<strong>Cypher</strong>，关于Cypher的使用和简单demo，可以参考<a href=\"http://www.uml.org.cn/sjjm/201203063.asp\" target=\"_blank\" rel=\"external\">Cypher查询语言–Neo4j中的SQL</a>。当然，为了减少学习Cypher的时间成本，我在python环境中安装了<strong>py2neo</strong>，<code>pip install py2neo</code>。</p>\n<p>py2neo的handbook见<a href=\"http://py2neo.org/v3/\" target=\"_blank\" rel=\"external\">The Py2neo v3 Handbook</a>。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。</p>\n<h2 id=\"python依赖\"><a href=\"#python依赖\" class=\"headerlink\" title=\"python依赖\"></a>python依赖</h2><ul>\n<li>requests<br>requests是一个非常好用的网络依赖包，API文档见<a href=\"http://www.python-requests.org/en/master/\" target=\"_blank\" rel=\"external\">Requests: HTTP for Humans</a>。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。</li>\n<li>BeautifulSoup<br>BeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见<a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\" target=\"_blank\" rel=\"external\">Beautiful Soup 4.2.0 文档</a>。</li>\n</ul>\n<h1 id=\"爬虫概要\"><a href=\"#爬虫概要\" class=\"headerlink\" title=\"爬虫概要\"></a>爬虫概要</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V—轮子哥（<strong>vczh</strong>）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“<strong>三天三夜</strong>”。</p>\n<h2 id=\"开发思路\"><a href=\"#开发思路\" class=\"headerlink\" title=\"开发思路\"></a>开发思路</h2><p>爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是<strong>BFS</strong>的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num<strong>不超过100</strong>的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。</p>\n<h2 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h2><p>整个爬虫的代码我push到Github上，附上<a href=\"https://github.com/tripleday/zhihu_link\" target=\"_blank\" rel=\"external\">链接</a>。<br>贴上几个想到的小细节：</p>\n<ul>\n<li>这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装<strong>EditThisCookie</strong>插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。</li>\n<li>知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的<strong>data-id</strong>，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。</li>\n<li>BeautifulSoup的官方文档里的一张解析器对比表格</li>\n</ul>\n<p><img src=\"/uploads/img/20160629/bs.png\" alt=\"BeautifulSoup解析器对比\"><br>实际使用中，在解析<code>https://www.zhihu.com/people/hong-ming-da</code>这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。</p>\n<ul>\n<li>代码中有关知乎爬虫的代码，我是在<a href=\"https://github.com/egrcc/zhihu-python\" target=\"_blank\" rel=\"external\">egrcc/zhihu-python</a>的基础上改动的，非常感谢原作者的分享。</li>\n<li>其他的细节想到后再补充。</li>\n</ul>\n<h1 id=\"爬取结果\"><a href=\"#爬取结果\" class=\"headerlink\" title=\"爬取结果\"></a>爬取结果</h1><p>爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：<br><img src=\"/uploads/img/20160629/whole.png\" alt=\"部分用户关系图\"></p>\n<p>在命令行执行Cypher语句：<code>MATCH (a {_id : &#39;0970f947b898ecc0ec035f9126dd4e08&#39;}), (b {_id : &#39;bd648b6ef0f14880a522e09ce2752465&#39;}), p = allShortestPaths( (a)-[*..200]-&gt;(b) ) RETURN p</code>可以得到轮子哥到我的最短路径：<br><img src=\"/uploads/img/20160629/shortestpath.png\" alt=\"最短路径图\"></p>\n<p>可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 <strong>6</strong> 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。</p>\n","excerpt":"","more":"<p>前两天做了一个爬取知乎用户<strong>follow</strong>关系的爬虫。做这个爬虫是受一个知乎专栏的启发<a href=\"https://zhuanlan.zhihu.com/p/20546546\">Web Crawler with Python - 09.怎样通过爬虫找出我和轮子哥、四万姐之间的最短关系</a>，我有部分代码参考了xlzd。由于当时也想了解一下NoSQL里Graph Database，于是花了几天时间做了一个简单的爬虫，感觉收获不少。封面图片可以理解成是一个<strong>六度分隔理论</strong>的直观展现，也是我在做爬虫时的意外验证。</p>\n<h1 id=\"环境安装\"><a href=\"#环境安装\" class=\"headerlink\" title=\"环境安装\"></a>环境安装</h1><p>首先交代一下爬虫所用到的数据库和环境：</p>\n<h2 id=\"MongoDB\"><a href=\"#MongoDB\" class=\"headerlink\" title=\"MongoDB\"></a>MongoDB</h2><p>MongoDB一种基于分布式文件存储的数据库，属于NoSQL里的文档型数据库。它的性能较高，面向集合存储，爬虫所抓取的用户信息都存储在其中。<br>在python里使用MongoDB，只需要在本机下载安装MongoDB服务，在python的环境里安装pymongo依赖，<code>pip install pymongo</code>就可以了。如果嫌MongoDB的命令行操作不方便，可以装一个MongoDB的可视化工具<a href=\"https://robomongo.org/\">Robomongo</a>。</p>\n<h2 id=\"Neo4j\"><a href=\"#Neo4j\" class=\"headerlink\" title=\"Neo4j\"></a>Neo4j</h2><p>Neo4j是一个高性能的,NoSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。<br>关于Neo4j的安装，可以参考这篇博客<a href=\"http://blog.csdn.net/dyllove98/article/details/8635965\">Neo4j介绍与使用</a>。Win7环境下，官网<a href=\"https://neo4j.com/download/\">下载</a>可以一键安装Neo4j。</p>\n<p>Neo4j使用类似SQL的查询语言<strong>Cypher</strong>，关于Cypher的使用和简单demo，可以参考<a href=\"http://www.uml.org.cn/sjjm/201203063.asp\">Cypher查询语言–Neo4j中的SQL</a>。当然，为了减少学习Cypher的时间成本，我在python环境中安装了<strong>py2neo</strong>，<code>pip install py2neo</code>。</p>\n<p>py2neo的handbook见<a href=\"http://py2neo.org/v3/\">The Py2neo v3 Handbook</a>。我对py2neo依赖库的理解：py2neo是一个Neo4j的客户端，其中对Neo4j的操作进行了封装。调用py2neo的一个函数，它会自动转化为Cypher语言并以HTTP API向Neo4j服务端口提交一个事务。当然它也支持直接提交Cypher语句到Neo4j执行，有些复杂的数据操作比如寻找两点之间最短路径，py2neo没有提供直接的函数调用，需要我们自己编写Cypher。</p>\n<h2 id=\"python依赖\"><a href=\"#python依赖\" class=\"headerlink\" title=\"python依赖\"></a>python依赖</h2><ul>\n<li>requests<br>requests是一个非常好用的网络依赖包，API文档见<a href=\"http://www.python-requests.org/en/master/\">Requests: HTTP for Humans</a>。文档网站的名字“HTTP for Humans”，算是程序员的一种幽默吧。</li>\n<li>BeautifulSoup<br>BeautifulSoup依赖库是一个非常实用的HTML解析器，不需要程序员再焦头烂额地写RegEx。虽然开发友好了，但解析时有时会出一些不可思议的bug。API文档见<a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\">Beautiful Soup 4.2.0 文档</a>。</li>\n</ul>\n<h1 id=\"爬虫概要\"><a href=\"#爬虫概要\" class=\"headerlink\" title=\"爬虫概要\"></a>爬虫概要</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>我爬虫的目的非常简单，和开头的那篇专栏一样：知乎大V—轮子哥（<strong>vczh</strong>）需要通过多少人才能认识并关注我？这里的认识是指单方面的知道，即成为我的follower（不需要为followee，虽然这是肯定的），知道这世界原来还有个知乎用户“<strong>三天三夜</strong>”。</p>\n<h2 id=\"开发思路\"><a href=\"#开发思路\" class=\"headerlink\" title=\"开发思路\"></a>开发思路</h2><p>爬虫从我自己的知乎出发，读取我的follower列表，对我的每个follower重复搜索操作，直到搜索到的follower list里有vczh。这个遍历是<strong>BFS</strong>的。当然，为了防止在广度优先搜索时，层与层之间节点数量扩张过快，我限制只搜索follower num<strong>不超过100</strong>的不活跃的小用户，当然我提前调查了轮子哥也有follow一些这种小用户。除了为了防止扩张过快导致的存储空间过大，这样做也给验证六度分隔理论提了更为苛刻的条件，毕竟轮子哥通过其他大V能follow到我的概率是远大于通过小用户的。</p>\n<h2 id=\"代码相关\"><a href=\"#代码相关\" class=\"headerlink\" title=\"代码相关\"></a>代码相关</h2><p>整个爬虫的代码我push到Github上，附上<a href=\"https://github.com/tripleday/zhihu_link\">链接</a>。<br>贴上几个想到的小细节：</p>\n<ul>\n<li>这个爬虫需要自己的知乎cookie才能爬取。建议使用chrome，安装<strong>EditThisCookie</strong>插件，将知乎的cookie复制粘贴到zhihu_cookie.json文件。</li>\n<li>知乎用户的唯一性不是靠用户名，而是html里内嵌隐藏的<strong>data-id</strong>，在ajax获取数据是发送的表单数据里也需要这个值。所以Neo4j中使用此值可以唯一标识用户。</li>\n<li>BeautifulSoup的官方文档里的一张解析器对比表格</li>\n</ul>\n<p><img src=\"/uploads/img/20160629/bs.png\" alt=\"BeautifulSoup解析器对比\"><br>实际使用中，在解析<code>https://www.zhihu.com/people/hong-ming-da</code>这条链接时，lxml解析一直都会出错，换成html.parser后解析成功。所以html.parser虽然解析速度慢，但容错性更好一点。</p>\n<ul>\n<li>代码中有关知乎爬虫的代码，我是在<a href=\"https://github.com/egrcc/zhihu-python\">egrcc/zhihu-python</a>的基础上改动的，非常感谢原作者的分享。</li>\n<li>其他的细节想到后再补充。</li>\n</ul>\n<h1 id=\"爬取结果\"><a href=\"#爬取结果\" class=\"headerlink\" title=\"爬取结果\"></a>爬取结果</h1><p>爬虫程序在爬了23928个用户才停下来，即找到了轮子哥。这是爬完的部分用户图：<br><img src=\"/uploads/img/20160629/whole.png\" alt=\"部分用户关系图\"></p>\n<p>在命令行执行Cypher语句：<code>MATCH (a {_id : &#39;0970f947b898ecc0ec035f9126dd4e08&#39;}), (b {_id : &#39;bd648b6ef0f14880a522e09ce2752465&#39;}), p = allShortestPaths( (a)-[*..200]-&gt;(b) ) RETURN p</code>可以得到轮子哥到我的最短路径：<br><img src=\"/uploads/img/20160629/shortestpath.png\" alt=\"最短路径图\"></p>\n<p>可以发现：轮子哥到我，中间正好经过了6个人。这条路的生成条件是较为严格的。不仅是因为我只选择的小用户进行爬取，而且要知道我的follower目前是只有一个的，轮子哥要连接到我只能通过他。虽然实验得到的 <strong>6</strong> 可能和六度分隔理论恰巧吻合，但鉴于路径选择的苛刻条件，六度的6也许并不只是一种猜想。</p>\n"},{"title":"CentOS下使用Graphite监测scrapy","date":"2016-10-06T13:43:19.000Z","comments":1,"_content":"受github上一个前人的[爬虫项目](https://github.com/gnemoug/distribute_crawler)的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。\n\n# Graphite简介\nGraphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率，RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。\n\n简单来说，Graphite主要做两件事情：\n- 实时监控第三方工具传来的数据\n- 根据数据绘制图形\n\nGraphite包含3个组件，carbon，whisper，graphite webapp其中：\n- carbon\t- 用于监控数据的 Twisted 守护进程\n- whisper\t- 用于存放和操作数据的库\n- graphite webapp\t- 用于绘制图形的Django webapp\n\n关于Graphite的详细官方文档可以参考[Graphite Documentation](http://graphite.readthedocs.io/en/latest/)。\n\n# Graphite安装\nGraphite的安装，我更多地参考了这一篇博客[用graphite diamond做监控](https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4)。\n\n首先，我的安装环境是CentOS 6.6，Python2.7.10。\n\n在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：\n```sh\npip install whisper\npip install carbon\npip install graphite-web\n```\n这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。\n\n启动carbon，carbon会在默认的2003端口接收数据。\n```sh\npython /opt/graphite/bin/carbon-cache.py start\n```\n\n启动django，即整个Graphite的web应用。\n```sh\npython /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222\n```\n其中的12222号端口可以自己任意修改。\n这样浏览器打开`http://127.0.0.1:12222`就可以看到Graphite的界面。 \n\n**然而启动django这一步我在运行的时候遇到了各种错误。**\n- 在我`pip install`的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。\n- 然而我`pip install django==1.9`之后，还是遇到了\n  - no such table: auth_user\n  - no such table: account_profile\n  - Unknown command: 'syncdb'\n\n  这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。\n- 这些错误都是因为django版本不兼容导致的，在我`pip install django==1.8`之后，整个世界就清静了。\n\n如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！\nGraphite界面会提示`import cairo`出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，`pip install cairo`之后你终于能松一口气欣赏一下Graphite了。\n\n页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在`/opt/graphite/storage/whisper`。\n\n另外，Graphite有个**时区设置**的问题，如果不更改，你的时间显示都是GMT时间。只需将`/opt/graphite/webapp/graphite/local_settings.py`文件里的TIME_ZONE配置改成如下：\n```python\n# Set your local timezone (Django's default is America/Chicago)\n# If your graphs appear to be offset by a couple hours then this probably\n# needs to be explicitly set to your local timezone.\nTIME_ZONE = 'Asia/Shanghai'\n```\n\n如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：\n- [Graphite](https://github.com/springside/springside4/wiki/Graphite)\n- [Graphite监控新手入门 ](http://m.linuxeden.com/wap.php?action=article&id=159746)\n\n# Graphite与scrapy的结合\n结合的方法详见原作者项目中[graphite.py](https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py)文件中的注释，我总结为一下几点：\n- 把`/opt/graphite/webapp/content/js/composer_widgets.js`文件中`toggleAutoRefresh`函数里的interval变量从60改为1。\n- 在配置文件`storage-aggregation.conf`里添加\n```sh\n[scrapy_min]\npattern = ^scrapy\\..*_min$\nxFilesFactor = 0.1\naggregationMethod = min\n[scrapy_max]\npattern = ^scrapy\\..*_max$\nxFilesFactor = 0.1\naggregationMethod = max\n[scrapy_sum]\npattern = ^scrapy\\..*_count$\nxFilesFactor = 0.1\naggregationMethod = sum\n```\n- 在爬虫的配置文件`setting.py`里添加\n```python\nSTATS_CLASS = 'scrapygraphite.GraphiteStatsCollector'\nGRAPHITE_HOST = '127.0.0.1'\nGRAPHITE_PORT = 2003\n```\n- 后两点是我自己的修改。\nscrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在scrapy源码的`scrapy/extensions/logstats.py`文件中添加了两个状态变量的发送。\n```python\ndef spider_opened(self, spider):\n    self.pagesprev = 0\n    self.itemsprev = 0\n\n    self.task = task.LoopingCall(self.log, spider)\n    self.task.start(self.interval)\n\n    self.stats.set_value('pages_min', 0, spider=spider)\n    self.stats.set_value('items_min', 0, spider=spider)\n\ndef log(self, spider):\n    items = self.stats.get_value('item_scraped_count', 0)\n    pages = self.stats.get_value('response_received_count', 0)\n    irate = (items - self.itemsprev) * self.multiplier\n    prate = (pages - self.pagesprev) * self.multiplier\n    self.pagesprev, self.itemsprev = pages, items\n\n    msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n           \"scraped %(items)d items (at %(itemrate)d items/min)\")\n    log_args = {'pages': pages, 'pagerate': prate,\n                'items': items, 'itemrate': irate}\n    logger.info(msg, log_args, extra={'spider': spider})\n    self.stats.set_value('pages_min', prate, spider=spider)\n    self.stats.set_value('items_min', irate, spider=spider)\n    states = self.stats.get_stats()\n    for key in states:\n        self.stats._set_value(key, states[key], spider=spider) \n```\n  这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置`LOGSTATS_INTERVAL`的值。因为scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。\n- 在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。\n```\n[default_1min_for_1day]\npattern = .*\nretentions = 60s:1d\n```\n  但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。\n\n# 最后\n贴一个Graphite的效果图：\n![Graphite](/uploads/img/20161006/graphite.png)\n如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤[graph-index](https://github.com/douban/graph-index)。","source":"_posts/graphite.md","raw":"title: CentOS下使用Graphite监测scrapy\ndate: 2016-10-06 21:43:19\ncomments: true\ntags: \n - Graphite\n - scrapy\n - CentOS\n - python\ncategories: Crawler\n---\n受github上一个前人的[爬虫项目](https://github.com/gnemoug/distribute_crawler)的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。\n\n# Graphite简介\nGraphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率，RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。\n\n简单来说，Graphite主要做两件事情：\n- 实时监控第三方工具传来的数据\n- 根据数据绘制图形\n\nGraphite包含3个组件，carbon，whisper，graphite webapp其中：\n- carbon\t- 用于监控数据的 Twisted 守护进程\n- whisper\t- 用于存放和操作数据的库\n- graphite webapp\t- 用于绘制图形的Django webapp\n\n关于Graphite的详细官方文档可以参考[Graphite Documentation](http://graphite.readthedocs.io/en/latest/)。\n\n# Graphite安装\nGraphite的安装，我更多地参考了这一篇博客[用graphite diamond做监控](https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4)。\n\n首先，我的安装环境是CentOS 6.6，Python2.7.10。\n\n在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：\n```sh\npip install whisper\npip install carbon\npip install graphite-web\n```\n这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。\n\n启动carbon，carbon会在默认的2003端口接收数据。\n```sh\npython /opt/graphite/bin/carbon-cache.py start\n```\n\n启动django，即整个Graphite的web应用。\n```sh\npython /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222\n```\n其中的12222号端口可以自己任意修改。\n这样浏览器打开`http://127.0.0.1:12222`就可以看到Graphite的界面。 \n\n**然而启动django这一步我在运行的时候遇到了各种错误。**\n- 在我`pip install`的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。\n- 然而我`pip install django==1.9`之后，还是遇到了\n  - no such table: auth_user\n  - no such table: account_profile\n  - Unknown command: 'syncdb'\n\n  这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。\n- 这些错误都是因为django版本不兼容导致的，在我`pip install django==1.8`之后，整个世界就清静了。\n\n如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！\nGraphite界面会提示`import cairo`出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，`pip install cairo`之后你终于能松一口气欣赏一下Graphite了。\n\n页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在`/opt/graphite/storage/whisper`。\n\n另外，Graphite有个**时区设置**的问题，如果不更改，你的时间显示都是GMT时间。只需将`/opt/graphite/webapp/graphite/local_settings.py`文件里的TIME_ZONE配置改成如下：\n```python\n# Set your local timezone (Django's default is America/Chicago)\n# If your graphs appear to be offset by a couple hours then this probably\n# needs to be explicitly set to your local timezone.\nTIME_ZONE = 'Asia/Shanghai'\n```\n\n如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：\n- [Graphite](https://github.com/springside/springside4/wiki/Graphite)\n- [Graphite监控新手入门 ](http://m.linuxeden.com/wap.php?action=article&id=159746)\n\n# Graphite与scrapy的结合\n结合的方法详见原作者项目中[graphite.py](https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py)文件中的注释，我总结为一下几点：\n- 把`/opt/graphite/webapp/content/js/composer_widgets.js`文件中`toggleAutoRefresh`函数里的interval变量从60改为1。\n- 在配置文件`storage-aggregation.conf`里添加\n```sh\n[scrapy_min]\npattern = ^scrapy\\..*_min$\nxFilesFactor = 0.1\naggregationMethod = min\n[scrapy_max]\npattern = ^scrapy\\..*_max$\nxFilesFactor = 0.1\naggregationMethod = max\n[scrapy_sum]\npattern = ^scrapy\\..*_count$\nxFilesFactor = 0.1\naggregationMethod = sum\n```\n- 在爬虫的配置文件`setting.py`里添加\n```python\nSTATS_CLASS = 'scrapygraphite.GraphiteStatsCollector'\nGRAPHITE_HOST = '127.0.0.1'\nGRAPHITE_PORT = 2003\n```\n- 后两点是我自己的修改。\nscrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在scrapy源码的`scrapy/extensions/logstats.py`文件中添加了两个状态变量的发送。\n```python\ndef spider_opened(self, spider):\n    self.pagesprev = 0\n    self.itemsprev = 0\n\n    self.task = task.LoopingCall(self.log, spider)\n    self.task.start(self.interval)\n\n    self.stats.set_value('pages_min', 0, spider=spider)\n    self.stats.set_value('items_min', 0, spider=spider)\n\ndef log(self, spider):\n    items = self.stats.get_value('item_scraped_count', 0)\n    pages = self.stats.get_value('response_received_count', 0)\n    irate = (items - self.itemsprev) * self.multiplier\n    prate = (pages - self.pagesprev) * self.multiplier\n    self.pagesprev, self.itemsprev = pages, items\n\n    msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n           \"scraped %(items)d items (at %(itemrate)d items/min)\")\n    log_args = {'pages': pages, 'pagerate': prate,\n                'items': items, 'itemrate': irate}\n    logger.info(msg, log_args, extra={'spider': spider})\n    self.stats.set_value('pages_min', prate, spider=spider)\n    self.stats.set_value('items_min', irate, spider=spider)\n    states = self.stats.get_stats()\n    for key in states:\n        self.stats._set_value(key, states[key], spider=spider) \n```\n  这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置`LOGSTATS_INTERVAL`的值。因为scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。\n- 在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。\n```\n[default_1min_for_1day]\npattern = .*\nretentions = 60s:1d\n```\n  但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。\n\n# 最后\n贴一个Graphite的效果图：\n![Graphite](/uploads/img/20161006/graphite.png)\n如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤[graph-index](https://github.com/douban/graph-index)。","slug":"graphite","published":1,"updated":"2018-05-12T09:20:26.503Z","layout":"post","photos":[],"link":"","_id":"cjh96tx5l001nhoc5mpylg4v2","content":"<p>受github上一个前人的<a href=\"https://github.com/gnemoug/distribute_crawler\" target=\"_blank\" rel=\"external\">爬虫项目</a>的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。</p>\n<h1 id=\"Graphite简介\"><a href=\"#Graphite简介\" class=\"headerlink\" title=\"Graphite简介\"></a>Graphite简介</h1><p>Graphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率，RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。</p>\n<p>简单来说，Graphite主要做两件事情：</p>\n<ul>\n<li>实时监控第三方工具传来的数据</li>\n<li>根据数据绘制图形</li>\n</ul>\n<p>Graphite包含3个组件，carbon，whisper，graphite webapp其中：</p>\n<ul>\n<li>carbon    - 用于监控数据的 Twisted 守护进程</li>\n<li>whisper    - 用于存放和操作数据的库</li>\n<li>graphite webapp    - 用于绘制图形的Django webapp</li>\n</ul>\n<p>关于Graphite的详细官方文档可以参考<a href=\"http://graphite.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"external\">Graphite Documentation</a>。</p>\n<h1 id=\"Graphite安装\"><a href=\"#Graphite安装\" class=\"headerlink\" title=\"Graphite安装\"></a>Graphite安装</h1><p>Graphite的安装，我更多地参考了这一篇博客<a href=\"https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4\" target=\"_blank\" rel=\"external\">用graphite diamond做监控</a>。</p>\n<p>首先，我的安装环境是CentOS 6.6，Python2.7.10。</p>\n<p>在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install whisper</span><br><span class=\"line\">pip install carbon</span><br><span class=\"line\">pip install graphite-web</span><br></pre></td></tr></table></figure></p>\n<p>这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。</p>\n<p>启动carbon，carbon会在默认的2003端口接收数据。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/bin/carbon-cache.py start</span><br></pre></td></tr></table></figure></p>\n<p>启动django，即整个Graphite的web应用。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222</span><br></pre></td></tr></table></figure></p>\n<p>其中的12222号端口可以自己任意修改。<br>这样浏览器打开<code>http://127.0.0.1:12222</code>就可以看到Graphite的界面。 </p>\n<p><strong>然而启动django这一步我在运行的时候遇到了各种错误。</strong></p>\n<ul>\n<li>在我<code>pip install</code>的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。</li>\n<li><p>然而我<code>pip install django==1.9</code>之后，还是遇到了</p>\n<ul>\n<li>no such table: auth_user</li>\n<li>no such table: account_profile</li>\n<li>Unknown command: ‘syncdb’</li>\n</ul>\n<p>这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。</p>\n</li>\n<li>这些错误都是因为django版本不兼容导致的，在我<code>pip install django==1.8</code>之后，整个世界就清静了。</li>\n</ul>\n<p>如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！<br>Graphite界面会提示<code>import cairo</code>出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，<code>pip install cairo</code>之后你终于能松一口气欣赏一下Graphite了。</p>\n<p>页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在<code>/opt/graphite/storage/whisper</code>。</p>\n<p>另外，Graphite有个<strong>时区设置</strong>的问题，如果不更改，你的时间显示都是GMT时间。只需将<code>/opt/graphite/webapp/graphite/local_settings.py</code>文件里的TIME_ZONE配置改成如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Set your local timezone (Django's default is America/Chicago)</span></span><br><span class=\"line\"><span class=\"comment\"># If your graphs appear to be offset by a couple hours then this probably</span></span><br><span class=\"line\"><span class=\"comment\"># needs to be explicitly set to your local timezone.</span></span><br><span class=\"line\">TIME_ZONE = <span class=\"string\">'Asia/Shanghai'</span></span><br></pre></td></tr></table></figure></p>\n<p>如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：</p>\n<ul>\n<li><a href=\"https://github.com/springside/springside4/wiki/Graphite\" target=\"_blank\" rel=\"external\">Graphite</a></li>\n<li><a href=\"http://m.linuxeden.com/wap.php?action=article&amp;id=159746\" target=\"_blank\" rel=\"external\">Graphite监控新手入门 </a></li>\n</ul>\n<h1 id=\"Graphite与scrapy的结合\"><a href=\"#Graphite与scrapy的结合\" class=\"headerlink\" title=\"Graphite与scrapy的结合\"></a>Graphite与scrapy的结合</h1><p>结合的方法详见原作者项目中<a href=\"https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py\" target=\"_blank\" rel=\"external\">graphite.py</a>文件中的注释，我总结为一下几点：</p>\n<ul>\n<li>把<code>/opt/graphite/webapp/content/js/composer_widgets.js</code>文件中<code>toggleAutoRefresh</code>函数里的interval变量从60改为1。</li>\n<li><p>在配置文件<code>storage-aggregation.conf</code>里添加</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[scrapy_min]</span><br><span class=\"line\">pattern = ^scrapy\\..*_min$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = min</span><br><span class=\"line\">[scrapy_max]</span><br><span class=\"line\">pattern = ^scrapy\\..*_max$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = max</span><br><span class=\"line\">[scrapy_sum]</span><br><span class=\"line\">pattern = ^scrapy\\..*_count$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = sum</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在爬虫的配置文件<code>setting.py</code>里添加</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">STATS_CLASS = <span class=\"string\">'scrapygraphite.GraphiteStatsCollector'</span></span><br><span class=\"line\">GRAPHITE_HOST = <span class=\"string\">'127.0.0.1'</span></span><br><span class=\"line\">GRAPHITE_PORT = <span class=\"number\">2003</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>后两点是我自己的修改。<br>scrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在scrapy源码的<code>scrapy/extensions/logstats.py</code>文件中添加了两个状态变量的发送。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spider_opened</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    self.pagesprev = <span class=\"number\">0</span></span><br><span class=\"line\">    self.itemsprev = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    self.task = task.LoopingCall(self.log, spider)</span><br><span class=\"line\">    self.task.start(self.interval)</span><br><span class=\"line\"></span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">log</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    items = self.stats.get_value(<span class=\"string\">'item_scraped_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    pages = self.stats.get_value(<span class=\"string\">'response_received_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    irate = (items - self.itemsprev) * self.multiplier</span><br><span class=\"line\">    prate = (pages - self.pagesprev) * self.multiplier</span><br><span class=\"line\">    self.pagesprev, self.itemsprev = pages, items</span><br><span class=\"line\"></span><br><span class=\"line\">    msg = (<span class=\"string\">\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"</span></span><br><span class=\"line\">           <span class=\"string\">\"scraped %(items)d items (at %(itemrate)d items/min)\"</span>)</span><br><span class=\"line\">    log_args = &#123;<span class=\"string\">'pages'</span>: pages, <span class=\"string\">'pagerate'</span>: prate,</span><br><span class=\"line\">                <span class=\"string\">'items'</span>: items, <span class=\"string\">'itemrate'</span>: irate&#125;</span><br><span class=\"line\">    logger.info(msg, log_args, extra=&#123;<span class=\"string\">'spider'</span>: spider&#125;)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, prate, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, irate, spider=spider)</span><br><span class=\"line\">    states = self.stats.get_stats()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> states:</span><br><span class=\"line\">        self.stats._set_value(key, states[key], spider=spider)</span><br></pre></td></tr></table></figure>\n<p>这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置<code>LOGSTATS_INTERVAL</code>的值。因为scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。</p>\n</li>\n<li><p>在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[default_1min_for_1day]</span><br><span class=\"line\">pattern = .*</span><br><span class=\"line\">retentions = 60s:1d</span><br></pre></td></tr></table></figure>\n<p>但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。</p>\n</li>\n</ul>\n<h1 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h1><p>贴一个Graphite的效果图：<br><img src=\"/uploads/img/20161006/graphite.png\" alt=\"Graphite\"><br>如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤<a href=\"https://github.com/douban/graph-index\" target=\"_blank\" rel=\"external\">graph-index</a>。</p>\n","excerpt":"","more":"<p>受github上一个前人的<a href=\"https://github.com/gnemoug/distribute_crawler\">爬虫项目</a>的指导，我学习了里面使用Graphite监控的部分，在这篇博客里把我在安装和使用中遇到的问题全部记录下来。</p>\n<h1 id=\"Graphite简介\"><a href=\"#Graphite简介\" class=\"headerlink\" title=\"Graphite简介\"></a>Graphite简介</h1><p>Graphite是一个Python编写的企业级开源监控工具，采用django框架，用来收集服务器所有的即时状态，用户请求信息，Memcached命中率，RabbitMQ消息服务器的状态，操作系统的负载状态。Graphite服务器大约每分钟需要有4800次的跟新操作，它采用简单的文本协议和绘图功能，可以方便的使用在任何操作系统上。Graphite自己本身并不收集具体的数据，这些数据收集的具体工作通常由第三方工具或插件完成（如 Ganglia, collectd, statsd, Collectl 等)。</p>\n<p>简单来说，Graphite主要做两件事情：</p>\n<ul>\n<li>实时监控第三方工具传来的数据</li>\n<li>根据数据绘制图形</li>\n</ul>\n<p>Graphite包含3个组件，carbon，whisper，graphite webapp其中：</p>\n<ul>\n<li>carbon    - 用于监控数据的 Twisted 守护进程</li>\n<li>whisper    - 用于存放和操作数据的库</li>\n<li>graphite webapp    - 用于绘制图形的Django webapp</li>\n</ul>\n<p>关于Graphite的详细官方文档可以参考<a href=\"http://graphite.readthedocs.io/en/latest/\">Graphite Documentation</a>。</p>\n<h1 id=\"Graphite安装\"><a href=\"#Graphite安装\" class=\"headerlink\" title=\"Graphite安装\"></a>Graphite安装</h1><p>Graphite的安装，我更多地参考了这一篇博客<a href=\"https://my.oschina.net/duoduo3369/blog/338142#OSC_h2_4\">用graphite diamond做监控</a>。</p>\n<p>首先，我的安装环境是CentOS 6.6，Python2.7.10。</p>\n<p>在python等开发环境都安装OK之后，我们使用pip安装Graphite的三个组件：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install whisper</span><br><span class=\"line\">pip install carbon</span><br><span class=\"line\">pip install graphite-web</span><br></pre></td></tr></table></figure></p>\n<p>这样的方法会将它们安装在默认路径/opt/graphite下。安装完成后，你会发现/opt/graphite下多了一堆东西，将/opt/graphite/conf下的*.example,拷贝到去掉example即可。</p>\n<p>启动carbon，carbon会在默认的2003端口接收数据。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/bin/carbon-cache.py start</span><br></pre></td></tr></table></figure></p>\n<p>启动django，即整个Graphite的web应用。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python /opt/graphite/webapp/graphite/manage.py runserver 0.0.0.0:12222</span><br></pre></td></tr></table></figure></p>\n<p>其中的12222号端口可以自己任意修改。<br>这样浏览器打开<code>http://127.0.0.1:12222</code>就可以看到Graphite的界面。 </p>\n<p><strong>然而启动django这一步我在运行的时候遇到了各种错误。</strong></p>\n<ul>\n<li>在我<code>pip install</code>的django最新版本为2.0的时候，我遇到了大量有关pattern的错误，我按照网上说的全部更改之后还是无法运行，所以弃掉2.0。</li>\n<li><p>然而我<code>pip install django==1.9</code>之后，还是遇到了</p>\n<ul>\n<li>no such table: auth_user</li>\n<li>no such table: account_profile</li>\n<li>Unknown command: ‘syncdb’</li>\n</ul>\n<p>这三种错误，发生的先后顺序不一定准，但我在疯狂查阅各种资料之后还是没有成功解决。</p>\n</li>\n<li>这些错误都是因为django版本不兼容导致的，在我<code>pip install django==1.8</code>之后，整个世界就清静了。</li>\n</ul>\n<p>如果这时候以为打开浏览器就能看到界面的话，那你还是太年轻了。命令行能够运行没错，但是web可视化的界面还是有错！<br>Graphite界面会提示<code>import cairo</code>出错，这个cairo也是个大坑，原因就是你没有安装cairo图形库，<code>pip install cairo</code>之后你终于能松一口气欣赏一下Graphite了。</p>\n<p>页面右上角的dashboard页面可以玩一下，有很多高阶的功能，你会看到左侧tree那边有一些数据，这些数据存储在<code>/opt/graphite/storage/whisper</code>。</p>\n<p>另外，Graphite有个<strong>时区设置</strong>的问题，如果不更改，你的时间显示都是GMT时间。只需将<code>/opt/graphite/webapp/graphite/local_settings.py</code>文件里的TIME_ZONE配置改成如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Set your local timezone (Django's default is America/Chicago)</span></span><br><span class=\"line\"><span class=\"comment\"># If your graphs appear to be offset by a couple hours then this probably</span></span><br><span class=\"line\"><span class=\"comment\"># needs to be explicitly set to your local timezone.</span></span><br><span class=\"line\">TIME_ZONE = <span class=\"string\">'Asia/Shanghai'</span></span><br></pre></td></tr></table></figure></p>\n<p>如果之后想对Graphite的详细架构和具体配置有更深入的了解的话，推荐两个链接：</p>\n<ul>\n<li><a href=\"https://github.com/springside/springside4/wiki/Graphite\">Graphite</a></li>\n<li><a href=\"http://m.linuxeden.com/wap.php?action=article&amp;id=159746\">Graphite监控新手入门 </a></li>\n</ul>\n<h1 id=\"Graphite与scrapy的结合\"><a href=\"#Graphite与scrapy的结合\" class=\"headerlink\" title=\"Graphite与scrapy的结合\"></a>Graphite与scrapy的结合</h1><p>结合的方法详见原作者项目中<a href=\"https://github.com/gnemoug/distribute_crawler/blob/master/woaidu_crawler/woaidu_crawler/statscol/graphite.py\">graphite.py</a>文件中的注释，我总结为一下几点：</p>\n<ul>\n<li>把<code>/opt/graphite/webapp/content/js/composer_widgets.js</code>文件中<code>toggleAutoRefresh</code>函数里的interval变量从60改为1。</li>\n<li><p>在配置文件<code>storage-aggregation.conf</code>里添加</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[scrapy_min]</span><br><span class=\"line\">pattern = ^scrapy\\..*_min$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = min</span><br><span class=\"line\">[scrapy_max]</span><br><span class=\"line\">pattern = ^scrapy\\..*_max$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = max</span><br><span class=\"line\">[scrapy_sum]</span><br><span class=\"line\">pattern = ^scrapy\\..*_count$</span><br><span class=\"line\">xFilesFactor = 0.1</span><br><span class=\"line\">aggregationMethod = sum</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在爬虫的配置文件<code>setting.py</code>里添加</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">STATS_CLASS = <span class=\"string\">'scrapygraphite.GraphiteStatsCollector'</span></span><br><span class=\"line\">GRAPHITE_HOST = <span class=\"string\">'127.0.0.1'</span></span><br><span class=\"line\">GRAPHITE_PORT = <span class=\"number\">2003</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>后两点是我自己的修改。<br>scrapy本身提供的状态记录偏少，且缺乏实时的速度信息，都是不断增长式的总和记录。我想让scrapy能够定时发送pages的抓取速度和item的生成速度给Graphite，所以我在scrapy源码的<code>scrapy/extensions/logstats.py</code>文件中添加了两个状态变量的发送。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spider_opened</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    self.pagesprev = <span class=\"number\">0</span></span><br><span class=\"line\">    self.itemsprev = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    self.task = task.LoopingCall(self.log, spider)</span><br><span class=\"line\">    self.task.start(self.interval)</span><br><span class=\"line\"></span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, <span class=\"number\">0</span>, spider=spider)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">log</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">    items = self.stats.get_value(<span class=\"string\">'item_scraped_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    pages = self.stats.get_value(<span class=\"string\">'response_received_count'</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    irate = (items - self.itemsprev) * self.multiplier</span><br><span class=\"line\">    prate = (pages - self.pagesprev) * self.multiplier</span><br><span class=\"line\">    self.pagesprev, self.itemsprev = pages, items</span><br><span class=\"line\"></span><br><span class=\"line\">    msg = (<span class=\"string\">\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"</span></span><br><span class=\"line\">           <span class=\"string\">\"scraped %(items)d items (at %(itemrate)d items/min)\"</span>)</span><br><span class=\"line\">    log_args = &#123;<span class=\"string\">'pages'</span>: pages, <span class=\"string\">'pagerate'</span>: prate,</span><br><span class=\"line\">                <span class=\"string\">'items'</span>: items, <span class=\"string\">'itemrate'</span>: irate&#125;</span><br><span class=\"line\">    logger.info(msg, log_args, extra=&#123;<span class=\"string\">'spider'</span>: spider&#125;)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'pages_min'</span>, prate, spider=spider)</span><br><span class=\"line\">    self.stats.set_value(<span class=\"string\">'items_min'</span>, irate, spider=spider)</span><br><span class=\"line\">    states = self.stats.get_stats()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> states:</span><br><span class=\"line\">        self.stats._set_value(key, states[key], spider=spider)</span><br></pre></td></tr></table></figure>\n<p>这里的log函数每隔interval的秒数就会执行一次，interval的值可以在setting里配置<code>LOGSTATS_INTERVAL</code>的值。因为scrapy里的状态值很多是在增长时才会调用inc_value去加一改变大小，数据不增长就不会变，也不会去发送给Graphite。所以我在定时执行的log函数里强行再去发送一下数据，不管值有没有改变，即最后的三行代码。虽然我感觉在Graphite的显示里这样做好像并没有什么效果。</p>\n</li>\n<li><p>在数据的分布定义storage-schemas.conf中，默认是按60秒一个数据的方式，存一天的数据，一天前的数据就没了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[default_1min_for_1day]</span><br><span class=\"line\">pattern = .*</span><br><span class=\"line\">retentions = 60s:1d</span><br></pre></td></tr></table></figure>\n<p>但是爬虫的数据60s存一个数据显得有点稀疏，特别是在想要显示实时抓取速度时候，这里可以根据不同需求进行更改。</p>\n</li>\n</ul>\n<h1 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h1><p>贴一个Graphite的效果图：<br><img src=\"/uploads/img/20161006/graphite.png\" alt=\"Graphite\"><br>如果嫌原版Graphite界面丑，据说可以使用豆瓣写的皮肤<a href=\"https://github.com/douban/graph-index\">graph-index</a>。</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjh96tx220000hoc54mj5s4ew","category_id":"cjh96tx2q0005hoc5sugiw1dd","_id":"cjh96tx38000choc5d0vu2uc5"},{"post_id":"cjh96tx2h0002hoc5ob6ur4g8","category_id":"cjh96tx2q0005hoc5sugiw1dd","_id":"cjh96tx3e000ghoc56fl2nuco"},{"post_id":"cjh96tx2r0006hoc5t4f2sjmr","category_id":"cjh96tx3a000ehoc5ndj8gtz1","_id":"cjh96tx3g000jhoc5l42o5v5e"},{"post_id":"cjh96tx2v0007hoc5mgewom5k","category_id":"cjh96tx3e000hhoc5gh66aa6q","_id":"cjh96tx3i000nhoc5akdow7uh"},{"post_id":"cjh96tx2x0009hoc5egzfxu3r","category_id":"cjh96tx3g000khoc5kol1plpn","_id":"cjh96tx3m000thoc5x6h7qgxg"},{"post_id":"cjh96tx32000bhoc5c8i8ae0z","category_id":"cjh96tx3j000qhoc58ms4nbe0","_id":"cjh96tx3p000yhoc511x2i9tf"},{"post_id":"cjh96tx38000dhoc5torxnwqk","category_id":"cjh96tx3e000hhoc5gh66aa6q","_id":"cjh96tx3q0010hoc5cf9b2trz"},{"post_id":"cjh96tx5j001mhoc5d2ehdak8","category_id":"cjh96tx2q0005hoc5sugiw1dd","_id":"cjh96tx5q001phoc581440upf"},{"post_id":"cjh96tx5l001nhoc5mpylg4v2","category_id":"cjh96tx2q0005hoc5sugiw1dd","_id":"cjh96tx5q001qhoc51hlgkjqm"}],"PostTag":[{"post_id":"cjh96tx220000hoc54mj5s4ew","tag_id":"cjh96tx2m0004hoc5z2n801ez","_id":"cjh96tx3i000mhoc50xkf559m"},{"post_id":"cjh96tx220000hoc54mj5s4ew","tag_id":"cjh96tx2x0008hoc5491a19yo","_id":"cjh96tx3i000ohoc5wnv4x01x"},{"post_id":"cjh96tx220000hoc54mj5s4ew","tag_id":"cjh96tx3b000fhoc5a5lm73z5","_id":"cjh96tx3l000rhoc5mz8ejy8f"},{"post_id":"cjh96tx220000hoc54mj5s4ew","tag_id":"cjh96tx3f000ihoc50s4zakd2","_id":"cjh96tx3l000shoc541ofnn20"},{"post_id":"cjh96tx2h0002hoc5ob6ur4g8","tag_id":"cjh96tx2x0008hoc5491a19yo","_id":"cjh96tx3n000whoc5tn8unrls"},{"post_id":"cjh96tx2h0002hoc5ob6ur4g8","tag_id":"cjh96tx3f000ihoc50s4zakd2","_id":"cjh96tx3p000xhoc5e6ljefem"},{"post_id":"cjh96tx2r0006hoc5t4f2sjmr","tag_id":"cjh96tx3m000uhoc59llcuqjt","_id":"cjh96tx3r0012hoc57ty8levt"},{"post_id":"cjh96tx2r0006hoc5t4f2sjmr","tag_id":"cjh96tx3p000zhoc5grdu7ayc","_id":"cjh96tx3r0013hoc5csj32bas"},{"post_id":"cjh96tx2v0007hoc5mgewom5k","tag_id":"cjh96tx3q0011hoc5uavbyv8v","_id":"cjh96tx400016hoc5wt060o2i"},{"post_id":"cjh96tx2v0007hoc5mgewom5k","tag_id":"cjh96tx3f000ihoc50s4zakd2","_id":"cjh96tx410017hoc52emr0gx6"},{"post_id":"cjh96tx2x0009hoc5egzfxu3r","tag_id":"cjh96tx400015hoc5vj5nc6bi","_id":"cjh96tx42001ahoc51hdbhonx"},{"post_id":"cjh96tx2x0009hoc5egzfxu3r","tag_id":"cjh96tx3f000ihoc50s4zakd2","_id":"cjh96tx42001bhoc53et0lsh0"},{"post_id":"cjh96tx32000bhoc5c8i8ae0z","tag_id":"cjh96tx410019hoc510a6tvrp","_id":"cjh96tx45001ehoc5skzp7uys"},{"post_id":"cjh96tx32000bhoc5c8i8ae0z","tag_id":"cjh96tx3f000ihoc50s4zakd2","_id":"cjh96tx45001fhoc5up870l6c"},{"post_id":"cjh96tx38000dhoc5torxnwqk","tag_id":"cjh96tx3q0011hoc5uavbyv8v","_id":"cjh96tx47001ihoc5tn43qfcs"},{"post_id":"cjh96tx38000dhoc5torxnwqk","tag_id":"cjh96tx45001ghoc5lwwdxi46","_id":"cjh96tx47001jhoc5i72qf26v"},{"post_id":"cjh96tx38000dhoc5torxnwqk","tag_id":"cjh96tx46001hhoc5hne2qcq4","_id":"cjh96tx48001khoc5zrlp6v9t"},{"post_id":"cjh96tx5j001mhoc5d2ehdak8","tag_id":"cjh96tx5o001ohoc55ks8dixv","_id":"cjh96tx5s001thoc5amkb9guy"},{"post_id":"cjh96tx5j001mhoc5d2ehdak8","tag_id":"cjh96tx5r001rhoc59vk3gt8q","_id":"cjh96tx5s001uhoc5za3to4ve"},{"post_id":"cjh96tx5j001mhoc5d2ehdak8","tag_id":"cjh96tx3f000ihoc50s4zakd2","_id":"cjh96tx5t001vhoc51fn0ifvb"},{"post_id":"cjh96tx5l001nhoc5mpylg4v2","tag_id":"cjh96tx5r001shoc5zzjbixan","_id":"cjh96tx5t001whoc5njjv2739"},{"post_id":"cjh96tx5l001nhoc5mpylg4v2","tag_id":"cjh96tx2x0008hoc5491a19yo","_id":"cjh96tx5u001xhoc5d9sn3de3"},{"post_id":"cjh96tx5l001nhoc5mpylg4v2","tag_id":"cjh96tx2m0004hoc5z2n801ez","_id":"cjh96tx5u001yhoc5rh5h1uhi"},{"post_id":"cjh96tx5l001nhoc5mpylg4v2","tag_id":"cjh96tx3f000ihoc50s4zakd2","_id":"cjh96tx5u001zhoc55e7xxeq5"}],"Tag":[{"name":"CentOS","_id":"cjh96tx2m0004hoc5z2n801ez"},{"name":"scrapy","_id":"cjh96tx2x0008hoc5491a19yo"},{"name":"ghost.py","_id":"cjh96tx3b000fhoc5a5lm73z5"},{"name":"python","_id":"cjh96tx3f000ihoc50s4zakd2"},{"name":"English","_id":"cjh96tx3m000uhoc59llcuqjt"},{"name":"Hexo","_id":"cjh96tx3p000zhoc5grdu7ayc"},{"name":"HMM","_id":"cjh96tx3q0011hoc5uavbyv8v"},{"name":"PIL","_id":"cjh96tx400015hoc5vj5nc6bi"},{"name":"NMF","_id":"cjh96tx410019hoc510a6tvrp"},{"name":"MEMM","_id":"cjh96tx45001ghoc5lwwdxi46"},{"name":"CRF","_id":"cjh96tx46001hhoc5hne2qcq4"},{"name":"zhihu","_id":"cjh96tx5o001ohoc55ks8dixv"},{"name":"Neo4j","_id":"cjh96tx5r001rhoc59vk3gt8q"},{"name":"Graphite","_id":"cjh96tx5r001shoc5zzjbixan"}]}}