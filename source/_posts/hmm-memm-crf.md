title: HMM、MEMM和CRF的学习与总结
date: 2016-07-14 16:03:45
comments: true
tags: 
 - HMM
 - MEMM
 - CRF
categories: NLP
photos: 
 - /uploads/img/20160714/cover.png
---
最近一直在学习NLP里最基础的几个语言模型：**隐马尔科夫模型**（Hidden Markov Model，HMM）、**最大熵马尔科夫模型**（Maximum Entropy Markov Model，MEMM）和**条件随机场**（Conditional Random Field，CRF）。这三种模型在自然语言处理中，可以解决分词（segment，Seg）、标注（Tag）和命名实体识别（Named Entity Recognition，Ner）等问题。学习的时候参考最多的两本书是李航老师的**《统计学习方法》**和吴军老师的**《数学之美》**。如需这两本书的电子版可以给我留言。

我先分别简单介绍一下几种模型，具体的推导过程就不列出来，《统计学习方法》上有非常详细的数学原理。

# HMM

下图是《统计学习方法》中的描述：
![隐马尔科夫模型](/uploads/img/20160714/hmm.png)
HMM模型将状态序列看作马尔可夫链，一阶马尔可夫链式针对相邻状态的关系进行建模，其中每个状态对应一个概率函数。HMM是一种**生成模型**（Generative Model)，定义了联合概率分布 ，其中$x$和$y$分别表示观测序列和状态序列的随机变量。

如果需要一些浅显简单的例子来理解HMM，下面的一个知乎问题和一篇博客可能有所帮助：
- [如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)
- [隐马尔可夫模型（HMM）攻略](http://blog.csdn.net/likelet/article/details/7056068)

# Maximum Entropy Model

首先贴一下关于最大熵模型的定义：
![最大熵模型](/uploads/img/20160714/me.png)
最大熵模型的基本思想就是不要把所有鸡蛋放到一个篮子里。式（6.12）中的$f_i$是**特征函数**，代表各个约束条件。最大熵模型就是在符合所有约束条件下作出**最不偏倚**的假设，求得可使熵最大化的概率分布。熵最大，表示该系统内各随机事件(变量)发生的概率是近似均匀的，等可能性的。

最大熵模型可以使用任意的复杂相关特征（即特征函数），在性能上最大熵分类器超过了Bayes分类器。但是，作为一种分类器模型，这两种方法有一个共同的缺点：每个词都是单独进行分类的，标记状态之间的关系无法得到充分利用，具有马尔可夫链的HMM模型可以建立标记之间的马尔可夫关联性，这是最大熵模型所没有的。

最大熵模型的**优点**：首先，最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型;其次，最大熵统计模型可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度;再次，它还能自然地解决了统计模型中参数平滑的问题。

最大熵模型的**不足**：首先，最大熵统计模型中二值化特征只是记录特征的出现是否，而文本分类需要知道特征的强度，因此，它在分类方法中不是最优的;其次，由于算法收敛的速度较慢，所以导致最大熵统计模型它的计算代价较大，时空开销大;再次，数据稀疏问题比较严重。

# MEMM

最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一个**判别模型**（Discriminative Model），这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度，召回率也大大的提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好得多。
![最大熵马尔科夫模型](/uploads/img/20160714/memm.png)
可以注意到MEMM在每个节点对所有可能的状态$y$求和然后用做局部归一化的分母。所以MEMM中节点状态转移的概率都是归一化的概率。

HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)。但MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型仍然存在**标注偏置问题**（Label Bias Problem）。

关于标注偏置问题，网上最多的是下面这个例子解释：
![](/uploads/img/20160714/label-bias-1.png)
路径1-1-1-1的概率：0.4\*0.45\*0.5=0.09
路径2-2-2-2的概率：0.018
路径1-2-1-2的概率：0.06
路径1-1-2-2的概率：0.066
由此可得最优路径为：1-1-1-1
![](/uploads/img/20160714/label-bias-2.png)
而实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2，这就是所谓的标注偏置问题，由于分支数不同，概率的分布不均衡，导致状态的转移存在不公平的情况。
例子的出处参见[标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较](http://blog.csdn.net/lskyne/article/details/8669301)

# CRF

![线性链条件随机场模型](/uploads/img/20160714/crf-1.png)
这是书上关于条件随机场的简化形式。本文所提的CRF都不是广义上最大熵准则建模条件概率的模型，而是约束在线性链上的特殊的条件随机场，称为线性链条件随机场（linear chain CRF）。
![线性链条件随机场模型图示](/uploads/img/20160714/crf-2.png)
上式中也同样有$f_i$**特征函数**。之前我对模型中的特征函数一直不太理解。大家可以参考[中文分词入门之字标注法4](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%954)这篇文章。文章主要介绍借用条件随机场工具“[CRF++: Yet Another CRF toolkit](http://tenet.dl.sourceforge.net/project/crfpp/crfpp-win32/0.54/CRF%2B%2B-0.54.zip)”来完成字标注中文分词的全过程。其中提及了特征模板文件，当然它的特征提取可能包含了前后多个节点。顺便推荐一下这个牛逼的群体博客[52nlp](http://www.52nlp.cn/)。

$$p(y_1, \ldots, y_T | x_1, \ldots, x_T) &=& \Pi_{i=1}^T p(y_i | x_1, \ldots, x_T, y_{i-1})$$

$$f(x_1,x_2,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2$$

$$
p(y_i|x_1,+...,+x_T,+y_{i-1}) &=&
\frac{exp(\sum_{k=1}^K+\theta_{k}f_k(x_1,+...,+x_T,+y_{i-1},+y_i)}
{\sum_y+exp(\sum_{k=1}^K+\theta_{k}f_k(x_1,+...,+x_T,+y_{i-1},+y)}
$$

https://www.zhihu.com/equation?tex=p%28y_1%2C+...%2C+y_T+%7C+x_1%2C+...%2C+x_T%29+%26%3D+%0A%5CPi_%7Bi%3D1%7D%5ET+p%28y_i%7Cx_1%2C+...%2C+x_T%2C+y_%7Bi-1%7D%29%0A%5C%5C%0Ap%28y_i%7Cx_1%2C+...%2C+x_T%2C+y_%7Bi-1%7D%29+%26%3D+%0A%5Cfrac%7Bexp%28%5Csum_%7Bk%3D1%7D%5EK+%5Ctheta_%7Bk%7Df_k%28x_1%2C+...%2C+x_T%2C+y_%7Bi-1%7D%2C+y_i%29%7D%0A%7B%5Csum_y+exp%28%5Csum_%7Bk%3D1%7D%5EK+%5Ctheta_%7Bk%7Df_k%28x_1%2C+...%2C+x_T%2C+y_%7Bi-1%7D%2C+y%29%7D%0A